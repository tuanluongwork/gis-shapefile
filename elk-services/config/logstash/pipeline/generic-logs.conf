input {
  # Beats input for Filebeat integration (primary method)
  beats {
    port => 5044
  }
}

filter {
  # Since Filebeat is configured with json.keys_under_root: true, 
  # JSON fields are already parsed and available at root level
  # We don't need additional JSON parsing

  # Handle plain text logs with custom patterns
  if [codec] != "json_lines" and "${LOG_FORMAT}" != "json" {
    # Apply custom grok pattern if defined
    if "${CUSTOM_GROK_PATTERN:}" != "" {
      grok {
        match => { "message" => "${CUSTOM_GROK_PATTERN}" }
        tag_on_failure => ["_grok_parse_failure"]
      }
    } else {
      # Default pattern for common log formats
      grok {
        match => { 
          "message" => [
            "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}",
            "%{COMMONAPACHELOG}",
            "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{PROG:program}: %{GREEDYDATA:message}"
          ]
        }
        tag_on_failure => ["_grok_parse_failure"]
      }
    }
  }

  # Extract correlation ID from log-services-style logs
  if [message] and "correlation_id:" in [message] {
    grok {
      match => { "message" => ".*correlation_id:(?<correlation_id>[^\s]+)" }
      tag_on_failure => []
    }
  }

  # Extract thread information if pattern is defined
  if "${THREAD_PATTERN:}" != "" {
    grok {
      match => { "message" => "${THREAD_PATTERN}" }
      tag_on_failure => []
    }
  }

  # Parse timestamps - log-services uses ISO8601 format with nanoseconds
  if [timestamp] {
    date {
      match => [ 
        "timestamp", 
        "ISO8601",
        "yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSS'Z'",
        "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'",
        "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'",
        "yyyy-MM-dd HH:mm:ss.SSS",
        "MMM dd HH:mm:ss",
        "MMM  d HH:mm:ss"
      ]
      target => "@timestamp"
    }
  }

  # Normalize log level
  if [level] {
    mutate {
      lowercase => ["level"]
    }
    
    # Map common log level variations
    if [level] == "warn" {
      mutate { replace => { "level" => "warning" } }
    }
    if [level] == "err" {
      mutate { replace => { "level" => "error" } }
    }
  }

  # Set metadata explicitly as single values
  ruby {
    code => "
      event.set('[@metadata][index_prefix]', '${INDEX_PREFIX:pxp-logs}')
      event.set('application', '${APPLICATION_NAME:pxpoint}')
      event.set('log_source', 'filebeat')
      event.set('log_type', '${LOG_TYPE:gis_application}')
    "
  }

  # Extract additional fields from log-services message format
  if [message] and [message] =~ /\|.*\w+:[\w\.-]+/ {
    grok {
      match => { 
        "message" => [
          ".*\| (?<extracted_fields>.*)",
          ".*correlation_id:(?<correlation_id>[^\s|]+).*",
          ".*operation:(?<operation>[^\s|]+).*",
          ".*duration:(?<duration>[^\s|]+).*",
          ".*query_time:(?<query_time>[^\s|]+).*",
          ".*component:(?<component>[^\s|]+).*",
          ".*status:(?<status>[^\s|]+).*",
          ".*pid:(?<pid>[^\s|]+).*",
          ".*version:(?<version>[^\s|]+).*",
          ".*load_time_ms:(?<load_time_ms>[^\s|]+).*",
          ".*response_time_ms:(?<response_time_ms>[^\s|]+).*"
        ]
      }
      tag_on_failure => []
    }
    
    # Convert numeric fields to numbers
    if [duration] {
      mutate { convert => { "duration" => "float" } }
    }
    if [query_time] {
      mutate { convert => { "query_time" => "float" } }
    }
    if [load_time_ms] {
      mutate { convert => { "load_time_ms" => "float" } }
    }
    if [response_time_ms] {
      mutate { convert => { "response_time_ms" => "float" } }
    }
    if [pid] {
      mutate { convert => { "pid" => "integer" } }
    }
  }

  # Add component from logger name if present
  if [logger] {
    mutate {
      add_field => { "component" => "%{logger}" }
    }
  } else if [program] {
    mutate {
      add_field => { "component" => "%{program}" }
    }
  }

  # Add host information
  if ![host] and [beat] and [beat][hostname] {
    mutate {
      add_field => { "host" => "%{[beat][hostname]}" }
    }
  }

  # Clean up temporary and duplicate fields
  mutate {
    remove_field => [ 
      "beat", "prospector", "input", "offset", "source", 
      "extracted_fields", "log_message", "context_data", 
      "key", "value", "agent", "ecs", "container", "event"
    ]
  }

  # Only drop truly empty events (keep all messages with content)
  if ![message] and ![timestamp] and ![level] {
    drop { }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
  }

  # Debug output (disable in production by setting DEBUG_OUTPUT=false)
  if "${DEBUG_OUTPUT:true}" == "true" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
}