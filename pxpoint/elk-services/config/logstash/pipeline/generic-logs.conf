input {
  # Beats input for Filebeat integration (primary method)
  beats {
    port => 5044
  }
}

filter {
  # Since Filebeat is configured with json.keys_under_root: true, 
  # JSON fields are already parsed and available at root level
  # We don't need additional JSON parsing

  # Handle plain text logs with custom patterns
  if [codec] != "json_lines" and "${LOG_FORMAT}" != "json" {
    # Apply custom grok pattern if defined
    if "${CUSTOM_GROK_PATTERN:}" != "" {
      grok {
        match => { "message" => "${CUSTOM_GROK_PATTERN}" }
        tag_on_failure => ["_grok_parse_failure"]
      }
    } else {
      # Default pattern for common log formats
      grok {
        match => { 
          "message" => [
            "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{GREEDYDATA:message}",
            "%{COMMONAPACHELOG}",
            "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{PROG:program}: %{GREEDYDATA:message}"
          ]
        }
        tag_on_failure => ["_grok_parse_failure"]
      }
    }
  }

  # Correlation ID extraction - handle both direct JSON fields and message parsing
  # Use a ruby filter to avoid field duplication and array creation
  ruby {
    code => "
      # Extract correlation IDs with priority: direct fields > message parsing
      
      # Pipeline ID
      if event.get('correlation_pipeline_id').nil?
        if !event.get('pipeline_id').nil?
          event.set('correlation_pipeline_id', event.get('pipeline_id'))
        elsif !event.get('message').nil? && event.get('message').include?('pipeline_id:')
          match = event.get('message').match(/pipeline_id:([^\s|]+)/)
          if match
            event.set('correlation_pipeline_id', match[1])
          end
        end
      end
      
      # Process ID
      if event.get('correlation_process_id').nil?
        if !event.get('process_id').nil? && event.get('process_id') != event.get('thread')
          # Skip if process_id is actually thread ID
          event.set('correlation_process_id', event.get('process_id'))
        elsif !event.get('message').nil? && event.get('message').include?('process_id:')
          match = event.get('message').match(/process_id:([^\s|]+)/)
          if match && match[1] != event.get('thread')
            event.set('correlation_process_id', match[1])
          end
        end
      end
      
      # Activity ID
      if event.get('correlation_activity_id').nil?
        if !event.get('activity_id').nil?
          event.set('correlation_activity_id', event.get('activity_id'))
        elsif !event.get('message').nil? && event.get('message').include?('activity_id:')
          match = event.get('message').match(/activity_id:([^\s|]+)/)
          if match
            event.set('correlation_activity_id', match[1])
          end
        end
      end
      
      # Create full correlation ID (most specific available)
      if !event.get('correlation_activity_id').nil?
        event.set('correlation_full_id', event.get('correlation_activity_id'))
      elsif !event.get('correlation_process_id').nil?
        event.set('correlation_full_id', event.get('correlation_process_id'))
      elsif !event.get('correlation_pipeline_id').nil?
        event.set('correlation_full_id', event.get('correlation_pipeline_id'))
      end
    "
  }

  # Extract correlation hierarchy components for analysis
  if [correlation_pipeline_id] {
    grok {
      match => { "correlation_pipeline_id" => "pipeline-(?<pipeline_timestamp>[0-9]+)-(?<pipeline_instance>[^-]+)-(?<pipeline_hash>[a-f0-9]+)" }
      tag_on_failure => []
    }
  }
  
  if [correlation_process_id] {
    grok {
      match => { "correlation_process_id" => "pipeline-[0-9]+-[^-]+-[a-f0-9]+-proc-(?<process_type>[^-]+)-(?<process_hash>[a-f0-9]+)" }
      tag_on_failure => []
    }
  }
  
  if [correlation_activity_id] {
    grok {
      match => { "correlation_activity_id" => ".*-act-(?<activity_name>[^-]+)-(?<activity_hash>[a-f0-9]+)" }
      tag_on_failure => []
    }
  }

  # Extract legacy correlation ID format
  if [message] and "correlation_id:" in [message] and ![legacy_correlation_id] {
    grok {
      match => { "message" => ".*correlation_id:(?<legacy_correlation_id>[^\\s]+)" }
      tag_on_failure => []
    }
  }

  # Extract thread information if pattern is defined
  if "${THREAD_PATTERN:}" != "" {
    grok {
      match => { "message" => "${THREAD_PATTERN}" }
      tag_on_failure => []
    }
  }

  # Parse timestamps - log-services uses ISO8601 format with nanoseconds
  if [timestamp] {
    date {
      match => [ 
        "timestamp", 
        "ISO8601",
        "yyyy-MM-dd'T'HH:mm:ss.SSSSSSSSS'Z'",
        "yyyy-MM-dd'T'HH:mm:ss.SSSSSS'Z'",
        "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'",
        "yyyy-MM-dd HH:mm:ss.SSS",
        "MMM dd HH:mm:ss",
        "MMM  d HH:mm:ss"
      ]
      target => "@timestamp"
    }
  }

  # Normalize log level
  if [level] {
    mutate {
      lowercase => ["level"]
    }
    
    # Map common log level variations
    if [level] == "warn" {
      mutate { replace => { "level" => "warning" } }
    }
    if [level] == "err" {
      mutate { replace => { "level" => "error" } }
    }
  }

  # Set metadata explicitly as single values
  ruby {
    code => "
      event.set('[@metadata][index_prefix]', '${INDEX_PREFIX:pxp-logs}')
      event.set('application', '${APPLICATION_NAME:pxpoint}')
      event.set('log_source', 'filebeat')
      event.set('log_type', '${LOG_TYPE:application}')
    "
  }

  # Extract additional fields from log-services message format
  if [message] and [message] =~ /\|.*\w+:[\w\.-]+/ {
    grok {
      match => { 
        "message" => [
          ".*\\| (?<extracted_fields>.*)",
          ".*operation:(?<operation>[^\\s|]+).*",
          ".*duration:(?<duration>[^\\s|]+).*",
          ".*duration_ms:(?<duration_ms>[^\\s|]+).*",
          ".*processing_time_ms:(?<processing_time_ms>[^\\s|]+).*",
          ".*query_time:(?<query_time>[^\\s|]+).*",
          ".*component:(?<component>[^\\s|]+).*",
          ".*status:(?<status>[^\\s|]+).*",
          ".*pid:(?<pid>[^\\s|]+).*",
          ".*version:(?<version>[^\\s|]+).*",
          ".*load_time_ms:(?<load_time_ms>[^\\s|]+).*",
          ".*response_time_ms:(?<response_time_ms>[^\\s|]+).*",
          ".*event_type:(?<event_type>[^\\s|]+).*",
          ".*process_type:(?<extracted_process_type>[^\\s|]+).*",
          ".*worker_type:(?<worker_type>[^\\s|]+).*",
          ".*step:(?<step>[^\\s|]+).*",
          ".*algorithm:(?<algorithm>[^\\s|]+).*",
          ".*success:(?<success>[^\\s|]+).*",
          ".*items_processed:(?<items_processed>[^\\s|]+).*",
          ".*workers_spawned:(?<workers_spawned>[^\\s|]+).*",
          ".*total_processing_time_ms:(?<total_processing_time_ms>[^\\s|]+).*"
        ]
      }
      tag_on_failure => []
    }
    
    # Convert numeric fields to numbers
    if [duration] {
      mutate { convert => { "duration" => "float" } }
    }
    if [duration_ms] {
      mutate { convert => { "duration_ms" => "float" } }
    }
    if [processing_time_ms] {
      mutate { convert => { "processing_time_ms" => "float" } }
    }
    if [total_processing_time_ms] {
      mutate { convert => { "total_processing_time_ms" => "float" } }
    }
    if [query_time] {
      mutate { convert => { "query_time" => "float" } }
    }
    if [load_time_ms] {
      mutate { convert => { "load_time_ms" => "float" } }
    }
    if [response_time_ms] {
      mutate { convert => { "response_time_ms" => "float" } }
    }
    if [items_processed] {
      mutate { convert => { "items_processed" => "float" } }
    }
    if [workers_spawned] {
      mutate { convert => { "workers_spawned" => "float" } }
    }
    if [pid] {
      mutate { convert => { "pid" => "integer" } }
    }
    if [success] {
      mutate { convert => { "success" => "boolean" } }
    }
  }

  # Add component from logger name if present and not already set
  if [logger] and ![component] {
    mutate {
      add_field => { "component" => "%{logger}" }
    }
  } else if [program] and ![component] {
    mutate {
      add_field => { "component" => "%{program}" }
    }
  }
  
  # Set process type from extracted value or correlation
  if [extracted_process_type] and ![process_type] {
    mutate {
      add_field => { "process_type" => "%{extracted_process_type}" }
    }
  } else if [process_type] and ![extracted_process_type] {
    mutate {
      add_field => { "extracted_process_type" => "%{process_type}" }
    }
  }
  
  # Convert pipeline timestamp to readable date
  if [pipeline_timestamp] {
    date {
      match => [ "pipeline_timestamp", "UNIX" ]
      target => "pipeline_start_time"
    }
  }

  # Add host information
  if ![host] and [beat] and [beat][hostname] {
    mutate {
      add_field => { "host" => "%{[beat][hostname]}" }
    }
  }

  # Clean up temporary and duplicate fields
  mutate {
    remove_field => [ 
      "beat", "prospector", "input", "offset", "source", 
      "extracted_fields", "log_message", "context_data", 
      "key", "value", "agent", "ecs", "container", "event",
      "pipeline_id", "process_id", "activity_id"  # Remove original fields as we have correlation_ prefixed ones
    ]
  }
  
  # Add tags for easier filtering
  if [correlation_pipeline_id] {
    mutate {
      add_tag => [ "has_pipeline_correlation" ]
    }
  }
  
  if [correlation_process_id] {
    mutate {
      add_tag => [ "has_process_correlation" ]
    }
  }
  
  if [correlation_activity_id] {
    mutate {
      add_tag => [ "has_activity_correlation" ]
    }
  }
  
  if [event_type] {
    mutate {
      add_tag => [ "event_%{event_type}" ]
    }
  }
  
  if [duration_ms] or [processing_time_ms] or [total_processing_time_ms] {
    mutate {
      add_tag => [ "performance_metrics" ]
    }
  }

  # Only drop truly empty events (keep all messages with content)
  if ![message] and ![timestamp] and ![level] {
    drop { }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "${INDEX_PREFIX:pxp-logs}-%{+YYYY.MM.dd}"
  }

  # Debug output (disable in production by setting DEBUG_OUTPUT=false)
  if "${DEBUG_OUTPUT:true}" == "true" {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }
}