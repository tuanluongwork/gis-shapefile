1 Introduction
1.1 Demystifying PxPoint
CoreLogic has engaged Slower to perform an analysis for specific aspects of PxPoint to Identity the underlying causes of issues and provide recommendations for remediating them. This document records CoreLogic's PxPoint deployment at the time of the assessment, detailing the state of the deploymen described to Slower, gaps that CoreLogic may wish to address, and recommendations for addressing
1.2 How to Read the Document
Each major section for a subject is organized into subsections, including but not limited to:
Description
Git Location
CLR Dependencies
Inputs
outputs
Logging
Dependencies on SQL Server
Deralls
Gaps
Recommendations
2 Definitions
2.1 Ratings
Gaps and recommendations provided in this document are rated with priorities. A priority is
derived ov considering the risk and imoact on the PxPoint environment:
• Critical 1 - A gap presents high risks and affects the reliability and availability of the production environment. It may also directly impact business operations. Corelogic should act upon the
recommendations) immediatelv.
• Critical 2 - A gap has the potential to impact the PxPoint environment, application development,
and business strategy sumport. Corelogic should act unon the recommendations) in the near. future to reduce risks.
• Critical 3 - A gap has no immediate impact. Still, it may prevent CoreLogic from establishing the foundation of a solid xPoint environment and reduce the environment's ability to support business strategy in the future.
3 Executive Summary
3.1 Summary
CoreLogic has engaged Slower for a consultation to review a specific code base of PxPoint, and Slower will:
Review Parcel Build Process via CoreLogic Code Repositories
•Develon. Document. and Present Recommendations for New Stack lechnology Solutions
: Develop, Document and Present Recommendations to Mer se the Tect oles process.
lhe consultation will produce the tollowing two deliverables:
: Recontrenda fions to improve Cose Repositories f
Recommendations to improve the speed of data refresh (also including modernizing this part of
the rxrount Moennes).
In preparing for the engagement CoreLogic has identified the following goals that they wish to achieve:
* ﻿﻿Speed: Contributor Data sets need to include: addresses, Gadberry data (delivered on a quarterly basis), Navteq (updated on a monthly basis), USPS (updated monthly), parcel data, structure data (updated once a year),
* ﻿﻿speed: Inis neeas to nappen through all the environments.
Speed: From a design standpoint, I would want this to be able to process data from multiple
* ﻿﻿Flexibility/Management: With the focus on "Speed" it should be explicitly stated that the outcome has a more flexible and easier to manage pipeline. While this is "implied" the goal should be stated as such.
Instrumentation/Audit Log: In addition, we may need the flexibility to be able to make changes to the data sets. One example is where we need to override the data we get from the providers with something more specific
Instrumentation/Audit Log: We need this to be instrumented. An example of this is where we
neea to make a decision on an Inaiviaual address and we neea to know now the decisions were made to make the final decisions. We need to know the history of the decisions made on an address where we may alter the final output due to decisions (e.g. Facebook example and hacker way)
Deliverables
Deliverable #1: Demystifying PxPoint Codebase (that is in-scope)
Deliverable #2: Recommendations to improve the speed of Refresh (by modernizing the pipeline)
3-phase plan to improve the speed and efficiency of a build pipeline process.
Phase & Tocuses on Voservality oy Implementing a stanaaraized logging Tramework a roving alerting and messaging for errors and major milestones. This will enable engineers
react swittiy to problems ana Increase speea.
   * ﻿﻿Phase 2 targets Data Operations by automating and cleaning the data prep phase, reducing edundancies and unnecessary reprocessing, and validating inputs and outputs between tasks.
   * ﻿﻿Phase 3 involves implementing a new Orchestrator, such as AWS Step Functions or Managed Worktlows for Airflow.
Overall, the plan aims to enhance the build pipeline process by improving observability, speed, data
oberations and orchestration.
Process for remediation
lhis document is written with the intent to action the recommendations. Our recommendations are based upon on findings in each of the directories within PxPoint and summarized as Critical 1, Critical 2 and Critical 3 as well as with a reference to the overall project goals outlined in the EPIC Stories and
Prolect Upaares, both or which are providea and reviewed every week. tach recommendation nas a name such that the content, the recommendation and the sections from this document can be pasted into Jira for execution. This document shall serve as a reference for all of the work needed to be done.
Our recommended course of action will be to organize all of the c1. C2 and C3 recommendations into a prioritized list and then staffed with 3-4 Universal Engineers who will develop, test and implement the recommended changes in 2 week sprints. We will break down the recommendations based upon the points associated with each of the identified issues and tasks.
PHASE 1: Observability
PxPoint must improve the logging, alerting, and error capturing within the build pipeline process to increase speed. Many speed issues arise from a lack of awareness (e.g., alerting) when errors occur.
Given the numerous distinct operations initiated ov the bulld process. capturing ALL errors—both within. le main process AND its sub-processes - is crucial. Enhanced logs and comprehensive error conte able engineers to react swiftly to problems. Addressing this issue transparently and in real tir
Tacilitates Taster engineer response and immediate impact on speed. This tirst step. observabilitv. willi have an immediate and positive effect.
Solution:
• Implement a standard Logging Framework
a. All applications and Tasks should use a standardized logger
b. All loggers should also write to a file, even if they already write to the console. That way history and errors will be persistent across executions, and be introspectable after the
All log messages should have full context, including information about what it's currently
doing and what FIPS or even Parcel it is processing (especiallv for error messages)
   * ﻿﻿﻿Logs, whether as part of the messages or in the file name, should indicate the User
executing the Task
   * ﻿﻿﻿All log messages must have a timestamp and information about where it occurred When a subprocess fails - catch the exception and log the full context, including stack-trace
When running, we want to know what each Task did and did not do, and why
• Better alerting and messaging for errors or major milestones.
Continue to use a Popup or Toast in the app: whatever exists currently
b. Send important messages to Slack, like errors or when a Task, or even Subtask, starts/stons or has an error Continue to log everything, as well
d. This isnot k yet. That Wi be addressed when changing Orchestrator in Phase 3
PHASE 2: Data Operations
• The data prep phase should be more automated and cleaned/appropriately validated. (Cleanse your inputs!)
Ihis Includes cleansing and validating initial data trom external inputs
b. USPS, NavTeq/HERE, StarMap (ParcelPoint?), Political Build, SpatialRecord, Diablo,
Structure, etc...
      * ﻿﻿keauce reaundancies ana unnecessary reprocessir
      * ﻿﻿Before each Task. validate data intearitv and correctnes
Make sure all input data exists, is complete/integrous and has the proper schema
This is especially important for "3rd party" data from other systems, like ParcelPoint, Diablo. USPS. Navtea. etc...
c. A Task should not start if any of its input data is invalid !. Validation failures should be sent to the Logger, Slack and the U
• Validate inputs and outouts between Tasks
PHASE 3: New Orchestration
• Build or buy a new Orchestration tool.
Overview
Current State
Jhe current Parcel bulld process is a hierarchy of C#. C++ and hvorid Apolications run against some naller Databases and large CSV files. The output is to large files in proprietary PXY format, related inde les, and some DB Tables. The pipeline runs through a C#/ NET/WPF Ul, where a user selects what file
to load and some tlags to control execution. Kunning takes anvwhere trom hours to davs. depending on the flags used. Any failures in running the pipeline require restarting the whole thing from the beginning However, some flags allow the user to skip earlier steps to avoid re-running tasks that were successful during an earlier run. Output is reported through Ul messages, Standard Out, and Log files, where
Standard Out from downstream applications is read by the calling application and reported by that application. Ul messages also appear in downstream Ul applications. The entire pipeline is run manually with parallelism-limited multi-threaded optimizations and manual tuning, though much of the heavy lifting is done on a very powerful CPU.
Gaps
There are many areas that need improvement. The impact of these problems is exacerbated by the
overall application design, where iT anvihing goes wrong, we nave to start lasks Trom the beginning, and sometimes even start from scratch entirely. This is a huge bottleneck, more so than the number of machines/parallelism/performance. The biggest issues, as reported by its users, are around reliability
ana visionity.
Without visiblity, the application lacks transparency which makes it alicult to determine what the bulld is doing and where it breaks. As we have no transparency into the breakage, end users waste time in
trving to tind a fix which might solve the smotom but not the root cause. When something goes wrong. we have to start Tasks from the beginning, and sometimes even start from scratch entirely. Constant restarting limits speed more so than the number of machines, parallelism, or application performance.
by solving tor the root cause, data processing can run smootniy & rellably tor multiple Invocations ana
Improve overall periormance.
In summary, the desire is to make the build more transparent about what it is doing and where it breaks.
This will enable faster data fixes and re-processing and less guesswork and stress on the users running the pipeline.
Critical Known (and not currently documented) Issues
1. A tew olaces in the code use a custom log file rotation that is limited to 10 tiles or less. Each time that application starts a new file is created. Once the 10 file limit has been reached, the application cannot be run again until those files are moved, renamed or deleted.
a. Using a standara Logger see below will completely obviate this issue.
2. There are several places where file modification time is used to determine if some part of the pipeline is complete and shouldn't be run again. This is dangerous because an incomplete file
will be treatea as complete It lentover trom a crasn or talled processing. Inis can lead to corrupt or incomplete data being used by downstream processing.
3. In reviewing the code, there were a couple of instances of embedded loops where the Geometry
of every Parcel was being compared to the Geometry of every Point, which is very inefficient (ON"Mi. Inese tvoes of Inetticiencies can be vastiv improved using ditterent algorithms with better performance (ON*log(M)) or better) or only comparing Geometries that have a chance of intersecting by using the generated RTX Indexes. Or even better would be to use built-in
Database operators that can do geometric operations across multiple rows all at once. and thus shift the burden of many geo-spatial calculations from code to the DB.
Observability and Telemetry
Summary
• The absence of a standardized logging framework leads to a fragmented and inconsistent
centralize l0% management, apply consistent tormatung, ana Implement davanced logging
teatures.
• The lack of a unified logging methodology further exacerbates the inconsistency in logging
practices. Ditterent developers may adopt varying approaches to log levels, message tormats, and the types of intormation logged. This creates a disjointed log output that is challenging to
• The insufficiency of context and immediate feedback in log messages limits their diagnostic
value. Log entries often lack crucial contextual intormation, such as timestamos. thread IDs. or user identifiers, that would aid in troubleshooting and root cause analysis.
• The Ul's limited feedback capabilities further restrict the visibility and accessibility of log data.
Users may not have convenient access to comprenensive log intormation or the ability to tilter, search, and analyze logs effectively within the Ul. This hampers their ability to gain insights from log data and proactively identify potential problems.
• The lack of consistent timing information in log entries makes it difficult to track the sequence of
events and identitv pertormance bottlenecks. inconsistent or missing timestamos can obscure the chronological order of log messages and impede the correlation of events across different
system components.
      * ﻿﻿The reliance on console-based logging introduces the risk of losing log messages due to the ephemeral nature of console-based logging. This can result in the loss of critical diagnostic nformation and hinder the investigation of system failures or anomalies
      * ﻿﻿Furthermore, the absence of immediate feedback mechanisms, such as real-time log monitoring
These shortcomings collectivelv highlight the need tor a more robust. standardized. and comorehensive logging solution that addresses these limitations and provides enhanced log management, analysis, and diagnostic capabilities
Observations
The application faces several logging-related challenges:
• It lacks a logging framework and file-based logging, leading to instability and potential message loss due to reliance on console-based logging (stdout).
• Logging practices are inconsistent and unsafe, with a lack of consistent thread synchronization
ana contextual Intormation about message origin or processea data.
• The absence of immediate event notifications (like push notifications or alerts) outside the Ul results in the build process being unnecessarily long.
• The Ul offers limited feedback on build status and lacks consistent timing information, making it
ditticult to track process durations or start times. Historical data for analvzing previous overation completion times is also unavailable.
Impact
Goal, Criticality (1 is most impactful, 2 is medium, 3 is low impact) and Name in order of priority order (e.g. assume you have limited money)
Speed. Critical level 1: Detailed logging
• Logs and error messages that lack contextual information make troubleshooting slow
and difficult. Engineers need to know when and where errors occur to react quickly and ettectivelv. Currentiv. the lack of context impedes engineers and negativelv impacts the
speed of the data retresh
Speed. Critical level 1: Timing and Bott/necks in logging
• The ability to view timing data enhances an engineer's understanding of application
perrormance, allowing Tor easier laenutication or bottlenecks ana errors. Inis Is cruciall
for engineers, as it facilitates both debugging and pertormance optimization efforts.
peed. Critical level 1: Persistence in logging
• Troubleshooting and resolving issues will be vastly easier by having the ability to view
logs from previous iterations.
This also makes debugging failures and crashes possible, as without file-based loggi
re is no way to determine what caused a catastrophic failu
Speed. Critical level 1: Push based notifications
• Enhanced turnaround time: Messages are sent to all intended recipients, including the
simplied troublesnooting: Problems are easier to laentity and resoiv
• Real-time notifications: Users receive instant notifications Slack. Text. Email. PagerDut etc...) on events and task completions.
• Increased visibility: Multiple users can track application progress and status.
Speed. Critical level 2: Metrics
• Metrics allow for both detecting bottlenecks in a system, but also track the status and behavior of a running system.
• Metrics also provide historical trends, thus giving insight into how parts of the pipeline perform over time and possible performance impacts from other factors, like adding memory, faster disks, etc...
Speed. Critical level 3: Ease of use
• Centralized Log Management: It all logs were consolidated into a unified repository and
knowledge and significantly streamline the process of monitoring the status of various subtasks and applications
• Dynamic Log Configuration: The process of adjusting log levels, formatting, and specifying the intended audience for the logs would be simplified, allowing for greater
Tlexibility and control over log output.
Data Integrity
summary
The current Pipeline is frequently disrupted due to the lack of robust data validation. Data changes corruption, or schema changes often necessitate the operator to rerun the entire ParcelPoint dataset multiple times, as problems often go unnoticed until much later in the process. This is a commor occurrence, often due to significant changes in external datasets like USPS, Navteq, or Counties
geo-spatial data since the last run. When this happens. the engineer has to modity the code to support. the new schema(s) or data structures and then rerun the entire dataset. The ability to detect these issues before or between major stages of the pipeline run could significantly reduce wasted time spent
on rerunning entire counties.
The system could be made less brittle by validating data both before initializing the pipeline and between tasks, as this would allow some problems or errors to be caught earlier in the processing pipeline.
Observations
• Lack of clear, documented, requirements of what input datasets are needed ahead of time examples of this include: USPS, Navteq, but also Forest Service and Zip Code geo data and also
Includes metnoas tor now to acquire, prepare ana stage these datasets. some or the ooservec challenges include:
Manually downloading and staging these datasets from external sources is error prone
ana unincultive.
      * ﻿﻿|here are man custom scripts used to create these datasets. including extracting tromi he DB into file and many of these scripts are not tracked in source control which is not n accordance with CoreLogic SDLC standards.
      * ﻿﻿Lack of pre-validation of these external datasets
      * ﻿﻿It is unclear if there is a pre-validation step of the external data sets.
      * ﻿﻿If pre-validation is present it is unclear if they are complete and with the proper
schemas.
      * ﻿﻿It appears that most inputs are Shapefiles and that there is no existing validation or testing of them which leads to a lack of robust data validation between processing steps
      * ﻿﻿The manual creation and updating of the application configuration (Parcels.ini) every quarte results in multiple related contiguration parameters and paths which creates unnecessary complexity and invokes unnecessary operator error.
• While there is some scant and incomplete documentation, insights on how these steps
are perrormed is based on tribal Knowledge. Inese kinds of unique signis include examples such as how to manually create configuration files or knowing where to copy
Inis Includes previousty seen probiems in the upstream data
Unpredicted schema changes. IE: The format of input datasets has changed Changes in semantics or significant changes in the datasets
For example, County redistricting or changes to geo-political regions
• The navigation in the Ul is unclear and not intuitive. As an example, some Ul flags allow skipping certain steps in the process, but only in specific places, which are unintuitive to a user. These kinds of details require that the user be savvy about which flags need to be set, and which flags
need to be set based on specitic errors. Ine desire should be to have a better, more intuitive and less manual reentrancy process.
• In discussing these issues with CoreLogic personnel, there is a desire for more automation, QA,
ana aara checks on Initial ana Intermealate aala tnrougnout the pipeline.
General Benefits
• Confidence in accuracy in the data, as well as confidence that the process will function correctly.
Impact
We have added an additional category called Stability, which was not originally identified as a goal of this project but that has become apparent to Slower in its assessment of the PxPoint build process Stability has a primary benefit of a consistent process and a secondary benefit of improving process
speed
Speed. Critical level 1: Existence of initial datasets
• Ensure all the dataset where they need to be
Documentation of datasets and their locations
Checking to ensure all the required input data are in the correct locations
Ensure that all upstream data exists
Ensure each dataset is in the right place
• Ensure the dataser is readable
Ensure the dataset has the right naming convention, before any processing occurs
Verify the Navtea, USPS (for example) data is correct before processing it
Speed. Critical level 1: Accuracy of the initial datasets
• Ensure the dataset has the right format and schema Ensure the dataset has approximately the right size
Speed. Critical level Z: Accuracy of contiauration
• Verify the contents of the configuration are accurate
Verify all paths are consistent with each other
Speed. Critical level 3: Configuration creation
• The ability to generate correct config files rather than manually copy and update every
Speed. Critical level 3: Documentation?
How to run the application; step by step
Speed. Critical level 2: Intermediate integrity
Recommendations
Conduct data validation on all external datasets and configuration files using data validation programs or scripts. In the short term, we can use Python scripts or add validation checks to the C# application.
However, when we start using an Orcnestrator, data validation will simply be another node on the grapn and can be written in the optimal language or framework for the specific data type. This approach would also eliminate the dependency on Shapefiles.
Database
Kecommendations
• Speed. Critical level 1: Use a single connection across queries instead of repeatedly creating and
closing connections to the ub
• This can be a malor pertormance bottleneck. especiallv when in a loon or deen call-chaini
• This can also cause stability issues, as large amounts of connections opening and closing can cause the Database to degrade performance, and even reject connections.
•Speed. Critical level 2: Implement telemetry logging Tor Database interactions, speciticaliv tracking the duration of each SQL Statement. This would help to:
• Identify performance bottlenecks: By monitoring the time taken by each SQL statement,
siow queries can be easily pinpointea. Inis allows developers to optimize unose specia queries or the database schema itself to improve overall application performance
      * ﻿﻿Monitor system health: Tracking the duration of SQL calls over time can reveal trends
and anomalies. For example, a gradual increase in query times might indicate a growing issue with the database or the underiving intrastructure.
      * ﻿﻿Capacity planning: By understanding the typical and peak load on the database in terms of query durations, it's easier to plan for future capacity needs and ensure the system
can handle the expected workload
         * Troubleshooting: When issues arise, having telemetry data on SQL query durations can help diagnose the root cause. For instance, a sudden spike in query times might be linked to a specific event or change in the system.
         * ﻿﻿Speed. Critical level 2: Refactor code and queries such that sort, group and filter operations are
done in sul instead of in code
• This can greatly improve overall performance, and lower the memory and CPU footprint from having to do more work in code that is faster and less resource intensive in the DB.
         * ﻿﻿Speed. Critical level 2: Stream SQL results rather than loading the entire query result into memory.
This can cause both memory issues and DB read slowdowns.
         * ﻿﻿It is better to have a sorted data stream from the DB, and process it as individual rows or blocks of related data.
         * ﻿﻿This can only be done if sorting and filtering is done in the DB.
C++ Codebase
Summary
Most of these recommendations are around following the C++ standards, and reducing execution time by preventing unnecessary memory allocations or method calls. They also make the code more readable
and make debugging and maintaining the codebase beer, as you will nave way tewer warnings wnen compiling or running the application, which causes warning fatigue and can easily hide real problems.
There are also some overall safety and stability improvements.
Impact
• Speed. Critical level 1: The use of cout, print(stderr), and printr issue
These operators employ different output methods, some of which may be buffered,
leading to potential delavs and out-or-order messages.
o In multithreaded environments, messages from various threads can interleave, creating
a disorganized and garbled output.
• Additionally, redirecting the output of these methods to a file can be challenging.
            * ﻿﻿Therefore, a Standard Logger should be used in all instances to avoid these problems.
            * ﻿﻿Stability. Critical level 1: Throw and catch Exceptions by reference
Never unrow a pointer to an exception, out Insteda unrow oy rererence.
Also. catch bv reterence. rather than catching a vointer to an Excention.
This is important because most exception handlers do not handle exception pointers, and using "new" means many expected and nonfatal exceptions will not be caught and
handled which can potentiallv terminate the abolication
o In general, doing this improperly is a bad practice because this can cause memory leaks and create unnecessary compiler warnings which can obscure actual important
warnings
• Stability. Critical level 1: Use RAll for Mutexes instead of explicit lock() and unlock()
Use something like Locking:Mutex instead of calling lock() and unlock() directly
• This is important because locking a Mutex and failing to unlock it can cause race conditions. deadlocks and hard crashes.
• There are many places in the code where something is locked and then manually
unlocked before the end of the functior
            * This is dangerous because functions don't always make it to the end, as sometimes an
            * ﻿﻿RAll guarantees both the unlocking of a locked Mutex, but also will preserve the order of multible locked Mutexes
• Stability. Critical level 1: Use RAll for Files instead of explicit open) and closel)
Similar to the RAll Mutex recommendation above.
In this case. Tiles can occasionaliv be left open and thus vou get resource and memory
leaks.
Even worse, if you have a file open in windows, other processes wanting to access that
The cannot ao so, ana tnis can cause nara to alagnose probiems in otner parts or the
abolication. or even in other abolications
• Stability. Critical level 2: Make sure all File paths are OS neutral, rather than using Windows-specific path separators and drive names, etc..
• Otherwise migrating to a ditterent plattorm. like Linux. will not work
Most cloud-based platforms and engines assume a UNIX-like filesystem
No part of the pipeline can be migrated to a newer system or platform without this
•Speed. Critical level 2: Pass ov reterence when possible, even for smart pointers
• Avoids having to allocate unnecessary temporary objects
For smart pointers this is less critical, but does add unnecessary wasted cpu time
• Stability. Critical level 2: Use default destructors, constructors, and copy constructors wher possible.
This is superior to manually writing and maintaining redundant constructors and
destructors
• This includes using default assignment operators ("=")
This will make the code much smaller and easier to intuitively understand
• It also prevents some logical errors, where a manually written constructor can
accidentally torget to Initialize all variables, or a copy constructor talls to copy one or more values.
• Stability. Critical level 2: Use correct numerical types, and avoid implicit numerical casting
Inis Includes Tixing Implicit casts Trom signea<->unsignea or 04<-752-01 Integers.
• This can cause some rare. but brutaliv difficult to detect errors to occur
This is because, especially for large numbers, casting from signed to and from unsigned
can cause numerical wrapping, where a negative signed number becomes a huge positive unsigned number during conversion
• Stability. Critical level 2: Remove the boost library entirely
This library is completely obsolete and unsupported
• Mixing standara synchronization classes with boost synchronization classes can have unpredictable results, including crashes and deadlocks
• Removing it will reduce the complexity and size of the codebase
            * ﻿﻿Stability. Critical level 2: Use correct format when converting numbers to strings
            * ﻿﻿Use correct format indicators for numeric types (Vollu vs %d vs zu)
。。
But it does mean that numbers converted into strings might be formatted wrong, and thus contuse someone looking at the logs, as the number logged differs trom the actual
numper.
• Stability. Critical level 3: Use range-based loops rather than iterators
•Inis Is less critical, but can make the code much more Intuitive, prevent unnecessary allocation of temporaries, and make the code much more readable and intuitive.
            * It also makes the loop invariant easier to understand, and control. Especially if it should or should not be const (read-only)
            * ﻿﻿Prevents accidental misuse of iterators, where one compares an iterator to the beginning of the vector rather than the end, or compares to a different vector iterator entirely.
            * ﻿﻿IE, instead of doing this:
C/C++
tor vector<C1tventry>::const_1terator citvlt = c1tventries.bea1n):
citvit = c1tventries.end:
+十CitVI亡）《
const CityEntry& cityEntry = *cityIt;
• Instead. do this:
C/C++
for (const auto &cityEntry: cityEntries) {
• stability, Critical lever 3: Use Posix numeric types Instead or custom type.
The code overwhelmingly uses non-portable numeric types like int32 and uint64 and even ini
It is better to use Pusia dalatypes like Ints<_t, unts<_t ana size-
This is so endemic in the code, I don't recommend doing this everywhere all at once
• But using potentially incorrect macros it means the underlying datatypes may not match the type implied by the name
• The int32 macro maps to int but int can have 32 bits on some machines, 64 bits
on otners ana even 1o oits on olaer machines.
            * ﻿﻿Using the built in POS X tvoes guarantees variable alignment and prevents sneaky casts between incompatible datatypes, which the compiler has a harder time catching
            * ﻿﻿Using standard datatypes also means the code is portable and POSIX compliant, so migrating trom one architecture to another is wav easier
• The code base is smaller, because there are no custom datatype macros being defined
It can also improve performance as implicit casts can consume cpu cycles
            * ﻿﻿Speed. Critical level 3: Use vector.emplace back over vector.push back
            * ﻿﻿This prevents the allocation of unnecessary temporaries
Thus the code will perform faster and more efficiently.
            * ﻿﻿Speed. Critical level 2: Reserve memory when populating new vectors. IE: vector.reserve (size)
            * ﻿﻿If you know the size a vector is going to be before populating it, you can save many
            * ﻿﻿This is especially helpful for large vectors, which can reallocate large chunks of memory over and over as they grow to tit the values being put into them
• Speed. Critical level 3: Use std:make_shared) instead of manually creating a shared pointer
using new
This prevents the allocation of unnecessary temporaries.
Using new is implementation specific, and sometimes costly. std::make_shared uses a ditterent memorv allocator. which is more efficient.
• Stability. Critical level 2: Use std:: vector instead of native C arrays
This can prevent coding errors like buffer overruns and underruns.
• Looping over them is easier, sater and more intuitive (range-based loops, above)
• When a vector goes out of scope, all the things in the vector get destroyed as well.
Arrays do not do this, and so using dynamic arrays (rather than static) can cause memory
ana resource leaks
• Stability. Critical level 2: Avoid unsafe parsers like sscanf
sscanf is notorious for being unsafe, as it can easily cause buffer overflows or even
memory corruption
• Is recommended to use strinastream or strtol insteadi
• This is because sscanf cannot easily detect parsing failures and/or strings with trailing non-numerical characters
• IE: sscanf cannot easily detect invalid strings like "123abc"
• Stability. Critical level 2: Run any related C++ code through a modern linter (IE: Clang-Tidy)
This will catch many unexpected code problems and bugs, and if fixed, can prevent many
potential probiems Trom ever being In the code In the Tirst place
Also standardizes the code more to be more readable and efficient for the comoiler.
This can improve performance and error handling, and puts more work on the compiler
ana less on the engineer. Inus potentially using less engineering resources
Parcels.ini
File containing configuration values used throughout the build pipeline.
• | his tile is in INi tormat. and is required to exist in order for the pipeline to run.
• Default file: gdsbuilders/Parcels/Parce|APP/Parcels.ini
sections:
• PxPoint License and Root Directory
• PxUara airectories containing Usts ana Naviea data
PLayers files containing info about County, State and Zip shapes (See below)
ParcelParams miscellaneous flags and behavioral controls
BuilderParams values for the build process itself, including connecting to the DB
PLayers
Pxlavers is a section in the Parcels.ini File.
ReferenceShapesDfp
            * ﻿﻿Base directory for all other PLayer files and subdirectories
Contains Shapefiles of Coastlines, Municipal boundaries, Zip Code shapes, etc...
            * ﻿﻿Usually defined as C:| Parce|ReferenceData
Value is configured using Ul field "Layer Directory"
• Previously named Shape Directory
ReprocessDfp
• Directory containing generated Shapefiles for use in some Normalizations steps
C/rxrolntvatabulla/ 20z404/working/parcel/ normalize
• The GenerateFailureShapefiles Step writes Shapefiles to
C:/PxPointDataBuild/2024q4/working/parcel/normalize/parcel/<fips>_parcel.shp
• C:/PxPointDataBuild/2024q4/working/parcel/normalize/points/<fips>_points.shp
• The ParcelGroup Step (CabinInThe Woods) reads those files. and writes new files to
               * ﻿﻿C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<fips>_groups.shp
               * ﻿﻿The Renormalize Step reads files from that "groups" directory
• This directory also contains the ReprocessOverride file, described below
ZipPolygons
• Directory containing Shapefiles of Zip-Code-5 Polygons
• Usually detined as LIP. POlY
• Previously defined as USA ZIP5_polys_ 0110
ZipPoints
• Directory containing Shapetiles of Zio-Code-5 Poin
Usually defined as ZIP_POINT
Counties
• Directorv containing Shamefiles of Counties
• Usually defined as NT_County
Municipal
• Directory containing shapetiles of Municipalities
• Usually defined as PX Municipal
States
• Directorv containing Shamefiles of Us States
• Usually defined as STATES
ReprocessOverride
• File containing mappings from Parcel IDs to Addresses
• These values are used as overrides when Normalizing some Parcels which are otherwise hard to ormalize. These values are basically hard-coded Normalized Addresses for those Parcel:
• | nis Tile must be located In the kererencesnapesutp airectory
               * ﻿﻿Usually called something like
ReprocessOverride.txt
               * ﻿﻿And thus, the actual location is something like
               * ﻿﻿С:/PxPointDataBuild/2024q4/working/parcel/normalize/ReprocessOverride.txt
Coastlines
• Directory containing Shaperiles or coastlines
• Usually defined as COASTLINES
DebugOutputDtp
Debug uper trin our les one to Meres
Pxbata is a section in the Parcels.ini File.
It contains the following properties:
NavteqGdxFfp
                  * ﻿﻿Path to file containing the Navtea (HERE) dataset
                  * ﻿﻿С:/PxPointDataBuild/2024q4/dataset/navteq_us.gdx
UspsGdxFfp
: to file containin 02405/S tas postal service) dataset
C:/PxPointDataBuild/2024q4/dataset/usps_us.gdx
BuilderParams
(BuilderParams] is a section in the Parcels. ini File.
Not all properties are relevant for ParcelBuilderNew
It contains the following properties:
BuildDir
Dataoaseserver
• SOL Server Database Server
ParcelDB
• The Database on the SQL Server Database Server used to store Parcel data
Dataoaseusername / DB USEKNAME
• Username for connecting to the Database
DatabasePassword / DB_PASSWORD
• Password for connecting to the Database
rirstoold
• The first Job to process, as defined in the tbl _Jobs table
Jobs before this ID are skipped, and are usually Jobs that have already been completed
This is used by both pre-processing and normalization
LastJobld
                  * ﻿﻿The last Job to process, as defined in the tbl Jobs table
                  * ﻿﻿Jobs after this ID are skipped, and are usually Jobs that have already been completed :
Each Job refers to a single FIPS, but Job IDs are used to control the order of processing
This is used by both pre-processing and normalizatior
DatasetBuildCategory
• One ot: ParcelOniv. Gadberrv. Clip. Parcel. ParcelPlus. Structure. Structureplu:
• The ParcelBuilderNew Ul allows selecting Parcel or ParcelPlus only.
DancAlmoortrilerathi
containing DnA/Diablo/D&A data in pipe-separated CSV forr
C:/PxPointDataBuild/2024q4/VendorPrepared/DNA/Parcel_Point_Delivery_20240105
RawShapeDir
• Directory containing the Raw Parcel and Point Shapefiles, before being pre-processed
C:/PxPointDatabulld/202404/Vendorkaw/parce point/ Data/
• With Parcel and Point Subdirectories:
C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/parcel/
• cirxromntvalabulla/ 20<404/ venaorkawparcelpoint vara/points/
AddNavteaAreaNam
• DbfFilesDii
"<RefDataDir>/GlobalPlaces/"
: Dobapl
GlobalPlacesDir
RefDB
Paths
RefDataLocation - AKA RefDataDir
UspsDB
kepopulatestrings
PxPointDataLocationLogFileLocation
: SMantestour
StarMapOutputDir
• SpatialRecord ImportDircomoilestatistics
• StatQuarter
StarMapDatasetName
• StarMapDbserve
StarMapDbName
                     * ﻿﻿starmapvorassworc
                     * ﻿﻿USADirLoc
Steps
Descriotion.
Each Step controls whether some Task is enabled/disabled based upon configuration set through the Ul.
Git Location
Part of the ParcelBuilderNew C# application gaspullders/rarcels/ParcelbullderNew/Posteps.cs
List of Steps
• ResetNormalize - Does the database work required to re-set the normalization
Enabled via the "Reset Normalization" CheckBox in the Ul
• RebuildParcelDatabase - Deletes and rebuilds the entire parcel dataset
Enabled via the "Create Empty Parcel Database" CheckBox in the Ul
• CreateJobBlocks - Creates entries in tb Jobs using the contents of the parcel and point
manest.cov lles
Enabled via the "Create Jobs from PP Manifests" CheckBox in the Ul
• PrepareAndLoadParcels - Performs ParcelPoint preprocessing including blocking, borrowing, coast clipping, and promoting and loads database.
• Enabled via the "Prepare And Load Parcels" CheckBox in the UI
• LoadParcelsOnly - Performs ParcelPoint loads database Only
Enabled via the "Load Parcels Only" CheckBox in the Ul
UpdateJobBlocks -- Updates the JobParcel table using direct database inserts
Enabled via the "Update Job Blocks" CheckBox in the Ul
• ImportFromDandA - Takes the global D&A (Diablo) file and splits it into FIPS level files for
faster processing, also loading to database
• -nabled via the "'Import rrom D&A Into Counties" Checkbox in the U
• ImportFromSpatialRecord -- Ingests the spatial record table from global text files and updates
Extld and UnvPelld in raw parcels
• BuildRawParcelIndexes -- Build the raw parcel and other database indexes needed for normalize after finishing load
• BuildNormalizelndexes - Build the addresses and other database indexes needed for PXY load
aner completing normalize
                     * ﻿﻿NormalizeParcels - Normalize parcels using ParcelBuilderNormalizationStep.exe, which invokes
                     * ﻿﻿LoadPxyFiles -- Creates PXY files for each county, using ParcelBuilderLoadPxyStep. exe which
Invokes Parcel Load4G.exe per countv.
MergePxyBlocks - Merges individual PXY files to parcel_us.pxy using street indexer
IndexPxyFile - Create the parcel GDX file using street indexer
CreateAlusgax - Create All Us GUx Tlle using street indexei
BuildAdHocIndexes - Build the ad hoc indexes that we use for test queries against the
database
1.0 ParcelBuilderNew
Description
This is the top level application that is run to build all PxPoint datasets.
• Parses Parcel data, Geometries, addresses and other metadata from multiple sources, and
generates validated and moditied Parcel into with related data merged together.
• It does so by calling a series of sub-tasks and external applications.
This class is an Orchestrator that coordinates calls to other Tasks.
Git Location
• CH Application
gdsbuilders/Parcels/ParcelBuilderNew/ParcelBuilderMainWindow.xaml.cs
CLR Dependencies
• No direct CLR dependencies. But it calls other applications that may have CLR dependencies.
Inputs
• Parcels.INI file (
• Ul rielas and Checkboxes
• List of Steps, based on combinations of Check Boxes in the UI
                     * ﻿﻿Parcel and Point Shapefiles from ParcelPoint
                     * ﻿﻿S:/PxPointDataBuild/2024q4/dataset/PxPointReference.ref
                     * ﻿﻿Shapefiles containing Coastline boundaries
• Shaperiles containing county boundarie.
Shapefiles containing Zip Code boundaries
Shapefiles containing Municipal boundaries
• shaperiles containing state boundaries
Shapefiles containing Forest boundaries
Reprocess Override file
• nolrect) -COPA. Gadberrv
Outputs
Parcel file (large) and associated Parcel Index file, containing all validated and transformed Parcels and their metadata. Also various DB Tables populated with Parcel and Address data.
Logging
• Minimal logging, and does not log what Task is being executed.
All logging is into the Ul; nothing is written to any sort of log file or external logger.
Often when a message is displayed in the Ul, it is not cleared when another action is performed.
Ihis causes Talse positives tor the second action, that an error occurred when it did not.
Dependencies on SQL Server
Calls sub-tasks that access SOL Server.
Tables accessed:
• tb Jobs
tbl_DnaRecord
• tol spatialkecora
• tb_RawParcels
Details
1. ChecKUB
a. Described below
The 3 embedded checks normally do not pass when the application first starts.
This task is started, but never waited on to see if it completes. This is a race condition.
d. Should at least check if this step completes before running it again, as it is interleaved with other steps below.
2. LoadSettings
a. Load Parcels.INI File into global Settings
3. PopulateStepsVar
Creates a list of Steps based on Ul Settings and config files.
4. purgenormalizeavata
a. Described bes it,
Only executes if the ResetNormalize and/or RebuildParcelDatabase Step is enabled
5. createJobs
a. Described below
Only executes if the CreateJobBlocks Step is enabled
6. importFromDandA (Diablo)
a. Described below
Only executes if the ImportFromDandA Step is enabled
7. checkDB
Described below
Same method as before, called asynchronously.
This task is started, but never waited on to see if it completes. This is a race condition.
d. Major red flag here, as the result of this check is used by later steps.
Should wait for this task to complete before continuing.
o. preparerarcels
Described below
                     * ﻿﻿﻿Only executes if the PrepareAndLoadParcels and/or LoadParcelsOnly Step is enabled
                     * ﻿﻿﻿Calls ParcelBuilderPrepareParcelsStep.exe
9. IIn Parallel] importFromSpatialRecord
a. Describea below
b. Only executes if the ImportFromSpatialRecord Step is enabled
10. [In Parallel] rebuildRawindexes
Described below
b. Only executes if the BuildRawParcellndexes Step is enabled
11. [In Parallel] updateJobBlocks
Described below
b. Only executes if the UpdateJobBlocks Step is enabled
12. CalculateJobBlocks
Described below
. Executes after update/ob Blocks completes, independent of the other parallel jobs
13. normalizeParcels
ف ن
Described belov
Only executes if the NormalizeParcels Step is enabled
c. Executes after import-romspatialRecord, rebuildRawindexes and updateJobBlocks have completed.
14. rebuildNormalizelndexes
15. loaarxvriles
                     1. ﻿﻿﻿﻿pxyChecker. Check
                     2. ﻿﻿﻿﻿mergePxyFiles
1o. Indexrxyrlies
9. In Paralle | createAllUs — Create all us.gd
20. [In Parallel] rebuildAdHocIndexes - Build the ad hoc indexe:
Gaps
Insufficient logging and missing context for log messages. No Stack Traces in error logs.


Logs are all UI, as described above.


The UI flags are hard to decipher, and using them incorrectly can cause already processed data to be processed again, thus impacting run times significantly.


to be processed again. thus impacting run times signiticantiv.
• A failure in a FIPS will require a rebuild of all FIs for that same Task (usually)
The working/ directory is where all intermediate files are created, but also contains
external/ required Tles as well, which is very unintuitive.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Speed. Critical level 1: Add more context to log messages, including the name of the Task being
executed.
• Speed. Critical level 1: Add Stack Traces to error messages.
Speed. Critical level 1: Write outputs to a local logfile.
Speed. Critical level 1: Include the Task name in log messages to provide additional context.
Speed. Critical level 2: Prevent the logtile trom becoming too large by setting a maximum file size and adding file rotation
Other Recommendations: These are additional recommendations which would need to be added
AbUvt and beyony a standara logger.
                     * ﻿﻿Speed. Critical level 1: Resolve the race condition in checkDB.
                     * ﻿﻿Speed. Critical level 2: Avoid storing external files in the working/ directory.
1.1 PurgeNormalizedData
Description
Resets, or deletes, and then recreates all relevant DB Tables depending on Ul settings.
• This step is currently disabled in the Ul and all tables are created by manually running commands and scripts in SQL Server.
Git Location
ParcelStep in ParcelBuilderNew C# application
gdsbuilders/Parcels/ParcelBuilderNew/PurgeNormalizedData.cs
CLR Dependencies
None
Inputs
List of ster
DB connection info from global Setting
Outputs
Empty DB Tables and Views, with associated Foreign Keys
Logging
There is minimal logging, describing what SQL files are being executed.
No error messages are captured.
Errors from Sol calls are swallowed/ignored.
Dependencies on SQL Server
DB Views
• v PxvloadNulizips
• v_ PxyLoad
• NormalizationStat
Foreign Keys
• FK_tb|_Addresses_tbl_RawParcels
Fk_tbl_Addresses_tbl_Streets
FK_tbl_Addresses_tbl_StreetCollections
FK tbi Addresses thi Citvcollections
• FK_ tb _Links_ tb_ RawParcels
FK_tb|_StreetsStreetCollections_tb|_StreetCollections
rk tol streetsxstreetcollections tol street
FK_tb|_Cities_tb|_CityCollections
DB Tables
tol Addresses - Iruncated thi Cities - Truncatedi
tb_CityCollections - Truncated
tbl_DnaRecord
tol Jobs
tol Links
: tol RawParcel s- Updated
•All rows are marked as not-processed. and so will be loaded (again. in later steps
• tb_SpatialRecord
tb_StreetsStreetCollections - Truncated
tol streercollections - Iruncatea
• tbl_Streets - Truncated
Details
                     1. ﻿﻿﻿Only executes if the ResetNormalize and/or RebuildParcelDatabase Step is enabled.
                     2. ﻿﻿﻿If the ResetNormalize Step is enabled then certain DB Tables will be cleared of all data. Six Tables will be truncated and one will be updated; see above.
                     1. ﻿﻿﻿If the RebuildParcelDatabase Step is enabled then all referenced DB Tables, along with related Views and Foreign Keys, will be deleted and recreated instead of just cleared.
                     2. ﻿﻿﻿Lastly, shrink/truncate the DB Transaction Log, in order to keep it from growing too large. If this
fails, the error is ignored.
Gaps
• Missing context for log messages, and many errors are swallowed or ignored. Lack of visibility as to which SOL file is executing, which is especially painful when a failure occurs.
• some tallures are outright ignored. and no error is raised at alli
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Speed. Critical level 1: Add more context to log messages, including the name of the lask being
Speed. Critical level 1: Add Stack Traces to error messages.
speed. Critical level 1: write outputs to a local logrile
Speed. Critical level 1: Include the Task name in log messages to provide additional context.
Speed. Critical level 1: Incorporate the name of the SQL file that is being executed into the log
messages to provide daaltional context
• Soeed. Critical level 2: Prevent the logtile from becoming too large ov setting a maximum tile size and adding file rotation
                        * ﻿﻿Speed. Critical level 2: Ensure that errors are logged rather than ignored, and that the
                        * ﻿﻿Speed. Critical level 2: Implement telemetry for each SQL statement, specifically tracking the duration of each SQL Statement. This would help to:
Identify performance bottlenecks: By monitoring the time taken by each SQL statement, slow queries can be easily pinpointed. This allows developers to optimize those specific
queries or the database schema itself to improve overall application performance. ii. Monitor system health: Tracking the duration of SQL calls over time can reveal trends and anomalies. For example, a gradual increase in query times might indicate a growing
Issue win une aalabase or the underlying Intrastructure.
iii. Capacitv planning: By understanding the tvoical and beak load on the database in terms of query durations, it's easier to plan for future capacity needs and ensure the system can handle the expected workload.
I. Troubleshooting: When issues arise, naving telemetry data on sul query durations can help diagnose the root cause. For instance, a sudden spike in query times might be linked to a specific event or change in the system.
1.2 CreateJobs
Description
• Populate the Jobs table (tb| Jobs) with FIPS from Parcel and Point Manifest files.
Git Location
Parce Steo in Parcel BullderNew C# apolicationi
gdsbuilders/Parcels/ParcelBuilderNew/Create/obs.cs
CLR Dependencies
None
Inputs
• Jb connection into trom global settings
"Parcel Point Manifest Directory" as ManifestDir from global Settings parcel_manifest.csv file in the ManifestDir/
• point manifest.csv file in the ManifestDir/ Outputs
• Pooulated DB table: thi Jobs
Logging
Observation: Throws Exceptions if something fails, but doesn't correctly identify what file caused the error. Logging is minimal, and could be improved
• Throws Exceptions: Knowing that something failed is important, but not knowing the specific file
makes troubleshooting and tixing the issue ditticult and time-consuming.
• Logging is minimal: Improved logging would provide more information about the error, making it easier to identify the cause and resolve the problem.
Dependencies on SQL Server
• tb_Jobs
Details
1. Only executes if the CreateJobBlocks Step is enabled
L. Reads in the < CSv Manitest Tiles, points and parcels, each line containing
County name
.. count or recoras parcels or points
3. Sum the record-counts (marcel count + point count) for each FIPS. across both file
4. Write each to the DB Table tbl Jobs
The order in the DB Table is based on record-count, highest to lowest
5. Shrink/truncate the DB Transaction Log, to keep it from growing too large.
a. If this fails, the error is ignored.
Gaps
Logging is minimal, and could be improved.


No Telemetry around duration of SQL calls, or the Task overall


This will add duplicate rows for all FIPS if run more than once


• Correctly throws exceptions if something falls, but doesn't correctly identity what file caused the error.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will Inherentiv be adopted.
• Instrumentation/Audit Log. Critical level 1: Enhanced Debugging and Troubleshooting: Detailed
log messages with context, including SQL queries, enable quicker identification and resolution of issues during development and in production.
                           * ﻿﻿Speed. Critical level 1: Logs having timing information, especially around SQL statements, allows
for targeted optimization of database interactions, leading to faster and more efficient application pertormance, and the ability to diagnose query speed and identity bottlenecks.
                           * ﻿﻿Instrumentation/Audit Log. Critical level 1: Improved User Experience: More log messages
throughout task execution provides users with visibility into the process, reducing uncertainty and improving understanding of progress
: Speed Critical level t: Logging: Adi Stack Traces to eror messages.
Speed. Critical level 1: Logging: Write outputs to a local logfile.
• Speed. Critical level 1: Logging: Include the Task name in log messages to provide additional context.
• Speed. Critical level 2: Logging: Prevent the logfile from becoming too large by setting a
laximum file size and adding file rotatio
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger
• Stability. Critical level 2: Data Integrity: Preventing duplicate rows through database constraints
or upsert operations ensures accurate and reliable data within the application.
1.3 ImportFromDandA
Description
• Split Diablo (D&A) CSV file into smaller files, one per FIPS.
• Write that same data into the thi DnaRecord DB tab
Diablo is a CoreLogic internal product that has Tax dat
Git Location
• ParcelStep in ParcelBuilderNew C# application gdsbuilders/Parcels/ParcelBuilderNew/ImportFromDAndA.cs
CLR Dependencies
None
Inputs
Values from global Settings:
                              * ﻿﻿DandAlmportFilePath - CSV File containing DnA records
                              * ﻿﻿C:/PxPointDataBuild/2024q4/VendorPrepared/DNA/Parcel_Point _Delivery_20240105.txt
                              * ﻿﻿PreprocessDir - Output Directory for all pre-processed files
                              * ﻿﻿C:/PxPointDataBuild/2024q4/working/parcel/DNA/
                              * ﻿﻿DB connection into
Outputs
One CSV File per FIPS, grouped by State FIPS
                              * ﻿﻿State FIPS is the first 2 digits of the 5-digit FIPS code.
                              * ﻿﻿C:/PxPointDataBuild/2024q4/working/parcel/DNA/<state-fips>/
Populates DB table: tb|_DnaRecord
Logging
Logs exceptions if something fails, but not a stack-trace.
• Logged messages are minimal, but sufficient.
Dependencies on SQL Server
• tbl_DnaRecord
Details
1. Only executes if the ImportFromDandA Step is enabled
Create local export directory tor DNA (Diab o) Datal Open connection to DNA Database for SQL Queries
Truncate table tb|_DnaRecord; which is normally empty, except when rerun from failure
5. Read in Diablo/UNA (CSV rile located in DandAlmportrilerath (derined in settings)
a. Verifv it is a Parcel File and identifv its column headers and senarato
9. Load a list of FIPS from that file, and keep track of lines in that file for each FIPS
1. Create andrector fer ath ifse rey freachharacters of FIPS)
Create an output file in that directory for each FIPS
rle pain is sometning like.
.../working/parcel/preprocess/DNA/<FIPS-prefix>/<FIPS>
8. Copy lines associated with each FIPS to each FIPS file.
a. Each file will have the same Header, and CSV for a (still CSV), as-is, with heade
9. Also write each record to the tbl_DnaRecord table in SQL Serv
Loads CNIYco into the Unarios column
10. Re-index the tb_DnaRecord table
Gaps
: Lack or validation orthe imput sv data itself.
• No lelemetry around what Fips are being processed, and duration of each when processed
• No feedback of progress when loading. No idea if it is working, how long it has been going, or if it is stuck.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
                              * ﻿﻿Speed. Critical level 2: Logs having timing information, especially around SQL statements, allows for targeted optimization of database interactions, leading to faster and more efficient application performance, and the ability to diagnose query speed and identify bottlenecks.
                              * ﻿﻿Speed. Critical level 1: Adding Stack Traces to error messages will make it faster and easier to track down failures and diagnose problems. Thus improving turnaround time for data processing
• Speed. Critical level 1: Write outputs to a local logfile.
• Speed. Critical level 2: Prevent the logtile from becoming too large bv setting a maximum tile size and adding file rotation
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Instrumentation/Audit Log. Critical level 1: Enhanced Debugging and Troubleshooting: Detailed log messages with context, including which FIPS is currently being processed, will enable monitoring and tracking of the system's behavior and get insights into performance.
• Instrumentation/Audit Log. Critical level 2: Improved User Experience: More log messages throughout task execution provides users with visibility into the process, reducing uncertainty
• Speed. Critical level 1: Incorporate the name of the SQL file that is being executed into the log
messages to provide additional context.
• Speed. Critical level 2: Logging: Ensure that errors are logged rather than ignored, and that the application halts when errors occur.
Stability. Critical level 1: Add validation of the input CSV data itself, making sure it matches the
expected schema and isn't missing any data, eto
• This will help with stability, and prevent unnecessary execution of the pipeline. If the
data is bad or in an unsupported format or schema, that needs to be fixed before the pipeline is run
• This ties back to the Data Integrity goal in the Overview section.
1.4 CheckDB
Description
• Checks to make sure all needed DB Tables exist and are populated correctly.
Git Location
Task containing a group of C# Methods in the ParcelBuilderNew application. gdsbuilders/Parcels/ParcelBuilderNew/ParcelBuilderMainWindow.xaml.cs
CLR Dependencies
None
Inputs
DB connection info from global Settings
Outputs
one
Logging
There is no logging.
All feedback is through Ul Toasts/Messages.
Dependencies on SQL Server
• tbl_Jobs
• tb| DnaRecord.
tb|_SpatialRecord
tbl_RawParcels
Details
Called asynchronously when the application is loaded, before any steps start executing Each of these 3 checks will stop being run when they pass individually, otherwise that check will be run
again whenever checkob is called.
CheckJobs)
Verify there any rows in the tb _Jobs (Jobs) table
CheckDNAI
Verify there are a minimum 10 million rows in tbl_DnaRecord (DNA) table
CheckدoataRecoro
Verify that there are least 1 million rows in the tb_ SpatialRecord table, and 1 million rows in the tb|_RawParcels table with data in the UnvPclid column (IE: The row has Spatial data)
Gaps
There is no logging at all, only messages in the Ul.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Debugging and Troubleshooting: Log messages provide insights into the application's behavior, allowing CoreLogic to trace the execution flow
and identity the root cause or errors or unexpected benavior. ruli context (e.g., timestamps, variable values, relevant data) within the log messages aids in pinpointing the exact location and
• Instrumentation/Audit Log. Critical level 1: Monitoring and Maintenance: Log messages enable monitoring of the application's health and performance in real-time. By tracking key metrics and events through logs, administrators can proactively identify potential problems and optimize the
applications emiciency.
• Instrumentation/Audit Log, Critical level 1: Auditing and Securitv: Log to a file serves as ar audit trail, recording user activity, system events, and security-related incidents. Detailed logs with full context help in forensic investigations, compliance checks, and identifying unauthorized
access attempts.
                              * ﻿﻿Instrumentation/Audit Log. Critical level 1: Error Handling and Recovery: Well-structured log messages facilitate automated error handling and recovery mechanisms. By logging errors with
full context, the application can trigger appropriate actions, such as notifying administrators, retrying failed operations, or gracefully degrading functionality
                              * ﻿﻿Speed. Critical level 2: Prevent the logfile from becoming too large by setting a maximum file
size dna daams we rolduon
• Speed. Critical level 1: Adding Stack Iraces to error messages will make it taster and easier to track down failures and diagnose problems. Thus improving turnaround time for data processing overall.
1.5 PrepareParcels
Description
• ask that calls an external application. Parce BullderPrepareParce sstep.exe and waits tor it to
complete.
Git Location
Parcelstep in Parcelbulldernew C# application
gdsbuilders/Parcels/ParcelBuilderNew/ParcelBuilderMainWindow.xaml.cs
CLR Dependencies
None
Inputs
• List of Steps
Should be controlled by the LoadParcelsOnly Step, but isn't for some reason.
• Global Settings, many of which are passed into the downstream application.
Outputs
None
Logging
There is no logging.
But downstream applications will have side effects.
All teedback is through Ui loasts/Messages.
Dependencies on SQL Server
No direct dependencies, but calls an external application that does access the DB.
Details
• Only executes if the PrepareAndLoadParcels and/or LoadParcelsOnly Step is enabled.
• Inis Parcelstep simpiv wraps a call to Parcelbullderpreparerarcelsstep.exe
• That Executable does TONS of stuff with Shapefiles, and then loads the results into the DB.
Gaps
                                 * ﻿﻿Inere Is no Tile logging at all, only messages In the u
                                 * ﻿﻿This fails if run more than once, and no real indication of why. or how to fix it
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Incorporating comprehensive log messages with
contextual Intormation is essential tor enrecuive deougging, troublesnooting, and galning Insignus into system behavior, particularly when errors occur or unexpected situations arise
                                 * ﻿﻿Instrumentation/Audit Log. Critical level 1: Auditing and Security: Log to a file serves as an audit trail, recording user activity, system events, and security-related incidents. Detailed logs
with full context help in forensic investigations, compliance checks, and identifying unauthorized
access autempts.
                                 * ﻿﻿Instrumentation/Audit Log. Critical level 1: Error Handling and Recovery: Well-structured log
messages facilitate automated error handling and recovery mechanisms. By logging errors with full context, the application can trigger appropriate actions, such as notifying administrators, retrying failed operations, or gracefully degrading functionality
                                 * ﻿﻿Speed. Critical level 2: Prevent the logfile from becoming too large by setting a maximum file
size and adding tile rotation
• Speed. Critical level 1: Adding Stack Traces to error messages will make it faster and easier to
track down failures and diagnose problems. Thus improving turnaround time for data processin;
overall.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
Flexibility/Management. Critical level 1: Ensuring a solution is idempotent is crucial because it allows for the safe and predictable re-execution of operations, preventing unintended side effects or data corruption, especially in distributed systems or scenarios with potential for
different kinds of failures.
0 This means running this lask more than once doesn't compromise data integrity or
o It also means that no manual intervention is needed before executing this Task again.
1.5.1 ParcelBuilderPrepareParcelsStep.exe
Description
• Compares the number of Parcels and Points in the input Shapetiles to the output Shapetiles, by
FIPS, and calls ParcelPrepareParcels. exe for each unprocessed FIPS.
Git Location
C# Application
gdsbuilders/Parcels/Parce|BuilderPrepareParcelsStep/ParcelBuilderPrepareParcelsStepWindow.xaml.cs
CLR Dependencies
None
Inputs
INPUT_PARCEL_ROOT string - as inputParcelRoot
• Directory containing the Raw Parcel and Point Shapetiles
                                    * ﻿﻿C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/
                                    * ﻿﻿(looking for contents and schema data, sample records, description of the logic
going in and coming out)
• With Parcel and Point Subdirectories:
C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/parcel/
c:/rxrointUatabulla/202404/venaorkaw/parcelpoint/ vata/ points/
OUTPUT_PARCEL_ROOT string - as outputParcelRoot
• Directory to write the pre-processed Parcel and Point Shapefiles
C:/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/
• With Parcel and Point Subdirectories:
C:/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/parcel/
C:/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/points/
• DNA ROO| string
• Directory containing the parsed DnA files (created by importFromDandA, above)
C:/PxPointDataBuild/2024q4/working/parcel/DNA/
• Passed through to ParcelPrepareParcels.exe.
STARTJOB D int32
                                    * ﻿﻿The first Job to process, as defined in the tbl _Jobs table
                                    * ﻿﻿Jobs before this ID are skipped, and are usually Jobs that have already been completed Each Job refers to a single FIPS, but Job IDs are used to control the order of processing
                                    * ﻿﻿Same as FirstJobld in Settings
ENDJOBID int32
                                    * ﻿﻿The last Job to process, as defined in the tb _Jobs table
                                    * ﻿﻿Jobs after this ID are skipped, and are usually Jobs that have already been completed
                                    * ﻿﻿Each Job refers to a single FIPS, but Job IDs are used to control the order of processins
Same as LastJobld in Settings
COAs LINEs FIlt string
Path to the ShapeFile containing Coastline geometri ssed through to ParcelPrepareParcels.e
COUNTIES FILE string
Path to the ShapeFile containing County geomet sed through to ParcelPrepareParcels.
LICENSE_FILE string
Path to the License File
DB_SERVER string
Address of the SqlServer Database Server
Ob USERNAME string
DD PASSRO strue to access the SalServer Database
                                    * ﻿﻿Password used to access the SqlServer Database
                                    * ﻿﻿Number of parallel threads to run, to increase performance LOADONLY flag - based on the LoadParcelsOnly Step, but isn't used.
Outputs
Shapefiles, per FIPS, written to directory <outputParcelRoot>/
C:/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/parcel/<state-fips>/


C:/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/points/<state-fips>/


Logging


Many places errors are only sent to the UI (MessageBox) and not logged at all


When logged, only the Messages are logged, not StackTrace, nor context around the call


Some messages are logged to outputParcelRoot/log/00000_PrepareParcels.log


Logging
Dependencies on SQL Server
• tb|_RawParcels
• tol Jobs
Details
1. Calls Checkshaperiles
Calls StartFixer()
Starts multiple RunPrepareForFips) Tasks/Threads, one per incomplete FIPS
4. waits Tor all parallel lasks/ Inreads to complete
CheckShapefiles)
1. Get the list of valid FIPS from DB table tbl_Jobs
                                    1. ﻿﻿﻿Counts Parcels and Points, per FIPS, from Shapefiles in directory inputParcelRoot/
                                    2. ﻿﻿﻿Counts Parcels and Points, per FIPS, from Shapefiles in directory outputParcelRoot/ Counts total rows, per FIPS, from DB table tb_ RawParcels
Calculates Status based on whether input counts match output counts, so we know which FIPS have been processed alreadv and which have not.
6. This step returns Jobs that still need to be processed.
StartFixer()
1. Sorts unorocessed FIPS ov size (Parcels + Points)
Asynchronously fire off parallel Tasks for each FIPS
a. Limit parallel tasks by threadCount
b. Some with large size FIPS and others with small size FIPS
This is done to optimize parallelism and manage memory footprint
This may or may not be an ideal optimization
3. Call RunPrepareForFips) for each FIPS in parallel (limited by thread Count)
4. Wait for each Task to finish, and fire off new Tasks when one completes, until all are complete
5. Does not log StackTrace on failure, just the Error Message; This should be changed RunPrepareForFips)
1. Prepares CLI arguments for a single FIPs
Calls external executable with those arguments: ParcelPrepareParcels.exe
                                       1. ﻿﻿﻿Waits for it to complete, or until a specified timeout
                                       2. ﻿﻿﻿If it reaches a timeout. it forciblv kills/terminates the underlving process (EXE call)
5. Logs the result, and returns success or error to the upstream caller => StartFixer ()
Gaps
                                       * ﻿﻿Ul Flags seem unintuitive and Incomplete with regaras to the need to start trom aerent place. in the pipeline. This may not be a huge concern initially, but should be seriously looked at later
                                       * ﻿﻿Logging is extremely limited, and often lacks context for what is actuallv happening. trror
• Input arguments are passed in a non-standard way. Instead of using Parcels.INI, it expects all
parameters to be passea In Inaiviaually. Inis may not de worn tixing
• Many errors only go to the C# WPF Ul, not the Log file. And since this application is run by ParcelBuilderNew, this is clunky and can potentially make communicating errors back to the user
very amicult or even Impossible
Many messages go to Standard Out, which can lose messages if the Application terminates immediately the message was generated (synch buffer)


Takes in Parcel Shapefiles and Diablo CSV Files, but those files may or may contain errors or unexpected fields and properties.
                                       * This is one of the places where having corrupt or incompatible data from external systems can cause problems
                                       * ﻿﻿It is also where errors normally start to happen, as the subprocesses called are doing complex geospatial calculations and address comparisons for the first time.
                                       * ﻿﻿There are sometimes problems with parallel access to Windows directories, where one FIPS will succeed being processed. often after a retrv. and another FIPS will fail entirelv. But it isn't clear what went wrong from the logs. The logs say that the (failing) FIPS promoted 0 parcels in the PromoteParcels step, but no output Shapefiles were created. It is likely a Windows filesystem locking issue, where another thread is locking the directory.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Add context to all trrors messages, and make sure they all go to log files as well as Standard Out. Failures here mean some FIPS failed to process, ind without context and stack-trace information, diagnosing and fixing the problem is way more
alicult. Ana thus take much more time to reproauce and tix.
• Instrumentation/Audit Log. Critical level 1: Auditing and Security: Log to a file serves as ar udit trail, recording user activity, system events, and security-related incidents. Detailed logs with full context help in forensic investigations, compliance checks, and identifying unauthorized
access attemots.
• Instrumentation/Audit Log. Critical level 1: Error Handling and Recovery: Well-structured log nessages facilitate automated error handling and recovery mechanisms. By logging errors with
tuli context. the application can trigger appropriate actions. such as notitving administrators. retrying failed operations, or gracefully degrading functionality
• Speed. Critical level 2: Prevent the logfile from becoming too large by setting a maximum file
size ana adaing the rotation
• Speed. Critical level 1: Adding Stack Traces to error messages will make it faster and easier to track down failures and diagnose problems. Thus improving turnaround time for data processing
overall.
Other Recommendations: These are additional recommendations which would need to be added
AbOVE and BLYOND a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each
message, including what riPs, subtask, tile and data it is processing.
Still show messages in the Ul, but make sure they go to log files as well.
• Stability. Critical level 2: Logs having timing information, especially around major loops or
known siow operations, allows tor insignts into pertormance and bottlenecks, and nelps tocus energy on the worst pertorming parts of the system. Timing intormation comes automatically with a Standard Logger, but more log messages will need to be added around each SQL statement
• Flexibility/Management. Critical level 1: As should be done earlier in the process. make sure alli input files and DB used are pre-validated before even starting this Task. This is where these external files are first really used, and so is where pre-validation would help the most to prevent
Tallures ana wasting time processing Invalla data
• Flexibility/Management. Critical level 3: Have the LoadParcelsOnly Step function as expected,
or document its obsolescence
This Step is deceiving, and needs to be audited.
1.5.1.1 ParcelPrepareParcels.exe
Description
• Sub-orchestrator that calls a linear series of other EXE programs for a specific FIPS. Calling of
each Ext is controlled bv combinations of innut tlags.
Git Location
C# Application
gdsbuilders/Parcels/ParcelPrepareParcels/ParcelPrepareParcelsMain.cs
CLR Dependencies
lone
Inputs
SRC_DIRECTORY
                                       * ﻿﻿Directory containing the Raw Parcel and Point Shapefiles
                                       * ﻿﻿Same as the RawShapeDir in Settings
                                       * ﻿﻿C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/
• With Parcel and Point Subdirectories:
• C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/parcel, C:/PxPointDataBuild/2024q4/VendorRaw/parcelpoint/Data/points
• With Parcel and Point Subdirectories:
/PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/parc /PxPointDataBuild/2024q4/working/parcel/preprocess/prepared/poin
DNA DIRECTORY
Directory containing the parsed DnA files (created by importFromDandA, above
C:/PxPointDataBuild/2024q4/working/parcel/DNA
FIPS
• The 5-digit FIPS (County) being processed
COASTLINES_ FILE
Path to the ShapeFile containing Coastline geometries
COUNTIES FILE
• Path to the ShapeFile containing County geometries
LICENSE_FILE
• Path to the License File
DB_SERVER
• Address of the SqlServer Database Server
PARCEL_DB
• Name of the Database on the SqlServer Database Server
DB_USERNAME
Username used to access the SqlServer Database
DB PASSWORD string
• Password used to access the SqlServer Database
AUTORUN
• kuns without user inpu
• Always set when called from ParcelBuilderPrepareParcelsStep.exe
AUTOCLOSE
                                       * ﻿﻿Closes automatically when finished
                                       * ﻿﻿Always set when called from ParcelBuilderPrepareParcelsStep. exe
NOCOTERMINOUS
Disables a feature if set
• Always set when called from ParcelBuilderPrepareParcelsStep.exe
LOADDATABASE
• Always set when called from ParcelBuilderPrepareParcelsStep.exe
NORIX
• Disables a feature if se
Never set when called from ParcelBuilderPrepareParcelsStep.exe
NOBLOCKID
• Disables a feature if set
Never set when called from ParcelBuilderPrepareParcelsStep.exe
NOAPNBORROW
• Disables a teature it set
Never set when called from ParcelBuilderPrepareParcelsStep.exe
VOPOINTPROMOTE
• Disables a teature It set
• Never set when called from Parce|BuilderPrepareParcelsStep.exc
VOCLIPCOASTLINESLOADONLY
• Disables a feature if set
• Never set when called from ParcelBuilderPrepareParcelsStep.exe
LOADONLY flag
• Based on the LoadParcelsOnly Step, but isn't used
Outputs
Parcel and Point files in the DST_DIRECTORY
Logging
Most executions log a message when starting, and when completed along with whether it succeeded o ailed. No stack-trace or input context is logged.
IE: If a parcel, point, coastline, DnA, or other file is missing, it doesn't explicitly log which file
Logging is made to a FIPS-specific file in a log directory under the Destination directory as one of:
• SDST_ DIRECTORY>/log/<FIPS>.log
• <DST DIRECTORY>/og/<FIPS>.<X>. 0
he log file name is "<FIPS>.log" for the first attempt. But if the application fails for the given FIPS, a ndex from [1-9) is added to the file name as "«X>" to create a unique log file name. The index i: incremented every attempt to call this application for the given FIPS, and if 10 files are created, the application cannot be run again, at least until those files are moved, renamed or deleted.
Dependencies on SQL Server
No direct DB access
Details
Call a series of binary executables (EXE), and pass the Shapefiles output from each execution to the next executable in the series. The output of this overall Task is the output of the final executable in the series Each Executable is run with a 12 hour timeout, and if any of them timeout, the entire workflow, for that particular FIPS, immediately halts with an error.
1. CreateLogFile() - Creates a logfile to write log messages to, in the DST_DIRECTORY
Not an Executable, but just code that sets up logging
Z. kunktxProcessinWorkero - Calls Bulldktx.exe
a. Executes BuildRtx.exe twice, once for Parcel file and once for Point files
3. RunPromoteProcessinWorker() - Calls PromoteParcels.exe
•kunblocklarrocessinworker - calls rarcelblockoenerator.exe
5. RunApnBorrowProcessinWorker)
Perform "full" APN matching to FIPS, including both Basic matching and fuzzy matching
b. If matched, will call BorrowParcelAddresses.exe
                                       1. ﻿﻿﻿RunCoastlineClippingProcessInWorker() - Calls ClipParcelsToCoastline.exe
                                       2. ﻿﻿﻿RunDatabaseLoadProcessinWorker()
a. For each Shapefile, call ParcelLoadDatabase.exe
b. There should be a single Parcel. and multible Point tiles
Important: It is executed per file, not per FIPS (which is a group of Parcel+Point files)
3. Output of final processing is a directory containing processed Shapefiles and indexes
Gaps
It is awkward and difficult to determine what tasks will be run or not, based on the flags giver
• Setting one of the "NO*" flags will cause the associated task to be skipped.
• Inus setting NocLifeuAsI LiNes will cause Kuncoastlineclppingrrocessinorker to oe
skipped.
• Should be reentrant for each FIPS, but during testing, re-running a successfully loaded FIPS
throws a "Layer already exists" error
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Incorporating comprehensive log messages with contextual information is essential for effective debugging, troubleshooting, and gaining insights
• Instrumentation/Audit Log. Critical level 1: Auditing and Security: Log to a file serves as a audit trail. recording user activitv. svstem events. and securitv-related incidents. Detailed logs with full context help in forensic investigations, compliance checks, and identifying unauthorized
access attempts.
• Instrumentation/Audit Log. Critical level 1: Error Handling and Recoverv: Well-structured log messages facilitate automated error handling and recovery mechanisms. By logging errors with full context, the application can trigger appropriate actions, such as notifying administrators,
retrving talled operations. or gracetuliv degrading tunctionalitv.
• Speed. Critical level 2: Prevent the logfile from becoming too large by setting a maximum file size and adding file rotation
• Speea. Critical level 1: Adaing Stack Iraces to error messages will make It Taster and easier to track down failures and diagnose problems. Thus improving turnaround time for data processing
• Stability. Critical level 1: Currently, the application will not run at all if 10 attempts have beer
nade for the given FIPS. This is a huge red flag. because if vou don't understand this limitatio you cannot know what steps need to be taken to "fix" it so it can be run again and will be completely blocked from re-running the application
• Using a Standard Logger will completely tix this issue.
• This allows the removal of all custom logging code, which reduces complexity of the codebase.
This also makes logging easier and more stable, and completely eliminates the file-count limitation discussed above.
1.5.1.1.1 BuildRtx.exe
Description
Git Location
C++ Application
• Generates Spatial Indexes (RTX, DBF, etc...) files for all Shapefiles in a specified directory.
lools/oullarx/oullarx.cog
CLR Dependencies
Uses Px Libraries, but is a C++ application, so CLR is not used.
Inputs
• A single directory, which contains shaperies
Some optional flags.
Flags of note:
• force - Force recreation of an index even if one already exists.
• Otherwise, existing RTX files will not be reconstructed
OBC - Perform OGC compliant normalization (Open Geospatial Consortiun
nunreaas - Number or parallel threaas to execut
• Each thread can process a single Shavetile at a time
Outputs
Spatial Index (RTX, DBF, etc...) files in the same directory as the input Shapefiles.
Generated files:
• <file_prefix>.rtx
Soatial Index file
                                       * ﻿﻿file _prefix›.shp
                                       * ﻿﻿Copy of the original Shapefile
• <tile orefix>.sho.xmil
XML Manifest of all related files with the ‹file> prefix
• [possibly) ‹file_prefix>.cpg
possibiv stile prerix>.dor
[possibly] <file_prefix>.dbfl [possibly] file_prefix>.cdx
• _possibly stile prerix>.cax lpossibly] <tile_prefix>.fpt
(possibly) ‹file _prefix>.fptl
[possibly] <file_prefix>.dbt lpossibly] <tile_prefix>.dbt
Logging
Almost none.
Exceptions thrown ao have aecent messages.
Dependencies on SQL Server
No direct Db access
Details
:
Generates Spatial Indexes (RTX, DBF, etc...) files for all Shapefiles in a specified directory.
Depending on flags, some files may be rebuilt if they exist, and some files may be configured to not be generated at alli
• It's not clear or intuitive what files are being generated
• It also modifies/fixes/corrects existing geometries as well. This is not intuitive
Kecommencations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherentiv be adooted.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each
message, including what riPs, subtask, tile and data it is processing.
• Instrumentation/Audit Log. Critical level 1: Add context to all Error messages.
Other Recommendations: These are additional recommendations which would need to be added
ABOV- and BLYOND a standard loggen
• Stability, Instrumentation/Audit Log. Critical level 2: Better document the files generated, and what they mean. This doesn't necessarily improve direct performance, but does help in
alagnosing potential problems, and training people on now to use the sorware oy
understanding the structure and purbose of the various files.
1.5.1.1.2 PromoteParcels.exe
Description
• "Promote" parcel address from point record to polygon record when there is a single point
contained in the polygon, and all points within some distance to the parcel polygon are accounted for. Useful when a Polygon Parcel has no address, but the intersecting Point does.
Git Location
C++ Application
gdsbuilders/Parcels/PromoteParcels/aqa
CLR Dependencies
Uses Px Libraries, but is a C++ application, so CLR is not used.
Inputs
outputLayerPath - Path to locate output shapefile results
Usually a subdirectory in the upper level destination directory.
• IE: <parcelBuilderNew-destDir>/prepared/parcel/promotion/...
• Some ontional flags
Flags of note:
                                       * ﻿﻿force/f - ???
The caller explicitly enables this flag
                                       * ﻿﻿db - ???
The caller explicitly enables this flag
                                       * ﻿﻿numThreads - Number of batch processing threads
Defaults to 4
• bufferSize - Distance (meters) in which all points must be accounted to consider parcel
candidate
Defaults to 50
• maxCount - Maximum number of parcels to process (-1 for all)
Defaults to -1
• streetTag - Street tag to filter address (for testing)
• verbose - More verbose output
Outputs
Modified Shapefiles in subdirectories under the outputLayerPath directory
Parcels and Points are written to State-deleneated sub-directories.
A State FIPS Code is the first 2 characters of a 5-letter County FIPS code.
<outputLayerPath>/parcel/<State-FIPS>/<FIPS>_parcel.shp
Points:
<outputLayerPath>/points/<State-FIPS>/<FIPS>_point.shp
Logging
Almost none
Exceptions thrown do have decent messages.
Dependencies on SQL Server
No direct DB access
Details
Walks through all Polygonal Parcels and compares all Point Parcels. If a Polygon is intersected by exactly
one Point, and no other points do so or share Address or owner data, that Point is Promoted to the Polygon. The resulting parcel layer will have Address, APN, APN2, and Owner from the point record for promoted parcels. All records will have a new PROMOTED attribute:
1. U = not a canalaate tor promotion
2. 1 = promoted; parcel has no address, and point and parcel APNs and owners are compatible
"Compatible" means they are equal, or is missing from point and/or parcel
3. 2 = point and parcel addresses are equal
4. 3 = point and parcel addresses ditter
                                          1. ﻿﻿﻿4 = point and parcel APNs differ
                                          2. ﻿﻿﻿5 = point and parcel owners differ
Owner names aren't deterministic across parcels/points, so the same owner might have different names. but only slightly different.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherentlv be adooted.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each message, including what FIPS, Subtask, file and data it is processing.
• Still show messages in the Ul, but make sure they go to log files as well.
Instrumentation/Audit Log. Critical level 1: Log a message when exceptions are thrown.


Currently some exceptions don't get logged, or they get logged without context. This makes diagnosing the cause of an error way more difficult, and thus will take much more time to fix.


Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Speed, Stability. Critical level 3: This application iterates over every point for every parcel, which is a very inefficient operation (OM*N)). Some sort of Geo-Database or similar abstraction night significantly improve performance. Or multi-thread across Parcels, use Spark or similar, so nore than once can be processed at a time.
1.5.1.1.3 ParcelBlockGenerator.exe
Description
Organize parcels into groups (city "blocks") based on contiguous boundaries. And also does "stacking," for overlapping polygons (same polygon for multiple parcels, like MDUs).
Git Location
C++ Application
gdsbuilders/Parcels/ParcelBlockGenerator/ParcelBlockGeneratorMain.cop
CLR Dependencies
Uses Px Libraries, but is a C++ application, so CLR is not used.
Inputs
tco - State/county code to process (5 digit number / FIPs
inputDir - Directory containing subdirectories/files organized by county FIPS cod
outputDir — Output Directory
• USUALLY IE: <parcelBuilder New-destDir>/prepared/parcel/333/...
•some optional tags:
Flags of note:
                                          * ﻿﻿force / f - Force regeneration of Blocks even if the associated output file already exists.
                                          * ﻿﻿The caller explicitly enables this flas
                                          * ﻿﻿numThreads - Number of batch processing threads (1 - 64)
• Verault IS 41
                                          * ﻿﻿maxSeparation - Threshold distance (meters) for grouping potential unit geometries together.
                                          * ﻿﻿maxParcelSize - Maximum size of a parcel to be grouped.
• Default is 99991
                                          * ﻿﻿minParcelsPerBlock - Minimum number of parcels allowed in a block.
                                          * ﻿﻿Default is 2
• minCompactness - Minimum compactness value (area/perimeter squared) for parcels to be grouped
Default is 0.02
• maxBlockDiagonal - Maximum diagonal span of any block, in meters.
• Default is 1000
• verbose - More verbose output
Outputs
Multiple Shapefiles in the outputDir/<stco>/ directory.
• outputDir/<stco>/parcel/<FIPS>_parcel.shp outputDir/<stco>/parcel/<FIPS>_blocks.shp outputDir/<stco>/parcel/<FIPS>-_stacks.shp outputDir/<stco>/points/<FIPS>_points.shp
Parcel and Point files are conied as-is
Logging
Logging is all to stdout, and no log files are generated
Logging indicates when a FIPS Worker starts and stops, along with what FIPS is being processed, but not
Uses "cout without proper locking/synchronization inside of parallel threads.
Dependencies on SQL Server
No direct Ub access
Details
un each FIPS in parallel, generating "blocks" and "stacks" for eac
hecks if blocks have already been generated or not. Skip re-generating them unless the force flag wa Will generate RTX files as well, if they are missing. Will try to do this multiple times, which is redundant and inefficient.
1. Generates a list of neighbors and large-parcel polygons by scanning through the input parcels
a. Large-parcel polygons are those that exceed the size threshold
2. This pre-filtering step also removes those that have too low a compactness, as these are typically
not neignborhood type parcels
                                          1. ﻿﻿﻿The large parcels are then classified, and any that appear to be "stacked" are held over for later
                                          2. ﻿﻿﻿For each parcel that is small enough, find all of its neighbors, and consolidate them into neighbor groups
                                          3. ﻿﻿﻿Union the geometry for each neighbor group (block) and filter out blocks that don't have enough
rEcorus
Calculate the diagonals for each union,and calculate the points within that geometry
Write out the Parcels, Points, Blocks and Stacks
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Stability. Critical level 1: Use consistent loggers, and synchronize access to stdout (cout) so multiple threads don't corrupt the log output.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each message, including what FIPS, Subtask, file and data it is processing.
• Stabilitv. Critical level 1: Add context to all Error messages
• Stability. Critical level 3: Since this application is called once for each FIPS, all the parallelization
and multi-threaded code is redundant and unused.
1.5.1.1.4 BorrowParcelAddresses.exe
Description
Merge DnA/Diablo Address data into the list of Parcels and Points, based on APN matching and other types of validation.
Git Location
C++ Application
gdsbuilders/Parcels/BorrowParcelAddresses/BorrowParcelAddressesMain.cpp
CLR Dependencies
Uses Px Libraries, but is a C++ application, so CLR is not used.
Inputs
stco - State/county code to process (5 digit number / Fl
utDir - Directory containing subdirectories/files organized by county FIPS cr outputDir - Output Directory
dnaDir - Directory containing the DnA (Diablo) Files
                                          * ﻿﻿force / f - Force processing, even if the associated output files already exist
The caller explicitly enables this flag
                                          * ﻿﻿apnMatchParams - Pipe-separated values used to match APNs
• The column in a DnA Record containing the Parcel ID
The column in a DnA Record containing the APN
o Prefix and suffix values to strip off when doing APN matching
• verbose — More verbose output
Outputs
Multiple Shapefiles in the outputDir/<stco>/ directory. outputdir/<stco>/parcel/<rirs>_parcel.s
outnutDir/<stco>/points/<rils> polnts.s
The output Parcels and Points should have the same number of records, but each record might have additional data from DnA.
Logging
No log files, and all output is using cout. This has synchronization risks for multithreaded usage, but since
we don't use it multithreaded. it mav be a non-issue.
The log messages are decent, though often lack context (FIPS code). This may be sufficient, depending on how the calling application handles these messages.
Excentions thrown have decent messages.
Dependencies on SQL Server
No direct Ob access
Details
Runs for a single FIPS (County)
Checks it output tiles have already been generated or not, and skips re-generating them unless the force flag was set. This determination is based on file modification time (which is a problem, see below).
If any input Parcel, Point or DnA file doesn't match their expected format, then the associated FIPS
worker Ihread emits a simple warning message and stoos immediatelv. and no output tiles are created for that FIPS.
1. Create a mapping of APN to parcel and/or point properties:
                                             * ﻿﻿﻿Irack Farcel or rolnt Il
                                             * ﻿﻿﻿Track Address
Track Block ID and size
&. Trer But A Ns wist have multiple Parcels/Points whose Blocks don't match
Track Parcel Group ID and size
Filter out APNs which have multiple Parcels/Points whose Parcel Groups don't match
Track Geometry centroid and extents (if it is a parcel)
n. Merge geometries it parcel geometries are close together
Recalculate middle centroid if parcels are updated
Filter out APNs whose Parcel geometries are too far apart, and cannot be merged
2. Load the DnA file with the associated State FIPS
                                             * ﻿﻿﻿If a DnA record has a matching APN with any parcel or point in the above mapping, then
                                             * ﻿﻿﻿This is the critical part of this application: Having APN records "Borrow" DnA addresses.
3. Write out the Parcels with uodated Dna addresses
4. Write out Points with lots of additional DnA data. This is a bit confusing, as so much DnA data goes into Points but not Parcels.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherentiv be adopted.
• Stability. Critical level 1: Use consistent loggers, and synchronize access to stdout (cout) so
multiple threads don't corrupt the log output.
Other Recommendations: Inese are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages, and more context to each message, including what FIPS, Subtask, file or data it is processing.
: inste entail level it toge tritical livet on sce cont exte for each es al es.
Speed. Critical level 3: Since this application is called once for each FIPS, all the parallelization
and multi-threaded code is redundant and unused. Ine multithreaded code doesn t need to be removed immediately, but is a good candidate for a refactor which could improve performance and reduce complexity.
• Stability. Critical level 1: Instead or a ris worker lust stopping when it detects tile corruption or other error, should the application fail and return an error instead? How does the upstream application know what FIPS worked and which did not, except for looking for the output files?
Inis benavior is que to errors being Ignorea ana not propagatea, so tnis oenavior neeas
to be audited to determine what the correct behavior should be.
This happens after the Promote step, so addresses from D&A won't be available for Promotion. Maybe re-order these steps?
                                             * ﻿﻿Stability. Critical level 1: Using Tile moditication time to determine it a tle has been generated or not is dangerous because an incomplete file will be treated as complete, even if the application stops or crashes while writing the file.
                                             * ﻿﻿This is a Known Issue as described earlier in this document
1.5.1.1.5 ClipParcelsToCoastline.exe
Description
or a specified range of FIPS (County) codes, clip parcel polygons to the coastline of that County he geometry is clipped only where it crosses the mask boundary
Thus, if a parcel is wholly on land or completely in the water, it is not clipped.
For non-coastal polygons, it computes the "Bob Magic" centroid ignoring handles.
All polygons are passed through directly, and the centroid updated in the dbf file.
Only Parcels are modified, so Point Shapefiles are passed through unchanged
Git Location
（++ Appllcatlon
gdsbuilders/Parcels/ClipParcelsToCoastline/ClipParcelsToCoastline.cpp
CLR Dependencies
Uses Px Libraries, but is a C++ application, so CLR is not used.
Inputs
                                             * ﻿﻿fips ‹start-fips> (<end-fips>] - 5 digit FIPS code range, or a single FIPS code
                                             * ﻿﻿ParcelBuildParcels only ever passes a single FIPS
• in parcels - Directory containing subdirectories/files organized by county FIPS code
out_ trimmed - Output Directory
• In coast — Shavetile file containing coastline bounds
                                             * ﻿﻿This file is created/maintained external to this build process
                                             * ﻿﻿in_counties - Shapefile file containing county bound:
•This file is created/maintained external to this build process
• type - What kind of processing to do. Must be one of:
by_fips|coast|geo_clip/n_cnty/allpolys
ParcelBuildParcels always sends allpolys, which means it does full coastline clipping
• The column in a DnA Record containing the APN
                                             * ﻿﻿print_level - pseudo Log Level, but numeric.
                                             * ﻿﻿Usually set to level 10, which is basically DEBUG, and logs everything.
Outputs
Parcel shaperiles in the out trimmed/<tips>/ directory.
• outputDir/<fips>/parcel/<FIPS>_parcel.shp
                                             * Parcels are passed through unchanged, except for their centroid
                                             * ﻿﻿outputDir/<fips>/parcel/<FIPS>_parcel_clipped.shp
• All Parcels are written to this Tlle, each with potentially transtormed geometry and
centrola.
This outputs 2 Parcel files, one with the original geometries and one with potentially modified/clipped geometries. Each contains the potentially modified centroid.
Logging
'he application mixes cout with writing to stderr. This can cause issues, where messages can arrive out f order or garbled. This is because writing to stdout (using cout) buffers temporarily, while stderr writes
Immedlately, causing messages to arrive out or order In some cases
This has synchronization risks for multithreaded usage, but since we don't use it multithreaded, it may The log messages are decent, though often lack context (FIPS code). This may be sufficient, depending on how the calling application handles these messages.
Dependencies on SQL Server
No alrect Ub access
Details
1. For the ParcelBuilderNew pipeline, this application is executed once for each individual FIPS, but is capable of running for several at once
2. This abolication assumes riPs in the Parcels/Points files match those in the Counties file see in counties above). But this is not always the case, as sometimes FIPS change. And if a FIPS doesn't exist in the Counties file, it ignores clipping for that County, and calculates the centroid
normaliv. It's not a problem tor how it's run in the Parcelbulldernew pipeline, but it we had run the application for multiple FIPS, any FIPS not in the Counties file we would have data loss.
3. By default, all of the coastal counties in the U.S. are processed.
a. This is controlled by what Counties existing the Counties file
4. Clipping is only applied if the initial and clipped geometry is larger than some minimum area.
                                             1. ﻿﻿﻿If a Parcel doesn't exist, that FIPS is skipped entirely. This is acceptable behavior.
                                             2. ﻿﻿﻿This application generates 2 files
One with the original geometry and one with a potentially modified geometry.
b. Each is updated with the potentially modified centroid.
/. sometimes Centroid calculations tail on clipped polygons. ror those Parcels, the centroid will be
calculated based on the non-clipped geometry.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherentv be adonted.
• Stability. Critical level 1: Use consistent loggers, and synchronize access to stdout (cout) so multiple threads don't corrupt the log output.
Other Kecommendations: Inese are additional recommendations which would need to be addec
ABOVE and BEYOND a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each
message. including what riPs. Subtask. tle and data it is processing.
• Instrumentation/Audit Log. Critical level 1: Add context to all Error messages.
Instrumentation/Audit Log. Critical level 1: Fix insufficient log messages indicating whether a
rips is being clipped or not
• Flexibility/Management. Critical level 3: Each Parcel is compared to each Coastline, and if there are a lot of Coastlines, this could be a very inefficient computation. Possibly could merge all
coastlines Into a single Mutirolygon, ana thus only neea to do a single comparison per rarcel.
• Stability, Speed. Critical level 1: This application requires manual memory and resource management (new/delete), which is very error-prone and dangerous. Instead, use smart
oomcers(smart_ger
1.5.1.1.6 ParcelLoadDatabase.exe
Description
Insert the data trom a single Parcels/Point Shapefile into the SqlServer DB. This includes eatures/properties, raw geometries and coastline-clipped geometries. It also loads shared data, lik
Blocks and APN. into a foreign-keved "inks" tabli
Git Location
CH Appllcatlon
gdsbuilders/Parcels/ParcelLoadDatabase/ParcelLoadDatabaseMain.cs
CLR Dependencies
Denends on the PxPoint DLL (Comoiled from C)I
Inputs
Basically, the Shapefile to process and the DB to load the data into.
• License File.
• SHAPEFILE
ShapeFile to load into DB; not the group of Shapefiles with a common prefix.
• UB-SERVER
PARCELDB
DB_USERNAME
• DB PASSWORD
Outputs
DB tables are updated with data trom the Shapefile.
Logging
Logging is made to a "log" directory 3 levels up from the directory containing the shapefiles. This is very
unintuitive and clunky. And if the directory hierarchy isn't at least 3 layers deep, it writes log files to that
"log" directory in the same directory as the Shapetiles.
The log file name is the same as the shapefile being processed, but with one of the following extensions:
•sodin to les ste names.lo:
• spath to file>/<file_name>.<X>.log
The "‹file_name>" is the name of the Shapefile without its normal suffix ("'shp"). The suffix is
Tile name>.log for the tirst attempt, but it the application talls for the given tile, an index trom 1-9 Is dded to the file name as "<X›" to create a unique log file name. The index is incremented ever ittempt to call this application for the given FIPS, and if 10 files are created, the application will not rur Logging indicates when a FIPS Worker starts and stops, along with what FIPS is being processed.
Exceptions thrown seem to have decent messages.
Dependencies on SQL Server
tb|_RawParcels
Details
1. CreateLogFile - Creates a log file for the given FIPS (see Logging above)
2. Load Coastline-clipped Parcels into a map of Parcel-IDs to Geometry
Iterate over each (unclipped) Parcel/Point, skipping those already in the DB Write each parcel/point into the tbl_RawParcels table, with fields:
statecode < chars trom start or Firs
b. CountyCode (3 chars from end of FIPS) APN
d. APN2
Owner
Name trimmed to 128 characters or less)
†. Address
CItv
State
Zip
1. FuS
From Parce Plus?
k. Borrowed
Integer > 0 means has address borrowed from Diablo
I. LOC PREC(Location Precision)
                                             * ﻿﻿﻿ADDR_SRC (Address Source)
                                             * ﻿﻿﻿Geometry
Overly complex geometries are "Generalized" by removing vertices outside of some sane bounds.
                                             * ﻿﻿﻿ClippedGeometry
If was Coastline-clipped
Overly complex geometries are "Generalized" by removing vertices outside of
p. ParcelType
One of: POINI. PARCEL. PMID (Promoted). DNA Address trom DnA)
q. Int_ld
Parcel or Point ID
GeocodeLat
5. Create Foreign Key linked rows to the tb Links table, with one row per Link Type + Parcel
6. Link Ivoes are
"BLOCK" - City Blocks
b. "APN" - APN Group
raketl - Parcel Group
7. The Link is the ID of the shared Resource, like BlockID or APN, referenced from multiple parcels/points
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 3: The mixing of Trace. WriteLine and Console.WriteLine means that, depending on log levels, some messages will appear on the CLI output and others will not. This is done betore the Logtile is created as well as when writing to the Log file. whichi neans that some errors will not appear in the logfile. Using a standard logger rather than creating manually managed log files completely eliminates this concerr
• Speed. Critical level 1: Ine application will not run at all IT 1u attempts nave been made tor the given file. This is a huge red flag, because if you don't understand this limitation, you cannot now what steps need to be taken to "fix" it so it can be run again and will be completely
blockea Trom re-running the application. An application snoula not fall to run pecause or an arbitrary limitation on the log file name. A quick fix is to use some other unique value as the
multiple log files. Something like seconds-since-the-epoch or the ISO8601 date-time would be a good value. The limit to the number of log files, can cause unexpected application tailures without clear communication to the user as to why.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
                                                * ﻿﻿Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each nessage, including what FIPS, Subtask, file and data it is processing.
                                                * ﻿﻿Stability. Critical level 2: Many parallel DB connections and calls could cause instability when writing to the DB. The best fix would be to batch process groups of Parcels rather than one at a time. And make sure those DB Connections use proper retry and reconnect logic, so they can recover from the occasional, but expected, DB communication failure
1.6 importFromSpatialRecord
Description
Calls Soatia RecordLoad.exe for each Soatial Record file.
Spatial Records contain additional Property information, like ownership, addresses, etc...
Git Location
ParcelStep in ParcelBuilderNew C# application
gdsbuilders/Parcels/ParcelBuilderNew/ImportFromSpatialRecord.cs
CLR Dependencies
one
Inputs
Values trom global Settings:
                                                * ﻿﻿SpatialRecordImportDir - Directory containing Spatial Record files
                                                * ﻿﻿C:/PxPointDataBuild/2024q4/VendorRaw/spatialrecord/min/
• DB connection info
Outputs
Ponulated DB Table:
• tb_SpatialRecord
Splits the Spatial Record data into per-FIPS files in:
• <SpatialRecordimportDir>/SpatialRecordFIPS/
Logging
Minimal, and mostly just progress and errors to the Ul.
It uses Console.WriteLine() for some output, which I'm not sure where that goes, and may be lost.
Dependencies on SQL Server
Tables Modified:
• tbl SpatialRecord
Details
• Only executes if the ImportFromSpatialRecord Step is enabled
Deletes all files in the
Iterates through each file in the SpatialRecordImportDir directory
                                                * ﻿﻿Serially executes SpatialRecordLoad.exe for each file
                                                * ﻿﻿Passes in the DB Connection info as well
• Adds records to the tb _ SpatialRecord table
Adds files to the output directory: <SpatialRecordImportDir>/SpatialRecordFIPS/
• Rebuilds the DB Index on the thi SpatialRecord tabli
• Updates the tb|_RawParcels table with values from the tb|_DnaRecord table and the (now populated) tb/_SpatialRecord table
• Columns updated: UnvPclld and Ext Id
Gaps
Minimal validation of the input CSV data itself. Mostly just counting the columns.
• No Telemetrv around what tile is being processed. and duration of each when processed.
No feedback of progress when loading. No idea if it is working, how long it has been going, or if it is stuck.
Loads files in the SpatialRecordimportDir directory with the ".txt" extension, unless it can't tir ny, then it looks for files with the "zip" extension. Seems brittle and prone to partial data los:
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
AbOV. and BLYONd a standard logger.
• Speed. Critical level 1: Load any combination of " txt" files and/or "* zip" files, rather than only
all Tiles with one or the other
• Speed. Critical level 1: If load time is too long, adding more parallelism to the processing o nput files would help. If the load time is low, that would be wasted effort, howeve
1.7 rebuildRawIndexes
Description
Adds DB Indexes to several DB Tables.
Git Location
Parce Sten in Parcel BullderNew C# apolication
gdsbuilders/Parcels/Parce|BuilderNew/RebuildIndexes.cs
Calls SQL Script: gdsbuilders/Parcels/ParcelBuilderNew/SQL/BuildNormalizelndexes.sal
CLR Dependencies
None
Inputs
Outputs
DB connection into from global settings
Modified DB Tables
Logging
Start and complete/error messages to the Ul only.
no progress updates or context in error messages
Dependencies on SQL Server
Tables Modified:
tbl_Addresses
tbl_ Cities
• tbl_Link:
• tbl_RawParcels
Details
Executes the gdsbuilders/Parcels/ParcelBuilderNew/SQL/BuildNormalizelndexes.sql script that adds DB
Indexes to several DB Tables.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherently be adonted.
                                                * ﻿﻿Stability. Critical level 1: Use consistent loggers, and synchronize access to stdout (cout) so ultiple threads don't corrupt the log output
                                                * ﻿﻿Instrumentation/Audit Log. Critical level 1: Add messages, especially around timing, to the log Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Speed. Critical level 1: Split up the index calls, so we can update progress (and log) when each completes.
1.8 updateJobBlocks
Description
Updates the tbl_Jobs. RecordCount for each County (FIPS) to the number of parcels in the tb_RawParcels for that County
Git Location
Parce Step in ParcelBulldernew ca application
gdsbuilders/Parcels/ParcelBuilderNew/UpdateJobBlocks.cs
CLR Dependencies
None
Inputs
Values from global Settings:
• RawShapeDir - Directory containing all Raw Shapefiles before being pre-processed
• PreprocessDir - Directory containing all Shapefiles after being pre-processed
• Same Directory used to load the DB in the ParcelLoadDatabase.exe Task above
• DB connection into
Outputs
Each FIPS-based row in tb_Jobs is updated with the count of Parcel+Point counts for that FIPS, as
queried trom the toi rawparcels table.
Logging
Messages are communicated with UO only, no file-based logging at all.
Logging indicates when the lask starts and stops.
Has weak messaging when any raw and pre-processed Shapefiles have a count mismatch.
Has detailed messages around mismatched counts between the DB and the processed Shapefiles.
Dependencies on SQL Server
lables Queried.
• tb_RawParcels
Tables Modified:
• tDi JObS
Details
Only executes if the UpdateJobBlocks Step is enabled
The intent of this Task is to prepare the orchestration of the Normalization Tasks by recording which counties should be done in which order, based on the "size" of each County.
Shrink the DB Transaction-Log to prevent disk space crashes
• Count up the number of records in the input shapefle set and shapefil set created by the
preparation ster
• Verify that there are more pre-processed Parcels/Points than the Raw Parcels/Points we began with.
• Verify that we all of the input records in the pre-processed shapefile were written into the tb_RawParcels table before proceeding
• Update the tb Jobs. RecordCount with the total counts of Parcels+Points for each FIPS, queried
Trom the tol Kawrarcels table
• Shrink the DB Transaction-Log again
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
ABOV- and BLYOND a standard loggen
• Stability, Speed. Critical level 1: Add counts of mismatched FIPS to the log message when the Raw and pre-processed Shapefiles don't match.
• Speed. Critical level 3: Do minimal changes here until we move to a better Orchestrator. Then
this task goes awav entirelv.
1.9 calculateJobBlocks
Description
Update the Parcels.ini file with the ID of the first and last Job
Git Location
Method in ParcelBuilderNew C# application
gdsbuilders/Parcels/ParcelBuilder/New/ParcelBuilderMainWindow.xam/.cs
CLR Dependencies
None
Inputs
Outputs
DB connection into from global Settings
Moaes Farcel.lu Te ana In-memor setings
Logging
INone
Dependencies on SQL Server
Toi Jobs
Details
• Small Task that finds the first Job ID and the last Job ID in the tbl _Jobs table, and writes them to the Parcel.ini file, into the [BuilderParams] section as:
OFIrSTODIG
• LastJobld
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Speed. Critical level 3: Leave this alone until we move to a better Orchestrator. Then this task goes away entirely.
1.10 normalizeParcels
Description
lask that calls an external application. Parcelbulldernormalizationstep.exe. and waits tor it to complete
This Task indirectly performs the Normalization of the Parcel data.
Git Location
ParcelStep in ParcelBuilderNew C# application gdsbuilders/Parcels/ParcelBuilderNew/NormalizeParcels.cs
CLR Dependencies
None, but some of the downstream applications use it.
Inputs
                                                * ﻿﻿List of Steps
Will only execute if the NormalizeParcels Step is enabled
                                                * ﻿﻿Global Settings
Outputs
No direct outputs. but downstream applications will have side eftects.
Logging
• Start and complete/error messages to the Ul only.
• No progress updates or context in error messages.
Dependencies on SQL Server
No direct access to the SQL Server Database, but downstream applications do use it.
Details
                                                   * ﻿﻿This ParcelStep simply wraps a call to the ParcelBuilderNormalizationStep.exe script, which normalizes the Parcels from the source Shapefiles.
                                                   * ﻿﻿The call is dependent on the NormalizeParcels Step being enabled.
Gaps
• There is no file-based logging at all, only messages in the Ul.
٨ ٥٦٥٦٥٦S
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Stability. Critical level 1: Use consistent loggers, and synchronize access to stdout (cout) so
multiple unreaas aon t corrupt the lo? output
• Instrumentation/Audit Log. Critical level 1: Log to a file serves as an audit trail, recording user ictivity, system events, and security-related incidents. Detailed logs with full context help in
forensic investigations, compliance checks, and identifying unauthorized access attempts
Other Recommendations: These are additional recommendations which would need to be added
AbOVE and BLYoNd a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages, especially around areas we want to see timing.
1.10.1 ParcelBuilderNormalizationStep.exe
Description
Orchestrates calls to other Applications (EXE/Executables), that in turn do the Normalization of Parcel records.
Git Location
C# Application
gdsbuilders/Parcels/ParcelBuilderNormalizationStep/NormalizationStepMainWindow.xaml.cs
CLR Dependencies
None, but some of the downstream applications use it.
Inputs
                                                   * ﻿﻿STARTID
First Job ID to Normalize
                                                   * ﻿﻿ENDID
Last loh ID to Normalize
• DB SERVER
                                                      * ﻿﻿Database Server address
This overrides the value in the Parcels ini file, for some reason
                                                      * ﻿﻿PARCELDB
• Parcel Database to use
This overrides the value in the Parcels ini file, for some reason
• PARCELSINI
Path to the Parcels ini file, which contains other needed properties
From Parcels.ini
• ReprocessDfp
Reprocess path
                                                         * ﻿﻿NavteqGdxFfp
                                                         * ﻿﻿Path to US Navteq GDX data
                                                         * ﻿﻿ReferenceShapesDfp
Path containing shapefiles of Coastlines and Municipal boundaries and similar
• StreetProximityBuffer
                                                         * ﻿﻿Floating point number
                                                         * ﻿﻿Passed into downstream Application
DB username and password on
DB Server Address and Parcel DB Name are passed in from the (
Outputs
No direct outputs, but downstream applications will have side effects.
Logging
Outputs minimal error messages to Trace and sometimes to a Popup Dialog
No logging of current behavior/status/progress except when executing sub-processes.
When executing sub-processes, it creates a logfile based on the currently executing NormalizeStep (There are 5. see below and the Job ID and an index. which is incremented every time the Sten is rerun for a Job (due to failure). This is a similar red-flag to other parts of the pipeline; where if you exceed a certain number of log files (15 in this case) for a Job+Normalizestep, it will prevent the user from
running it again until the log tiles are manually cleaned up/deleted.
Dependencies on SQL Server
• tbl_RawParcels
• tol Jobs
Details
Ine Application processes each Job, based on Job ID, in parallel, It that Job has unprocessed parcels Each parallelly running Job executes all NormalizeSteps serially and in order, but skips NormalizeSteps that have already been completed.
Each NormalizeStep calls an external Application (EXE) and waits for it to finish before moving onto the next NormalizeStep.
When all Normalizesteps complete for a Job ID, that Job ID is considered complete.
Each Job is run in parallel with up to NumProcessors number of parallel threads running at once, with
others waiting for a tree Thread to run on.
And when one Job ID completes, it starts processing another Job ID until all complete.
Then the application exits.
Lach Normalizestep has it own section in this document, but are called in order:
1. Normalize
Calls ParcelNormalizer.exe with no additional flag set
< Checkbasendaresses
Calls ParcelNormalizer.exe with CHECKBASEADDRESSES flag set
3. GenerateFailureFile
Calls ParcelNormalizer.exe with GENERATEFAILUREFILE flag set
4. Parcelgroup
a. Calls CabinInWoods.exe
5. Renormalize
a. Calls ParcelNormalizer.exe with REPROCESS flag set
Gaps
• No Telemetry around what Job is being processed, and duration of that processing.
• Limited Teedback or progress when loading. Univ Teedback is in the Ul Vatatable, which has one row per Job.
Only has log files for sub-processes, none for the orchestrator behavior itself.
* Calls to subprocesses (Applications) gives insufficient context or informatior
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherentlv be adooted.
• Instrumentation/Audit Log. Critical level 1: Log to a file serves as an audit trail, recording user activity, system events, and security-related incidents. Detailed logs with full context help in forensic investigations, compliance checks, and identifying unauthorized access attempts.
                                                         * ﻿﻿Stability, Speed. Critical level 1: Add Telemetry around each Subtask or major loop, especially the duration of each.
                                                         * ﻿﻿Stability, Speed. Critical level 1: Show messages in the Ul, but make sure they go to Standard
out and log tiles too.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
                                                         * ﻿﻿Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each message, including what FIPS, Subtask, file and data it is processing.
                                                         * ﻿﻿Instrumentation/Audit Log. Critical level 1: Add context to all Errors messages.
1.10.1.1 Normalize
Description
ParcelNormalizer. exe executed for a single FIPS with standard flags. This task "normalizes" the Address of each Parcel in that FIPS, by validating and updating Address information from USPS, Navteq and other
Git Location
C# Application - ParcelNormalizer.exe
gdsbuilders/Parcels/ParcelNormalizer/HeadlessParcelProcessor.cs
CLR Dependencies
Normalization is done in ParcelLib, which is C++ code accessed through the CLR.
Parcels are passed from CH into the C++ code, and resultant Address data is passed back the other way.
Inputs
                                                         * ﻿﻿FIPS
                                                         * ﻿﻿5-character FIPS County Code
                                                         * ﻿﻿COUNTYNAME
Name of the County, as per the CountyName column in DB Table tbl _Jobs
                                                         * ﻿﻿PARCELSINI
                                                         * ﻿﻿Path to the Parcels ini file, which contains other needed properties
                                                         * ﻿﻿NUMTHREADS
                                                         * ﻿﻿Number of parallel threads to use.
                                                         * ﻿﻿Usually one of: 5, 3 or 1, depending on the number of Parcels in the County
Outputs
Writes Address data to the DB, as well as Street, Block and similar data.
Logging
C# code logs start and stop messages, but lack sufficient context information. It also logs errors that
occur, and une error message.
Dependencies on SQL Server
Writes processed Address data to several lables
Also updates the status (Problem Code of each Parcel as it completes, in the tbl_ RawParcels table
• tol kawrarcels
Queried for Parcels to process
• Updated with Status changes
ProblemCode - Column
                                                         * ﻿﻿Contains ProblemCode indicating Address status and source
                                                         * ﻿﻿Processed - Columr
                                                         * ﻿﻿Indicated Processing complete or failed
                                                         * ﻿﻿ProcessedOn - Column
DateTime processing was completed
                                                            * ﻿﻿tbl_Links
                                                            * ﻿﻿Queried for Blocks to process
• tb|_CityCollections
                                                            * ﻿﻿tol Cities
                                                            * tbl_Addresses
tbl_StreetCollections
                                                            * ﻿﻿tbl Streets
Details
Parallelization is done on groups of ~5000 Parcels, each group running on an independent Thread, up to
NUMIHREAUS running at once. Ine count isn t exactiv suuu, as Parcels on the same City Block need to be processed together, so sometimes groups are slightly larger.
Parcels are then "Normalized" by passing them through the CLR barrier into the C++ code.
Then the resultant "Normalized" Addresses for those Parcels is passed back through the CLR the other way, and written to the DB
1. If the Parcel is part of a Block, do some work with the Block first:
Only do this once per Block, not once per Parcel
                                                               * ﻿﻿﻿Calculate bounding Geometry (polygon-ish) for the Block, and associated bounding rectangle.
                                                               * ﻿﻿﻿Load and cache all Streets from Navteq that are near this Block, using this bounding
These are cached in a static/global location, which makes subsequent calls faster, but has the potential to leak memory, as well as data integrity issues.
ii.
IE: subsequent calls might see data for the previous Block
d. Load alternate city names that may be used to geocode records in the Block, using the bounding Geometry
Load Cities within a certain distance from the Block
Ihis is a very complex calculation, with embedded loops, and thus might be a performance bottleneck.
2. Check if the Parcel is a bad parcel
a. |his tends to mean things like the Address contains garbage data
b. If it's bad, skip to the Zip Code + Municipal checks below (#8)
3. If the Parcel isn't part of a Block, load alternate city names
a. This is similar to the load done above for Blocks, but using the Parcel Geometry instead f the Block Geometry
1. Check to see if the Parcel is a Unit in an Apartment Complex or other "base" Address that has
alreaay been vallaatea tor this block.
a. A "base" Address is an Address with things like Unit numbers removed
b. IE: Has another Parcel for this Block already looked up and matched an already normalized "base" Address?
This prevents re-normalizing an already normalized "base" Address
d. If a Parcel matches an already processed "base" Address, bypass all other processing and vrite that Address data to the DB for that Parcel. This is a performance enhancement.
5. Calculate all neignboring streets to the Parcel, Trom Navrea
Try to find the best "candidate" for the Address
This does hundreds of checks and transforms on the Address to find the best matches
b. Each potential "candidate" Address (basically) has:
A value with units and extra data tripped off (IE: it is a "base" Address)
A Score number, where smaller means a better match; O means a perfect match A Problem Code describing why it didn't match, if it was a bad match.
c. These checks are run multible times. in ditterent wavs/permutations of the Address
d. Some of these retries are on Addresses with unit numbers removed, or other commonly seen one-off patterns (and there are a LOT)
• It also checks using Malling Address IT the given Address doesn t match anthing
It also caches some values, like "candidate" and "base" Addresses, so that multiple Parcels in a Block can parse faster
•This logic is so complex and convoluted, it's hard to articulate it beyond its end result
                                                                  1. ﻿﻿﻿The Address is then "Standardized" by comparing it to the USPS dataset, and "borrowing" the
                                                                  2. ﻿﻿﻿If no best "candidate" is found, strip off the end word from the Address, and try it all agair
a. The end word is anv text atter the last whitespace in the Address
b. This is tried up to 3 times, each time stripping off another word
9. If there is still no "best" match, it attempts to fill in city/state/zip information using the Zip Code
and Municipal data for the given countv.
                                                                  * ﻿﻿﻿First it tries to match an input zip
                                                                  * ﻿﻿﻿Next it will choose the closest zip
c. Then it tries the closest municipal area
                                                                  1. ﻿﻿﻿﻿If still not a valid Address, it marks the Parcel with a Problem Code indicating the status or error
                                                                  2. ﻿﻿﻿﻿Update DB with Parcel status (Problem Code) and other changes
12. It the Parcel has a valid "Normalized" Address. add rows to the Db tor It
. Addres, several B queries/ tatements for each Parcel, and thus could be a good
place to look at improving pertormance.
13. If the Parcel does not have a valid Address, then delete any existing DB Address related data for
a. This doesn't delete the Parcel, just Address-related (normalization) data.
Problem Codes
0. ٢٢C PEKFECI
a. Is a perfect match
1. PPC MATCH
Good USPS Match, with an associated NumberScore that indicates match qualitv
2. PPC_GROUP
a. Verified using neighbors in proximity group
3. PPC_NONUMBER
Nome kind of name match possible but no number
4. PPC PROXIMITY
Parcel Address matches USPS, but location is too far away.
5. PPC_UNPARSEABLE
Unparseable - PPC_NONUMBER used when no house number parsed
b. Not used
6. PPC_NOADDRESS
No matching address found
7. PPC MUNINOLP
b. Toe the muni layer for city/state/zip, to no avail
8. PPC NUMRANGE
a. Address number outside of nearest street segment range (final value)
9. PPC_BADGEOMETRY
baa geometr
b. Not used
Everything is expected to have geometry
10. PPC_BADNUMBER
a. Address number outside of nearest street segment range (intermediate value. nevel seen in final output)
11. PPC_EMPTY
a. Nothing to work with
b. Address is unusable - no street match, no USPS match
Gaps
                                                                  * ﻿﻿There is lots of code to work around Navteq schema changes, which seem to happen pretty often. This makes Address parsing and validation inconsistent and brittle, from quarter to
                                                                  * ﻿﻿The complexity of Normalization itself is massive, and has deeply complex dependencies on different Shapefiles and datasets.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Log to a file serves as an audit trail, recording user activity, system events, and security-related incidents. Detailed logs with full context help in
torensic investigations. compliance checks. and identitving unauthorized access attemots.
• Stability, Speed. Critical level 1: Add Telemetry around each Subtask or major loop, especially
the duration or each
• Stability, Speed. Critical level 1: Show messages in the Ul, but make sure they go to Standard Out and log files too.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Stability, Speed. Critical level 3: The C++ code is so massive and convoluted, and has so many one-off checks, fixes and tweaks, that to rewrite it from scratch, or use Graph DB functions,
would be a monumental error. we do not recommend replacing the business logic at this time
• Stability, Speed. Critical level 3: The calls through the CLR are mostly around orchestration and writing results to the DB, so rewriting that part is much more feasible. And the performance gain
mignt be signiticant, especially it aivided up alterently trom now it Is currently with suuu Parcels in a group).
Parallelize the processing of each Block of Parcels, or each individual Parcel (not in a
block)
                                                                  * ﻿﻿The calling code is inetticient. as there are a lot of static butters and tile handlers that are ecreated for each Parcel or Block of Parcels, but if you split it onto hundreds or thousands of parallel processors, pertormance would still be greatly improved overall
                                                                  * ﻿﻿Stabilitv. Speed. Critical level 1: Navtea Schema changes should be encapsulated. and thus detected and handled ahead of time, before processing even starts.
This ties back to the Data Integrity goal in the Overview
• Stability, Speed. Critical level 2: Buffer multiple DB inserts into a single statement. This could greatly improve DB performance and stability.
1.10.1.2 CheckBaseAddresses
Description
ParcelNormalizer. exe executed for a single FIPS with the CHECKBASEADDRESSES flag set. This task
normalizes the Adaress or each Parcel In that rifs that wasn t successtully normalized In the previous step, and that is a part of a larger "base" address (IE: Apartment Complexes, or similar), and then writes its results to the DB. Other than how it groups Parcels, the "normalization" behavior is nearly identical to the previous Normalize task.
Git Location
# Application - ParcelNormalizer.exe
gdsbuilders/Parcels/ParcelNormalizer/HeadlessParcelProcessor.cs
CLR Dependencies
Normalization is done in Parcellib, which is C++ code accessed through the CLR.
Parcels are passed from C# into the C++ code, and resultant Address data is passed back the other way.
Inputs
• CHECKBASEADDRESSES
Used to tell the application to do Base Address Normalization rather than the detault
Normalize used in the previous Task
                                                                  * ﻿﻿FIPSI
                                                                  * ﻿﻿5-character rips Countv code
• COUNTYNAME
Name of the County, as per the CountyName column in DB Table tbl_Jobs
                                                                  * ﻿﻿PARCELSINI
Path to the Parcels.ini file, which contains other needed properties
                                                                  * ﻿﻿NUMTHREADS
Number of parallel threads to use.
• Usuallv one of: 5. 3 or 1. depending on the number of Parcels in the Countv
Outputs
Writes Address data to the DB, as well as Streets, Cities, Block/Group and similar data.
Logging
C# code logs start and stop messages, but lack sufficient context information. It also logs errors that occur, and the error message.
Dependencies on SQL Server
Writes processed Address data to several Tables.
Also uodates the status (Problem Code of each Parcel as it comoletes. in the toi RawParcels table.
• tb|_RawParcels
Queried for Parcels to process
• Updated with Status changes
ProblemCode - Column
• Contains ProblemCode indicating Address status and source
• Processed - column
• Indicated Processing complete or failed
• DateTime processing was completed
• tb|_Links
• Querlea tor blocks to process
• tb|_CityCollections
                                                                     * ﻿﻿tol Cities
                                                                     * tbl_Addresses
tbl_StreetCollections
                                                                     * ﻿﻿tol streets
Details
This task is similar to the previous Normalization task, but instead of grouping Parcels using Groups
aerectea in pre-processing, It uses a alerent algorithm tor derecting groups or Parcels. It then Normalizes those grouped Parcels using the same logic as regular Normalization, but using the newly
stars executing in ch coae
2. Load all Cities already created trom previous executions of this Step
a. IE: Cache Cities already created if this Task is run multiple times
3. Loads base Addresses based ort of previousiv Normalized Parcels that are Units in a larger (Apartment) Complex
Each Parcel must have been successfully Normalized
b. tach Parcel must nave a value Tor column unitwumper or unitivoe
Each "base" Address must match 3 or more of these Parcels
Each mase Adress must much or mone of these Pro unded house number (but not
necessarly daaress, street (colection) ana city (colection)
e. tach of these grouped Parcels is merged into a single "base Address, containing a range of Zip Codes and bounding-box (based on latitude and longitudes). Sometimes called a
'Complex," as in an Apartment Complex.
4. Then, gather all non-Normalized Parcels that have the same house number as one or more
Complexes, and group them together by Zip Code.
5. Then find the best matching Complex for those Parcels, based on the distance of the centroid of
the Farcel ana the bounaing box or each complex.
a. The calculation also allows Parcels to match some threshold distance away from the Complex (usually within 100 meters)
6. If too few Parcels (less than 3) are matched to a Complex, then those Parcels are not Normalized
against that Comolex. But they mav Normalize against other nearbv Comolexes.
                                                                        1. ﻿﻿﻿Multiple Parcels are grouped together in Groups, with a single "base" address per grouping.
                                                                        2. ﻿﻿﻿This is where the Parcel data crosses the CLR
a. It is also where Navrea and other datasets are loaded. Into the c++ code. as part of initializing the CLR
9. Each group of Parcels Parcel that is part of a Complex is now Normalized, using the normal
normalization algorium, with the complex as Its base adaress.
a. This is done across the CLR, from C# to C++
This is effectively the same as the above Normalize Step, except with a different
Grouping (Complex instead of Block)
10. The result Addresses for each Parcel are passed back across the CLR from C++ Into C#
11. Those Parcels are marked as complete in the DB, and not compared to any other Complexes.
12. Parcels that don't have a matching Complex are left unchanged, and still not-Normalized
Gaps
same as Normalization, above, and.
                                                                        * ﻿﻿| here are several loops done on groups of Parcels. manv times embedded in other loops. There is even a triple-embedded loop, which is a red flag for performance. Parcels are looped over and over again in order to do some transformation or filtering.
                                                                        * ﻿﻿Iriple Loop: House numbers - complexes - Parcels
• The calculations for repeatedly expanding the range of possible Parcel matches to a Complex is very complicated, unintuitive and inefficient.
• Matching Parcels based on house number and Zip Code is unintuitive, and worth evaluating its correctness and efficiency. But clearly matching based on things like Street or Address are ineffective, or it would have been done differently. So the algorithm might be best left alone.
Lots of Geometry calculations are comparisons are done in the C# code instead of in C++. This seems like an inconsistent and inerticient wav to process ceo data
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following recommendations will inherently be adopted.
                                                                        * ﻿﻿Instrumentation/Audit Log. Critical level 1: Log to a tile serves as an audit trall. recording user ictivity, system events, and security-related incidents. Detailed logs with full context help in forensic investigations, compliance checks, and identifying unauthorized access attempts.
                                                                        * ﻿﻿stability, Speea. Critical level 1: Ada lelemetry around each subtask or malor loop, especially the duration of each.
• Stability, Speed. Critical level 1: Show messages in the Ul, but make sure they go to Standard
vut and log thies too
Other Kecommendations: Inese are additional recommendations which would need to be addec
ABOVE and BEYOND a standard logger.
                                                                        * ﻿﻿Stability, Speed. Critical level 2: It would be more efficient to group unprocessed parcels by house number in SQL, rather than doing so afterwards, in code
                                                                        * ﻿﻿Stability, Speed. Critical level 2: Filter out unprocessable Parcels in the same SQL query by dding a WHERE clause that rejects all Parcels not within Zip Code range of all "base" addresses This would require the "base" address SQL Query to correctly calculate Zip Code ranges, which is currently done in-code instead of in SaL.
                                                                        * ﻿﻿Speed. Critical level 2: Doing all of these would mean only needing to loop over all Parcels once,
instead of multiple passes trying to group them together and filter them out (From O(N^2) to
                                                                        * ﻿﻿Speed. Critical level 2: Decouple the SQL query from the Normalization processing, or at least
verify that keeping a long-running SQL connection open isn't causing periodic failures
• Speed. Critical level 3: Although there is some complex c# code. It shouldn't be too hard to rewrite in C+t, and thus be able to move this functionality out of CH entirely, eventually. There also needs to be a stable way to access the DB from C++ of course, like a library. That is an overall concern though, and not specific to this Task.
1.10.1.3 GenerateFailureShapefiles
Description
arcelNormalizer.exe executed for a single FIPS with the GENERATEFAILUREFILE flag set. This task copi I "imperfect" Parcels from the tb|_ RawParcels into Parcel (Polygon) and Point Shapefiles. This include
all Parcels wnose Probiem codes › U, which Includes all non-Normalizea Parcels ana some Normalizea Parcels with non-zero Problem Codes (1 and 2).
Git Location
C# Application - Parcelnormalizer.exe
gdsbuilders/Parcels/ParcelNormalizer/HeadlessParcelProcessor.cs
CLR Dependencies
The reading and writing of Shapefiles is done using the C++ libraries, but all DB queries are done using
native C#. IE: features and geometries queried from the DB, and then passed through the CLR to be written to the associated Shanetiles.
Inputs
Command line arguments:
• GENERATEFAILUREFILE
Used to tell the application to Generate Failure Shapefiles rather than the default
Normalization
                                                                           * ﻿﻿FIPS
                                                                           * ﻿﻿5-character FIPS County Code
• PARCELSINI
Path to the Parcels.ini file, which contains other needed properties
Outputs
Writes Parcel (Polygon) and Point Shapefiles containing data retrieved from the DB.
The path for the generated Shapefiles is based on the PLayers/ReprocessDfp property in the Parcels.ini
Where PLayers/ReprocessDfp is something like
• C:/PxPointDataBuild/2024q4/working/parcel/normalize/
The the Shapefiles generated would be:
C:/PxPointDataBuild/2024q4/working/parcel/normalize/parcel/<fips>_parcel.shp
: C./PxPointoataBuild/202494/workin oints shp
Logging
C# code logs start and stop messages, including how many records were written. That might be sufficient
context intormation for such a simole task.
also logs errors that occur, and the error message. It doesn't really explain where it failed however, a what Parcel caused the failur
All logging is only to the Console, and there is no file-based logging at all. for this or any of the other Normalizations Tasks.
Dependencies on SQL Server
Reads Parcel data from a single Table: tbl RawParcels
Details
This Task first deletes any leftover Shapefiles for this FIPS, that are going to be recreated.
Then it iterates through all Parcels with Problem Codes > 0, which mean anything but "perfectly" normalized Parcels, including Normalized using Geocoding or Neighbor-based Proximity groups.
Each Parcel is written to a Parcel (Polygon) or Point Shapefile, depending on the Parcel Geometry type.
Columns copied to Shapefile properties:
                                                                           1. ﻿﻿﻿Geometry
                                                                           2. ﻿﻿﻿Column ID is left unset
3. Int 1D => ORIG ID
StateCode => STATE_CODE
5. CountyCode => CNTY_CODE
6. APNI
APN2
Owner => OWNER
L. Address => ADDR
34
City => CITY
State => STATE
2=40 ٤٢ .
6. ProblemCode => FAIL_CODE
Gaps
• It is inconvenient to have to use the C++ Libraries to manipulate Shapefiles, and C# to query the
DR. It would be better overall to access the non-Normalized Parce s directlv from the DR in al later Task, rather than have to do all this inefficient and brittle copying of values back and forth.
Especially for something so simple.
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
AbOvE and BLYONd a standard loggen
• Stability, Speed. Critical level 3: This Task exists solely to get data from the DB into Shapetiles, because the next Task, ParcelGroup (CabinInTheWoods.exe) is a C++ application that cannot read from the DB, but can only use Shapefiles. Thus this Task is fairly simple and stable, and
there is no real need to optimize or change it. It will likelv go awav entirelv when we move to a different orchestrator.
1.10.1.4 ParcelGroup
Description
CabinInWoods.exe executed for a single FIPS. This task compares nearby non-normalized Parcels into
groups in order tor Parcels to validate each other based on criteria like distance, geometrv. location (in remote vs urban areas), etc. This is in order to be able to compute an Address based on related Address for the non-validated Parcels using the "Braces" algorithm and others. This Task then generates a
snaperlie containing the boundaries or these generatea groups
Git Location
C++ Apolication - CabininWoods.exe
gdsbuilders/Parcels/CabinInWoods/CabinInWoodsMain.cpp
CLR Dependencies
None. The Parcels are already copied from the DB into Shapefiles by the previous Task, and the output Shapefile of this Task will be used by the next Task.
Inputs
• Commandline varameters:
inputDir
Directory containing all Shapefiles ready to be processed by this Task
The directory may include Shapefiles for multiple FIPS, but each execution of this Tasl
oniv processes one rips at a time
C:/PxPointDataBuild/2024q4/working/parcel/normalize/
                                                                           * ﻿﻿forestLayer
Forest Shapefile used to filter parcels.
                                                                           * ﻿﻿Parcels groups which DO NOT intersect forest areas are ignored
C:/PxPointDataBuild/2024q4/working/parcel/shp/USGS_Forest.shp
                                                                           * ﻿﻿urbanLayer
• Urbanized Shapefile used to tilter parcels.
Parcels which intersect urban areas are ignored
Unused by this workflow
                                                                              * ﻿﻿inputStreets
                                                                              * ﻿﻿Navteq streets dataset
C:/PxPointDataBuild/2024q4/dataset/navteq us.gdx
• outputDir
Directory to write the normalized "group" Shapefile, ready to be used in the next Tas
:/PxPointDataBuild/2024q4/working/parcel/normalize/groups
• outputBlocks
Flag to write output Parcel Block (and blocked parcels/points layers) Shapefiles
• outputPLV
Flag to write PLV (Parcel Link Validation) output Shapefiles
veraults to Taise wnich is not overriaden
o Thus Parcel Link Value (PLV/ "link") Shapefiles are not generated (technically they are
created then deleted)
• stco
• 5-character FiPs Countv Code'
                                                                              * ﻿﻿Only process Parcels from this specific FIPS
                                                                              * ﻿﻿maxProximity
                                                                              * ﻿﻿Maximum distance (meters) for parcels to be considered proximate
                                                                              * ﻿﻿Defaults to 1000 (which is rarely used)
Set from Parcels.ini property ParcelParams/StreetProximityBuffer
                                                                              * ﻿﻿Usually set to something between 200 to 250
• maxBlockDist
Maximum distance from center of a block to be included in that b veldults to sou which is not overriac
• maxBlocksevaration
• Maximum separation allowed (meters) to still be connected to block
Defaults to 0.1 (which is not overridden)
• maxblockparcelArea
• Maximum area allowed in a blocked parcel
Defaults to 25000 (which is not overridden)
lypical city block is 4.2s acres
• Chicago is one of the larger with 'double blocks' of 5+ acres
An acre is a little less than 4050 sauare meters
• We want single parcel city blocks to be blocked, so we'll include parcels up to 25,000
squale mictels.
• lhis Is not maxblockArea
• maxBlockArea
Used in conjunction with maxBlockDist to limit maximum block size
• veraults to suuuu wnich Is not overriaden.
We allow blocks to grow to up to maxBlockArea square meters, as long as the maxBlockDist requirement is met.
• maxBlockFactor
Maximum parcel to median size tactor allowed to include a parcel in a block
Defaults to 3 (which is not overridden)
• We don't want to connect lots of little neighborhoods with larger ones simply because
there are large, ad acent parcels between them
                                                                                 * ﻿﻿For a parcel to be included in a block it needs to be no more than maxBlockFactor times larger or smaller than the median parcel size in the block.
                                                                                 * ﻿﻿minBlockCompactness
Minimum compactness value (area/perimeter squared) for parcels to be grouped
Defaults to 0.02 (which is not overridden)
Wealts want to thor away parer that are likely strips of land between blocks, such as
polvgons representing the shared roadwavs in a gated communitv. Inese lots have smalli
'compactness' values.
A value of .06 is very compact (square)
• A value of 1 is most compact, and those approaching zero are spaghetti and should be
eycluded
05 is a reasonable exclusion level
                                                                                    * ﻿﻿minLinkedParcels
                                                                                    * ﻿﻿Minimum number of linked parcels to pass validation test
Defaults to 3 (which is not overridden)
• maxProximity
• Maximum distance (meters) for parcels to be considered proximate
Defaults to 1000 (which is not overridden)
                                                                                    * ﻿﻿maxCount
                                                                                    * ﻿﻿Maximum number of parcels to process (-1 for all)
Defaults to -1 (all, which is not overridden)
• numThreads
Number of parallel threads to run, across different FIPS
Detaults to 6 which is not overridden
The current execution only runs a single FIPS, so this value is effectively unused, and all
processing Is aone In d single unreda.
                                                                                    * ﻿fI
                                                                                    * ﻿﻿Path to the License File
License information is not documented here
• Force reload of data
Normally the Task skips FIPs with existing output Shapefiles
This flag means it will rerun this Task even if those output Shapefiles exist
• If unset, and an output Shapefile exists for the given FIPS, running this Task again will do
nothing. Lven it the output shapetile is emoty or corrupted.
• show additional debug output.
Unused by this workflow
Outputs
Groups are written to
• C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<fips>_groups.shp
Logging
Mixing of couto and wcout!. and sometimes sunchronized and sometimes not. Unsvnchronized calls to out() can lead to log corruption if there are multiple threads. In this case, since the execution is for a ingle FiPS. onlv 1 thread is runnin
Dependencies on SQL Server
None
Details
Executes different algorithms, trying to compare and group non-validated Parcels to validated Parcels based on criteria like distance, geometry, location (in remote vs urban areas), etc in order to be able to validate the Addresses for these non-validated Parcels. Parcels are then grouped by house number or
the same street using the Braces algorithm, or grouped together into Proximity Groups. Inese groups then have their Geometry boundaries merged into a Block or Proximity Group. These Groups, and associated group boundary Geometries, are then written out to a Shapefile.
This uses a simple tile-existence check on the output tle to determine it a county nas already been processed by an earlier run. It processing falls these files are normally deleted, but it the application crashes then these files are left in an inconsistent/corrupt state, and may prevent reprocessing of the data. It is currently run with the force flag set, so re-processing is always done, and is never skipped due
to being already processed. This has its own set of pertormance concerns. but at least won't prevent reprocessing.
valloaterarces ٣LV
Group parcels into Blocks, based on outward/street facing boundaries, using the 'Braces' algorithm.
This sub-task will create a block at a time, obtaining bounding streets, creating topology, linking parcels,
Inis step pre-creates large arrays or parcel-groups and statuses to make the algorithm work. Ini: memory footprint may potentially be very large, but uses many tricks to keep the size down.
1. Iterate througn all not-perteculy-Normalized Parcels In the Input sha perile
a. Parcels with ProblemCode > 0
2. Gather them into groups of neighboring Parcels that are within maxBlockSeparation meters.
For a large number of Parcels, this could be a performance bottleneck (ON*M)).
b. It filters out Parcels that don't meet size/area and compactness limits It also filters out Parcels that have overly complicated or irregular shapes.
3. For each group of Neighbors, it tries to combine them into a Block, based on the area of each Parcel and Iterativelv expanding the size of the Block until its size reaches maxBlockArea or It runs out of neighbor
4. If this fails for any neighboring Parcel, all Parcels in this Neighbor Group are reset, and thus available to be grouped into a different Block.
Try to simplify the Geometry of each Block
Do more calculations to determine the neighboring Parcels that fit within that block
Filter out Parcels that are Stacked or have incongruent geometries
8. Create a Block Geometry that encompasses all Parcels in a Block, and calculate the boundary
edges of that Block Geometry
9. Write out each Parcel in a Block as-is, but with an added BLOCK_ID property
a. C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<fips>_parcel.shp
10. Write out each Block and associated Geometry to the Block Shapefile
This file is eventually deleted, as the outputBlocks is not set
c. I woula be more erricient to not write It at alli
11. Gather Address details for each Parcel in a Block, split into its component parts
a. street, house-number, city and unit
12. For Parcels on a Block with the same street, validate that the house numbers are in the range
between neighbors. It enough neighbors agree, accent the addresses as being valid. It there's al discrepancy, invalidate all addresses on that street in that block.
13. Do specialty checks on Parcels based on the number of neighbors, and how many neighbors
were validated.
a. Complicated checks here. For example:
b. A Parcel is validated if 2 of 3 neighbors validate
A Parcel with 2 neighbors is validated if the Parcel house-number is between them
                                                                                    * ﻿﻿﻿Re-split neighbor groups is house-numbers aren't contiguous or are separable across groups (house numbers like 10-20-30-140-150 can be split at 30-140)
                                                                                    * ﻿﻿﻿Verify Parcels are with the same City and Zip Code
T. etc...
14. Each Group (Block) is then written to the Groups Shapefile:
C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<fips> groups.shp
b. Geometry is a Multirolvgon or all Parcels in the Group (Block)
STREET is set to the parsed Street Name
CITY is set to the parsed City Name
ZIP is set to the parsed Zip Code
f. With the Proximity Group ID as property PROX_GROUP
Unique ID based on index of Group iteration
g. Algorithm set to PLV
runCITW (Cabin in the Woods)
This algorithm iterates through all non-normalized Parcels and tries to validate them against other nearby Parcels into Proximity Groups.
1. Iterate through all Parcels and Points in the input Shapefiles, and gather all Zip codes.
•This is a good place to consider optimizing, as iterating through these files multiple times (here and later) is a vertormance hit.
2. Then iterate through the Parcel and Point files again, multiple times, once per Zip Code
•From here on, processing is done on data (Addresses) within a Zip Code
b. Ihis makes memory usage smaller. but causes multiple loops through the tiles and thus causing some performance degradation due to IO. So a performance evaluation here
mignt be worthwnile
c. It would be faster to load all addresses up front, and then iterate through them, but the
memory tootprint might be high.
3. Gather all the "best" Addresses for that Zip code, grouped by "best" Street
Iterate through all Addresses in that Zip Code, and "merge" addresses with Addresses differing
only by an enaing unit number
5. Create parcel proximity grouping.
                                                                                    * ﻿﻿﻿Set PROX_GROUP (Proximity Group) property on each Parcel to a unique grouplndex for
all addresses having at least minLinkedParcels parcels within maxProximity distance to each other.
                                                                                    * ﻿﻿﻿This is a complicated multi-pass calculation, but doesn't seem to be a performance bottleneck itself.
6. If there are anv Parcels that could have been grouped together. but were not assigned to a Proximity Group, do another massive calculatior
ry to find a group to add the orphan Parcels t
b. Merge Proximity Groups, if doing so would include orphan Parcels and still be within
tolerances
7. For each Parcel, if its Proximity Group has too few Parcels, remove it from the Proximity Group
and discard the Proximity Group entirely.
8. For each Parcel in a Proximity Group, remove any Parcels that aren't part of a rural area
a. IE: Only keep Parcels that are in or intersect a Forest in the Forest Shapefile
b. Parcels that aren't in a Forest are ungrouped
J. Again, ungroup Parcels whose Proximity Group has tewer than minLinkedParcels parcels
O. Due to some questionable groupindex calculations, it appears that some Proximity Groups do ot get Processed. This is likely a bug
                                                                                       1. ﻿﻿﻿﻿Merge all remaining Parcel Geometries for each Proximity Group into a MultiPolygon
                                                                                       2. ﻿﻿﻿﻿Each Proximity Group is then written to the Groups Shapefile
C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<fips>_groups.shp
With the same properties as the first Parcel in the Group
C. With the Proximity Group ID as property PROX_GROUP
Unique ID based on index of Group iteration
d. Algorithm set to CITW
e. STREET is set to the "best" street, calculated earlier in this algorithm, not the street from the original Shapefile
f. Geometry is a MultiPolygon of all Parcels in the Proximity Group
Gaps
• Log messages seem sufficient, but there is no timestamp on messages, so it's hard to tell where the time is being spent. There are timers around overall method calls, so it isn't all bad.
Mixing or logging meinoas ana synchronization.see Logging, above
: Some of the logic is convoluted and possibly incorrect or busy. There is also lots of logic and processing that is done and then thrown away, or files get deleted.
• The output Group Shapefiles have no references from any Parcels, as no Parcel Shapefiles are
outout deleted ov the orogram as it finishes). How much value do thev add?
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will Inherently be adopted.
Stability. Critical level 1: Unity logging output rather than having a mishmashi nchronized/un-sychronized and cout/wcout calls. Otherwise it is clunky, buggy and can cau:
log corruption
• Instrumentation/Audit Log. Critical level 1: Add timestamps to logging.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger
• Stability, Speed. Critical level 2: Avoiding calling unused business logic, and avoid creating files
that get deleted at exit
• This could potentially increase performance by a large amount.
1.10.1.5 Renormalize
Description
ParcelNormalizer. exe executed for a single FIPS with the REPROCESS flag set. This task tries to Normalize
Parcels that have not been Normalized so far, but this time using the Groups/Blocks generated in the orevious ParcelGroun (Cabin in the Woods) Task. It then writes the Normalized Addresses to the DB.
Git Location
C# Application - ParcelNormalizer.exe
gdsbuilders/Parcels/ParcelNormalizer/HeadlessParcelProcessor.cs
CLR Dependencies
Normalization is done in Parcellib, which is C++ code accessed through the CLR.
Parcels are passed from C# into the C++ code, and resultant Address data is passed back the other way.
Inputs
                                                                                       * ﻿﻿REPROCESS
                                                                                       * ﻿﻿Used to tell the application to do Renormalization rather than the default Normalize
used in the previous Task
                                                                                       * ﻿﻿FIPS
0 5-character FIPS County Code
                                                                                       * ﻿﻿COUNTYNAME
Name of the County, as per the CountyName column in DB Table tbl _Jobs
                                                                                       * ﻿﻿PARCELSINI
Path to the Parcels ini file, which contains other needed properties
                                                                                          * ﻿﻿NUMIHREADS
                                                                                          * ﻿﻿Number of narallel threads to lise
• Usually one of: 5, 3 or 1, depending on the number of Parcels in the County
From Parcels.ini
Reprocessdfp
Directory containing Parcel file that is used to group non-Normalized Parcels into Groups
• C:/PxPointDataBuild/2024q4/working/parcel/normalize/
• Looking for Group Shapefiles in
C:/PxPointDataBuild/2024q4/working/parcel/normalize/groups/<FIPS>_groups.s
                                                                                          * ﻿﻿ReprocessOverride
                                                                                          * ﻿﻿File containing mappings of Parcel IDs to Addresses
                                                                                          * ﻿﻿These are used to override the Addresses of specific Parcels to a hardcoded value
This file is required to exist.
                                                                                          * ﻿﻿Must be in the ReprocessDfp directory, with a file of this name
So if the ReprocessOverride value is
• There must be a file located in
• C:/PxPointDataBuild/2024q4/working/parcel/normalize/ReprocessOverride.txt
Outputs
Writes Address data to the DB, as well as Streets, Cities, Block/Group and similar data.
Logging
C# code logs start and stop messages, but lack sufficient context information. It also logs errors that
occur. and the error message.
he C++ Logging is tolerable, but lacks detail and timing information, as do most C++ Logs so fa aving logging go to a File is also preferable, than just shuttling it across stdout from C++ to CH, Whic
gets lost it the application crashes or the lask is restarted.
Dependencies on SQL Server
Writes processed Address data to several Tables.
Also updates the status (Problem Code) of each Parcel as it completes, in the tbl RawParcels table
• tb|_RawParcels
Queried for Parcels to process
                                                                                             * ﻿﻿Upaatea with status changes
                                                                                             * ﻿﻿ProblemCode - Column I
• Contains ProblemCode indicating Address status and source
• Processed - Column
Indicated Processing complete or talled
• ProcessedOn - Column
DateTime processing was completed
• tbI CityCollections
• tb|_Cities
tbl_Addresses
                                                                                             * ﻿﻿tbl_ StreetCollections
                                                                                             * ﻿﻿tbi Streets
Details
This Task iterates through all non-Normalized Parcels, and tries to Normalize them using the Groups
generated in the previous ParcelGroup (Cabin in the Woods) Task
1. Loops through all non-normalized parcels
The next steps are all per each individual Parcel
                                                                                             1. ﻿﻿﻿Replace the Parcel Address with values from ReprocessOverride if possible
                                                                                             2. ﻿﻿﻿Parse the Address, and determine its "base" addresses using many different algorithms
a. computes all the parsea permutations or an adaress.
4. Municipal and Zip Code transforms done for Addresses that had a ReprocessOverride
a. This reads other Shapefiles listed in Parcels.ini, like ZipPolygons and Municipal
snaperlies
•b. This ends Renormalization for Parcels with a Reprocessoverride
5. For Parcels without hardcoded overrides, loop through intersecting parcel groups
a. This reads the Groups Shapefile created in the previous Task
6. rind Parcel Groups that match the Street of the a Parcell
a. This includes checking many properties of the Street, and different permutations
                                                                                             1. ﻿﻿﻿Fill in city/state/zip information using the zip code and municipal Shapefiles
This reads other Shapefiles listed in Parcels.ini, like ZipPolygons and Municipal
Shanefiles
                                                                                             2. ﻿﻿﻿Select the best parsed "base" address
                                                                                             3. ﻿﻿﻿More calculations on units and fractional house numbers
10. Load all "alternate" cities associated with the base" Street/Address
11. If Parcel matches no groups, fill in city/state/zip information using the zip code and municipal
Shapefiles
a. This reads other Shapefiles listed in Parcels.ini. like ZipPolygons and Municipal
Shapefiles
b. This is the same as above, for Parcels who have valid "base" addresses
c. Inis seems reaundant ana unnecessarv, as Parcels wno tall validation don t get updated in the DB
12. Copy the Parcel Address info back across the CLR boundary
I3. If Normalization was successful, insert the updated Address to the DE
a. Add the Address to the tbI_Address table
b. Insert/Update related City and Street tables
Update the Parcel in tb_ RawParcels column ProblemCode to PPC_Group (2)
d. IE: Mark the Parcel as verified using neighbors in a Proximity Group See Problem Codes above
14. Parcels that failed Normalization are left untouched in the DB
Gaps
terate through all non-normalized Parcels, and compare each to all intersecting Groups. This seem edundant, as these same Parcels were used in the previous step to create the Groups in the first place
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Speed. Critical level 2: If possible, allow the previous step to emit the Parcels with the GroupIDs set. Then load the Parcels directly into the DB in this step, instead of iterating through them all
• Stability, Speed. Critical level 3: This is an ideal case where the CLR is only there to avoid having
to access the DB from C++. If this C++ logic is too complicated to rewrite, then using native C++ libraries to read and write to the DB might be ideal.
1.11 rebuildNormalizelndexes
Description
Adds DB Indexes to several DB lables.
Git Location
ParcelStep in Parce|BuilderNew C# application gdsbuilders/Parcels/ParcelBuilderNew/RebuildIndexes.cs
Calls SQL Script: gdsbuilders/Parcels/ParcelBuilderNew/SQL/BuildPxyLoadIndexes.sql
CLR Dependencies
None
Inputs
DB connection into from global Settings
Outputs
Modified DB Tables
Logging
Start and complete/error messages to the Ul only.
No progress undates or context in error messages.
Dependencies on SQL Server
Tables Modified:
tbl Addresse
• tbl_Cities
tbl Links
: tbI aparcels
Details
Executes the gdsbuilders/Parcels/ParcelBuilderNew/SQL/BuildNormalizelndexes.sal script that adds DB
Indexes to several DB Tables.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherently be adopted.
• Instrumentation/Audit Log. Critical level 1: Add messages, especially around timing, to a log file.
Other Recommendations: These are additional recommendations which would need to be added
ABOVE and BEYOND a standard logger.
• Speed. Critical level 2: Split up the index calls, updating progress when each completes.
1.12 LoadPxyFiles
Description
Task that calls an external application, ParcelBuilderLoadPxyStep.exe, and waits for it to complete.
Git Location
Parcelstep in ParcelbullderNew C# application
gdsbuilders/Parcels/ParcelBuilderNew/LoadPxyFiles.cs
CLR Dependencies
None
Inputs
• List of Steps
Is controlled by the LoadPxyFiles Step, which is enabled by the "Load Pxy Files"
checkbox in the Ul.
• DatasetBUcCategorv
• One of Parcel or ParcelPlus
Controlled by a radio button in the Ul
• Global Settings, many of which are passed into the downstream application
Outputs
None
But downstream applications will have side effects.
Logging
There is no logging.
All Teedback is througn ul loasts/Messages, indicating that this step is executing the downstreal executable, and when it finishes
Dependencies on SQL Server
No direct dependencies, but calls an external application that does access the DB.
Details
Only executes if the LoadPxyFiles Step is enabled.
: This Pacelstep simply wrap a call to Pare BuilderP repareParcels Step. exe, which orchestrates
building the PXY files.
Gaps
• There is no file logging at all, only messages in the Ul.
Recommendations
Other Recommendations: These are additional recommendations which would need to be added
AbOVE and BLYOND a standard loggen
• Instrumentation/Audit Log. Critical level 1: Add log messages, with both behavior and errors
1.12.1 ParcelBuilderLoadPxyStep.exe
Description
This Task orchestrates parallel calls to ParcelLoad4G.exe, one per FIPS. It has a limited number of executions (processes) running at a time, and starts new processes/FIPS as others complete. It then
Git Location
C# Application
gdsbuilders/Parcels/Parce|BuilderLoadPxyStep/LoadPxyStepMainWindow.xaml.cs
CLR Dependencies
None, but some of the downstream applications use it.
Inputs
• STARTJOBID
• First Job ID to Load
• ENDJOBID
Last Job ID to Load
• DB SERVER
Database Server address
                                                                                                * ﻿﻿DB_USERNAME
Username for connecting to the SQL Server Database
                                                                                                * ﻿﻿DB_PASSWORD
Password for connecting to the SQL Server Database
                                                                                                * ﻿﻿PARCELDB
Parcel Database to use
• DATASETBUILDCATEGORY
One of Parcel or ParcelPlus, as passed in from the LoadPxyStep Step, based on the
DatasetBul dCategorv retrieved trom the Ui
                                                                                                   * ﻿﻿OUTPUTDIR
                                                                                                   * ﻿﻿Path to where the PXY files should be written
Either
• C:/PxPointDataBuild/2024q4/dataset/working/parcel/oxvs ParcelPlus/
o Or.
• C:/PxPointDataBuild/2024q4/dataset/working/parcel/pxys_Parcel/
• Depending on the DAlAstIbUILDCAlLGorY above
                                                                                                   * ﻿﻿REFDATADIR
                                                                                                   * ﻿﻿Path containing external datasets, like the Licensefile
• C:/PxPointDataBuild/2024q4/dataset/
Outputs
No direct outputs. out downstream applications will have side eftects.
Logging
None.
All feedback is done through the DataTable in the Ul, where each row represents a FIPS code and a Processed Code, and other information related to them.
Dependencies on SQL Server
                                                                                                   * ﻿﻿tb_RawParcels
                                                                                                   * ﻿﻿tbl_Jobs.
Details
This Application processes each Job, based on Job ID, in parallel.
Each Job is run in parallel with up to NumProcessors (up to 12, but can be controlled from the Ul)
numoer or parallel unreaas running at once, with otners walting for a tree Inread to run on.
And when one Job ID completes, it starts processing another Job ID until all complete.
The downstream calls to ParcelLoad4G.exe will generate 'tmp' files for whatever FIPS it is processing.
And once that rips is tinished. that tile is renamed to some tinal output name. This application looks tor that final output name, and thus won't re-run if it exists. This prevents completed FIPS from being run again. The only way to re-run a FIPS is to re-launch the application.
1. Queries the status of all Jobs (FIPS/Counties)
It counts the total number of Parcels in each FIPS
b. It also counts the number of Parcels for each Processed Code
This is disolaved in the Ul Data lable
2. Starts executing Jobs, trying to intersperse larger and smaller Jobs together in parallel
This is based on the total Parcel counts of each FIPS, and some arbitrary maximum (10
million number or Parcels it wants to be executing at once
                                                                                                   1. ﻿﻿﻿When a FIPS gets assigned to an open Thread/Slot, it executes ParcelLoad4G.exe for that FIPS
                                                                                                   2. ﻿﻿﻿When any call to ParcelLoad4G.exe completes, another FIPS is selected to run, until all are
tinished.
Gaps
No Telemetry around what Job is being processed, and duration of that processing. limited feedback of progress when loading. Only feedback is in the Ul Datatable, which has one
row ver Job ver Processed Code
• No logging to files at all.
Recommendations
Update to standard logger: If CoreLogic implements a standard logger, THEN all of the following
recommendations will inherently be adopted.
: intrumention/Audit log. Crical lvel r: Ad message, specal round ining, no a log file.
they all go to log files as well as Standard Out.
Stablry, speed. ste in el 1 eur aroun at sundast manolo i especial y
the duration of each call
Other Recommendations: These are additional recommendations which would need to be added
AbOV- and BLYONd a standard logger.
• Instrumentation/Audit Log. Critical level 1: Add more log messages and more context to each message, including what FIPS and/or Subtask it is processing.
1.12.1.1 ParcelLoad4G.exe
Description
This application is executed individually for each FIPS and generates a PXY file containing the data for Parcels in that FIPS.
Git Location
C# Application - ParcelLoad4G.exe
gdsbuilders/Parcels/ParcelLoad4G/ParcelLoad4GMain.cs
CLR Dependencies
All Parcel calculations are done in C#, and then passed through the CLR and written to the output PXY file using the PxpDSLoader C++ library.
Inputs
• FIPS
                                                                                                   * ﻿﻿5-character FIPS County Code
                                                                                                   * ﻿﻿CountvName
Name of the County (as defined in the tb _Jobs Table)
• OutputDir
• Directory tor writing Pxy tiles.
C:/PxPointDataBulld/2024q4/dataset/working/parcel/pxys ParcelPlus/
Each instance of this application will write to a different PXY file
• C:/PxPointDataBuild/2024q4/dataset/working/parcel/pxys_ParcelPlus/<FIPS>.pxy
DatabaseServer
• ParcelD
PxPointDataDir
Directory containing external datasets
• Mostly unused except for the Licensefile
                                                                                                   * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/
                                                                                                   * ﻿﻿DatasetBuildCategory
Normally will be ParcelPlus, but could be Parcel
                                                                                                   * ﻿﻿DatabaseUsername
                                                                                                   * ﻿﻿DatabasePassword
Outputs
The output PXY File.
There will also be an output file with a "_tmp" extension while the app is running, but will be renamed to the actual output PXY filename, if the application completes successfully.
                                                                                                   * ﻿﻿Initially
                                                                                                   * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/working/parcel/pxys_ParcelPlus/<FIPS>.pxy_tmp
                                                                                                   * ﻿﻿If successful, renamed to
                                                                                                   * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/working/parcel/pxys ParcelPlus/<FIPS>.pxy
Logging
Custom C++ file-based logger, which is written to by both the C# and C++ code.
The file name is based on the FIPS, thus there is 1 file per FIPS and no collisions across FIPS.
This log file file is deleted and rewritten for every execution of this Task for a given FIPS; no log rotation
or previous nistory
Also, there are no common context values written to the log, like timestamps or where in the code the message occurred; just the message itself.
The file also has no size constraints, and thus will grow without bounds for the given execution.
None of these are critical problems, but using a Standard Logger will completely fix all of them.
Very little is actually written to the log for the first 2 Threads (RunParcelReader and RunParcelMerger) other than start and stop messages for each Zip Code.
Dependencies on SQL Server
• tol Addresses
• tb Job: tbl_Links
tb|_RawParcels
                                                                                                   * ﻿﻿tbl Streets
                                                                                                   * ﻿﻿tbl StreetsXStreetCollections
Details
Fires off 3 separate independent Threads. One to read data from the DB (RunParcelReader), one to transtorm the data (RunParcelMerger and one to write the data to the outout Pxy file
(RunParcelWriter). This is basically custom "Reactive Programming" and trying to make the application execute faster, since the Thread that reads the data operates independently of the Thread writing the
data.
The output file will have a "_tmp" extension to its name while it is being populated. And that extension
is removed when the file is complete. thus indicating the processing was successful. If the application is run again, and the completed (without the "_tmp" extension) file exists, it will do nothing and immediately halt.
There is a 72 hour timeout built into the application, and it will automatically halt after that time, if it has not completed by then.
1. Load all Zip Codes in the County from the tb _Addresses DB Table
2. Load all Cities in the County from the tbl_Cities DB Table
3. Start Reader Thread, asynchronously
a. Calls RunParcelReader (see below)
4. Start Merger Thread. asnchronousiv
a. Calls RunParcelMerger (see below)
5. Load all Links for all Parcels in the County from the tb_Links DB Table
6. Load all Streets for all Parcels in the County from the tbI Streets and tb|_StreetsStreetCollections DB Tables
7. Start Writer thread, asynchronously
a. Calls RunParcelWriter (see below)
RunParcelReader
Queries Parcels and Addresses from the Db, grouped by Lip Code, and forwards them on to the RunParcelMerger Thread
                                                                                                   * ﻿﻿Queries the v_pxyLoad DB View for Parcels and Addresses for each Zip Code
                                                                                                   * ﻿﻿The v_pxyLoad View is a JOIN of tbl _rawParcels and tbl_Addresses Tables
                                                                                                   * ﻿﻿It forwards each group Parcels, which is all Parcels in a Zip Code, to RunParcelMergel
                                                                                                   * ﻿﻿It limits the number of in-flight Zip Code groups to 3, thus at most 3 groups of Parcels will be in
memory at a time.
• It will only load and forward Parcels for a Zip Code once it has room to forward the group
downstream.
                                                                                                   * ﻿﻿This is done to prevent too many Parcels from being loaded into memory at once.
                                                                                                   * ﻿﻿If 3 groups of Parcels (Zip Code groups) are in flight, this thread will sleep 30 seconds and try again.
RunParcelMerger
For each Zip Code group of Parcels, creates trees (graphs) of related Parcels, where the root of the tree is the Address with merged geometry of all Parcels at that Address, and the leaves of the tree are all the
Parcels at that Adaress.
This is a very complex set of calculations, and not easy to describe.
• Processes one "Lip Code group of Parcels, which is all Parcels in a single Lip Code, at a time.
                                                                                                   * ﻿﻿Find all Parcels in that "Zip Code group" that are part of the same Apartment Complex, MDU or
Street Segment ("Address Range")
                                                                                                   * ﻿﻿IE: Parcels with different Parcel IDs, but the same Street and Base Address and minimum
"Unit" or Address number (AddressLow)
Grouping is also sub-divided by sub-ranges; which often means by floor, where unit number ranges each have multiple units. IE 101-104, 201-204, etc... We can call each of
these a "Kange Group"
• Some MDUs have multiple Zip codes with the same complex; and this current code/algorithm might be insufficient.
• For each "Range Group" above, create a tree of "Master" and "Slave" records
The "Master" is the root of a tree, and contains the merged geometry of all "Slave" Parcels, as well as the Address, but without any unit or range information.
                                                                                                      * ﻿﻿The leaves of the tree are called "Slaves" and represent all Parcels in that "Range Group"
                                                                                                      * ﻿﻿For each "Solo" Parcel not matched earlier, create a "Master" and possible "Slave" record for it
as well.
                                                                                                      * ﻿﻿Do some more Address and Unit-number massaging, filtering of duplicate records, etc...
                                                                                                      * ﻿﻿Then, all the trees of Master and Slaves created for a Zip Code are forwarded to RunParcelWriter
RunParcelWriter
For each group of Master and Slave trees for a Zip Code, write out each tree and all related Links to the output PXY file.
1. For each "Master" Record, calculate its Aggregation Type, and the AggregationType of any Slave
Parcels (Children)
                                                                                                         * ﻿﻿﻿Range - Has "Address Range" values, but is not a Unit
                                                                                                         * ﻿﻿﻿Unique - Has no "Address Range" or Unit value; is "Solo"
                                                                                                         * ﻿﻿﻿Unit- is a Unit in a larger Complex, as distinct from an "Address Range"
2. For each "Master" Record, calculate the maximum-distance for each LinkType, across all Slave
Parcels
                                                                                                         * ﻿﻿﻿Links and LinkTypes are from the tbl_Links
                                                                                                         * ﻿﻿﻿LinkType is one of: BLOCK, APN, STRUCT or PARCEL
3. Write the Master to the output PXY file
a. This includes AggregationType, all calculated Link information and associated Parcel
Addresses or Slaves, etc..
4. Write each Slave to the output PXY file
a. This includes AggregationType, unmodified Link information (from tbl_Links) Parcel Addresses for all overlapping/duplicate Parcels, etc..
5. Writing a Record to the Pxy tlle is a very complex process
                                                                                                         * ﻿﻿﻿The library for writing to PXY files is in C++
                                                                                                         * ﻿﻿﻿Thus, the CLR boundary between C# and C++ code is crossed here, many times
                                                                                                         * ﻿﻿﻿Master and slaves are added to Pxy segments
                                                                                                         * ﻿﻿﻿Addresses. Units. Links. Streets, Cities. Counties and dozens of other datapoints are written to each Segment
                                                                                                         * ﻿﻿﻿This code writes out each Segment in the proprietary PXY format
                                                                                                         * ﻿﻿﻿This is tens of thousands of lines of code to write out each Segment, and is extremely complex, and thus not detailed here.
Gaps
                                                                                                         * ﻿﻿Only the third Thread (RunParcelWriter) is tracked at all, and the first 2 Threads are fire-and-forget. Thus, if the User tries to cancel the operation, those Threads will not be informed, and will keep running until they complete.
                                                                                                         * ﻿﻿This attempt at Reactive Programming also has some painful and unintuitive race conditions in it, where only when the RunParcelWriter is finished does the application exit (or due to timeout). This works only because when a User tries to cancel the application, those other Threads implicitly end because the Application itself stops. Trying to diagnose timing and synchronization issues with this code would be a nightmare.
                                                                                                         * ﻿﻿This application looks for the final output name, without the "_tmp" extension, and thus won't
re-run if it exists. This prevents completed FIPS from being run again without the User manually deleting or moving the file.
                                                                                                         * ﻿﻿Logging is better than in some other places, and actually writes to a file and contains timing information between major calls, but is also overly complex and convoluted.
                                                                                                         * ﻿﻿The CLR crossover from C# to C++ is also overly complex, and the business logic is on the C# side as well as on the C++ side. This will be a challenge when eliminating CLR
Recommendations
• Using a Standard Logger will remove a lot of logging complexity and brittleness.
• The overuse of the CLR will be challenging to untangle, and a lot of the business logic is in C# and so migrating it to something else, like C++ or otherwise, will be a heavy lift.
                                                                                                            * ﻿﻿That said, trying to lift the PY library out of C++ would be even harder.
                                                                                                            * ﻿﻿So the recommendation is to, in later Phases, rewrite the LoadPXY C# logic but keep using the
Pxpusloader C++ lIbrary.
• This will be critical when we reach the Phase where we move to an Orchestrator.
1.13 MergePxyFiles
Description
Git Location
Task that calls an external application, Street/ndexer.exe, with the PXYMERGER flag and waits for it to complete. That subtask merges together all previously generated per-FIPS PXY files into a single PXY file.
ParcelStep in ParcelBuilderNew C# application
gdsbuilders/Parcels/ParcelBuilderNew/MergePxyFiles.cs
CLR Dependencies
none
Inputs
• List of Steps
• Is controlled by the MergePxyBlocks Step, which is controlled by the "Merge Pxy Blocks" checkbox in the Ul.
                                                                                                            * ﻿﻿DatasetBuildCategory
                                                                                                            * ﻿﻿One of Parcel or ParcelPlus
                                                                                                            * ﻿﻿Controlled by a radio button in the UI
                                                                                                            * ﻿﻿Global Settings, many of which are passed into the downstream application.
Outputs
None
Logging
There is no file-based logging.
All feedback is through Ul Toasts/Messages, indicating that this Step is executing the downstream executable, and when it finishes.
Dependencies on SQL Server
Ione
Details
This ParcelStep simply wraps a call to Street/ndexer.exe, which orchestrates merging the PXY files into a
                                                                                                            1. ﻿﻿﻿Checks if the MergePxyBlocks Step is enabled.
                                                                                                            2. ﻿﻿﻿Deletes all previously generated merged output files and associated logs.
                                                                                                            3. ﻿﻿﻿Calls Street/ndexer,exe with the appropriate arguments (see below)
                                                                                                            4. ﻿﻿﻿The PXYMERGER flag so the Streetindexer.exe knows to merge PXY files
                                                                                                            5. ﻿﻿﻿Path containing the per-FIPS PXY files, based on DatasetBuildCategory
                                                                                                            6. ﻿﻿﻿File path to write the merged PXY files into, based on DatasetBuild Category
d. Reterence tile directorv. used mostiv to tind the Licensefile
Gaps
                                                                                                            * ﻿﻿There is no file logging at all, only messages in the Ul.
                                                                                                            * ﻿﻿Streaming stdout from the subprocess to a file is buffered, and thus may not get written if an
error occurs.
• Some tile overations ignore errors.
Recommendations
                                                                                                            * ﻿﻿Fix logging redirect to use a Standard Logger rather than manually piping stdout from the subprocess to a file.
                                                                                                            * ﻿﻿Have the subprocess write to its own log file rather than stdout, except maybe some basic start/stop messages to stdout.
                                                                                                            * ﻿﻿Capture and handle all errors, and do not ignore them.
1.13.1 StreetIndexer.exe - PXYMERGER
Description
This merges all FIPS-specific PXY files into a single PXY file.
This application also has other functionality that can be used for other purposes.
Git Location
（++ Appllcatlon - Streetindexer.exe
gdsbuilders/StreetIndexer/Street/ndexerMain.cpp
CLR Dependencies
None
Inputs
• PXYMERGER
This flag tells the application to merge PXY files.
There are other flags that can be passed to trigger this application to perform other behavior or actions.
• Directory containing reference datasets
• Mostly unused except for the License File, and the date of the Reference File
O
                                                                                                            * File path of the output merged PXY file
                                                                                                            * ﻿﻿С:/PxPointDataBuild/2024q4/dataset/working/parcel/output/parcel_us.pxy
: Ci/PxPointDataBuild/2024q4/dataset/working/parcel/output/parcelplus us.pxy
• Directory containing PXY files to merge, each file containing data for a single FIPS.
                                                                                                            * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/working/parcel/pxys_ Parcel/ C./PPointDataBuild/202404/dataset/working/parcel/pys Parcel Plus/
                                                                                                            * ﻿﻿PLUS
Uptional rlag
• If set, indicates a ParcelPlus build, otherwise it is assumed to be a Parcel build
Outputs
PXY File containing the merged data from all per-FIPS PXY files.
The output file will have a "_temp" extension until the merge completes, and then will be renamed to the actual output file name if the merge was successful.
Logging
There are a few wcout) calls around major events, but no file-based logging, and no context. Using a Standard Logger would fix many related weaknesses, as well as support file-based logging and rotation.
Dependencies on SQL Server
None
Details
This copies data from all FIPS-specific PXY files into a single merged PXY file.
1. Delete the existing output file, if it exists
L. Create an output Tile With the temp extension
a. This will truncate any existing tile of the same name, if it exists.
3. Iterate through each input PXY file and check the "pageShift" and headers of each PXY file
a. Basically, make sure each file has the same internal alignment and structure
4. Iterate through each input PXY tile again, and copy its content to the output PXY Tile
                                                                                                            * ﻿﻿﻿Copy the internal "table" definitions (not DB Tables, but PXY lables)
                                                                                                            * ﻿﻿﻿Copy the now-aligned "pages" of content, one at a time
c. This is where the heaviest data copying occurs
5. Set the "Plus" header to "1" it the PLUS input argument was set
6. Rename the output file, removing the "_temp" suffix.
Gaps
• A Standard Logger, along with more detailed log messages, would give better insight into the
behavior and performance of the application.
                                                                                                            * ﻿﻿The read from the input PXY files, and the write to the output PXY file, is linear and synchronous.
This can be a performance bottleneck when there are many large files to process.
                                                                                                            * ﻿﻿There are some exceptions that get caught and logged, but then are ignored
Recommendations
• Fix logging redirect to use a Standard Logger rather than manually piping stdout from the
subprocess to a tlle.
                                                                                                               * ﻿﻿This will also give better insight into the behavior, timing and performance of the application.
                                                                                                               * ﻿﻿Add more log messages at different log levels to make debugging and diagnosing problems
easier.
• The single-threaded nature of this processing does not seem easily parallelizable, and there may be no way to refactor the code to make it parallelizable at this time.
1.14 IndexPxyFiles
Description
Task that calls an external application, Street/ndexer exe with the ALLINDEX flag, and waits for it to
complete. The subtask will create an "Index" for the previously merged PXY file, in order to make queries into that Pxy file easier and taster.
Git Location
Parcelstep in Parcelbulldernew C# application
gdsbuilders/Parcels/Parce|BuilderNew/IndexPxyFiles.cs
CLR Dependencies
None
Inputs
• LIst or steps.
                                                                                                               * ﻿﻿Is controlled by the IndexPxyFile Step, which is controlled by the "Index Pxy File'
checkbox in the Uil
                                                                                                               * ﻿﻿DatasetBuildCategory
                                                                                                                  * ﻿﻿One of Parcel or ParcelPlus
                                                                                                                  * ﻿﻿Controlled by a radio button in the Ul
• Quarter
                                                                                                                  * The name of the vearlv Quarter of this buili
                                                                                                                  * ﻿﻿Defined by the "Quarter" text field in the U
                                                                                                                  * ﻿﻿Something like "2024-04"
• Global Settings, many of which are passed into the downstream application.
Outputs
lone
Logging
There is no file-based logging.
All feedback is through Ul Toasts/Messages, indicating that this Step is executing the downstream
Dependencies on SQL Server
None
Details
This ParcelStep simply wraps a call to Streetindexer.exe, which orchestrates creating an index file for the
previousty merged Pxy the
                                                                                                                  1. ﻿﻿﻿Checks if the IndexPxyFile Step is enabled.
                                                                                                                  2. ﻿﻿﻿Deletes all previously generated index and associated logs.
                                                                                                                  3. ﻿﻿﻿Calls Streetlndexer.exe with the appropriate arguments (see below)
a. The ALLINDEX flag so the Streetindexer.exe knows to index the merged PXY file
                                                                                                                  * ﻿﻿﻿Full path to the merged PXY file to use as input
                                                                                                                  * ﻿﻿﻿Filename to write the merged PXY files into.
                                                                                                                  * ﻿﻿﻿Both input and output tile are usually in the same directory
                                                                                                                  * ﻿﻿﻿Reference file directory, used mostly to find the Licensefile The Quarter the build is for
Gaps
: trees no file le ring tall only mess as in the u.
Streaming stdout from the subprocess to a file is buffered, and thus may not get written if an
error occurs.
• Some file overations ignore errors.
Recommendations
                                                                                                                  * ﻿﻿Fix logging redirect to use a Standard Logger rather than manually piping stdout from the subprocess to a file.
                                                                                                                  * ﻿﻿Have the subprocess write to its own log file rather than stdout, except maybe some basic
start/stop messages to stdout.
• Capture and handle all errors, and do not ignore them.
1.14.1 Streetlndexer.exe - ALLINDEX
Description
This creates a GDX Index file for the merged PXY file.
This application also has other functionality that can be used for other purposes.
Git Location
C++ Application - StreetIndexer.exe
gdsbuilders/Streetlndexer/StreetIndexerMain.cpp
CLR Dependencies
None
Inputs
• ALLINDEX I
                                                                                                                  * ﻿﻿This flag tells the application to create a GDX Index of a PXY file
                                                                                                                  * ﻿﻿There are other flags that can be passed to trigger this application to perform othe
benavior or actions.
                                                                                                                  * ﻿﻿Directory containing reference datasets
                                                                                                                  * ﻿﻿Mostly unused except for the License File, and the date of the Reference File
                                                                                                                  * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/
File path of the output index file
                                                                                                                  * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/working/parcel/output/parcel us.gdx /xPoint@ataBuld/20244/dataset/warking/arcel/output/arcellus. S./PxPoint@0ataBuld/202404/dataset/warking/arcel/outpurt/arcelblus.us.edx
                                                                                                                  * ﻿﻿i
File path containing PXY file to index.
                                                                                                                  * ﻿﻿C:/PxPointDataBuild/2024q4/dataset/working/parcel/output/parcel us.pxy
                                                                                                                  * ﻿﻿q
C./xPointDataßuild/202404/dataset/working/parcel/output/parcelolus_us.DX
                                                                                                                     * ﻿﻿Yearly quarter this build is for
                                                                                                                     * ﻿﻿Example: 2024-Q4
                                                                                                                     * ﻿﻿PLUS
                                                                                                                     * ﻿﻿Optional Flag
• If set. indicates a ParcelPlus build. otherwise it is assumed to be a Parcel build
• DEFAULTALIAS
                                                                                                                     * ﻿﻿Uniy sent IT IT PLUs Is set o If sent, is always sent as "Parcel: ParcelPlus"
                                                                                                                     * ﻿﻿Otherwise there will be no Default Alias for the Index file
Outputs
Index (GDX) file for the input PXY file.
or ° i/PxPeintDataBulid/20244/dataser/werkine/aarcel/outeut/oarcel.us.dx
• C:/PxPointDataBuild/2024q4/dataset/working/parcel/output/parcelplus.us.gdx
Logging
Console logging using wcout() and cout().
Also has a custom file-based logger that throws an exception if an excess number of data errors occur.
Dependencies on SQL Server
None
Details
This creates a GDX Index file based on the contents of a PXY file.
                                                                                                                     1. ﻿﻿﻿Validate CLI arguments, input files and output filename
                                                                                                                     2. ﻿﻿﻿Delete the existing output GDX file, if it exists
                                                                                                                     3. ﻿﻿﻿Calculate all Layer types
Iterate through all Segment Records in the PXY file
                                                                                                                     1. ﻿﻿﻿Filter out Slave/Child Segments with units
                                                                                                                     2. ﻿﻿﻿Fliter out segments missing required values
Transform each Segment into an Index Segment, flattening ranges and shared ids
8. Write out each Index Segment, grouped by "Page"
9. Write each Segment to the Index
                                                                                                                     1. ﻿﻿﻿﻿Iterate through each Segment in the output GDX file
                                                                                                                     2. ﻿﻿﻿﻿Calculate all Named Index Keys based on these Indexed Records
                                                                                                                     3. ﻿﻿﻿﻿Append each Index Key and associated Name to the GDX Index file.
Gaps
                                                                                                                     * ﻿﻿Some logging to wcout/cout and some to a custom FileLogger, which writes to a file, but lacks
                                                                                                                     * ﻿﻿FileLogger does more than log, but also counts errors and throws an Exception if it exceeds some threshold (100 by default)
Recommendations
                                                                                                                     * ﻿﻿Fix logging redirect to use a Standard Logger
                                                                                                                     * ﻿﻿Verify error handling design/behavior is what we want: ignoring until 100 error
thresnold
                                                                                                                     * ﻿﻿Write custom hook or handler to reproduce the 100-error threshold behavior, if desired.
Preferably handle and fix all errors, instead
                                                                                                                     * ﻿﻿Add more log messages at different log levels to make debugging and diagnosing problems
easier.
• Inis calculation is hugely complex, and so it is not recommended to do anthing with this
application other than add a standard logger and basic or obvious C++ tixes.
Recommendations
Not Covered
1. Handling extreme changes to the upstream data sources, like the (semi-recent) redistricting of
2. Details of the Orchestrator used in Phase 3
3. Telemetry and tracking systems like centralized logs (Elastic), alerting (Pager Duty) and data visualization (Grafana)
                                                                                                                        1. ﻿﻿﻿Techniques to speed up "embarrassingly parallel" bottlenecks in the pipeline, like using Spark, Beam or in-Database transformations (BigQuery).
                                                                                                                        2. ﻿﻿﻿Details of how to replace the C# CLR with native C++ libraries and applications
                                                                                                                        3. ﻿﻿﻿Discussions of replacing the PxPoint C++ libraries entirely
                                                                                                                        4. ﻿﻿﻿Details of the more complicated algorithms, like CabininWoods or ClipCoastlines
                                                                                                                        5. ﻿﻿﻿Level of effort to rewrite those algorithms, without losing all the one-off fixes and tweaks added
over 1s years.
                                                                                                                        * ﻿﻿﻿Rewriting the C++ code is not part of the current recommendations.
                                                                                                                        * ﻿﻿﻿Rewriting the C# code in C++ in order to remove the CLR will need to be done, however.
9. Nagy's suggestion that Normalization itself be re-evaluated, and possibly create a completely
new algorithm to verify Addresses from Parcel Point, as the data content and accuracy has changed a lot over the vears.
Phase 1 - Observability and Code Fixes
Observability
Vithout visibility, the application lacks transparency which makes it difficult to determine what the buil s doing and where it breaks. As we have no transparency into the breakage, end users waste time i trying to tind a fix which might solve the symptom but not the root cause. When something goes wrong, ve have to start Tasks from the beginning, and sometimes even start from scratch entirely. Constant estarting limits speed more so than the number of machines, parallelism, or application performance
by solving Tor the root causes, data processing can run smootniy & rellably tor multiple invocations and improve overall performance.
In summary, the desire is to make the build more transparent about what it is doing and where it breaks.
This will enable faster data fixes and re-processing and less guesswork and stress on the users running
These are the highest priority Recommendations, as visibility and history will give insights into where optimizations need to be added, and where and how often bugs are occurring.
Effort
To implement logging and push notifications should take 3 engineers 3 months to complete.
The majority of the work will actually be around getting the libraries incorporated into the compiled build, setting up the configurations and downstream Slack channels, and testing. Adding logging to the
code should be relatively simple, and will lust take time to add to all the important places in the code.
Risks
As standard logging has been around for manv decades. there should be verv little risk to the stabilitv of e application, or to working as expected. That said, using the C# Logger needs further evaluation as
¡ capability to push messages to the current Ul, and if it is insufficient, then wrapper code will have
be written to send messages to both. Inis will add some small amount or complexitv. but shoulan t compromise stability or efficacy.
Logging
Use a Standard Logger throughout the C++ and C# codebase
The added benefit of logging around Database calls will allow for measuring timing of SQL queries, detect slow queries and identify overall DB performance. This allows for operators to update those slow queries, or fix the DB to improve its performance.
Note that there are also places in the code that use a custom log tile rotation that is limited to 10 tiles or ss. Each time that application starts a new file is created. Once the 10 file limit has been reached, the pplication cannot be run again until those files are moved, renamed or deleted. All of that code shoul
be completely removea, ana replacea with a standara Logger.
• Add a Standard Logger to all related Applications
• Make sure all logging has rotation, timestamps and full context
a. Context like: FIPS, Job ID, Parcel ID, and stacktrace, etc...
                                                                                                                        * ﻿﻿Add INFO log messages around all major events or loops
                                                                                                                        * ﻿﻿Add DEBUG log messages inside important but possibly spammy loops and less critical event
• Log using a standard format, whether plain text and/or JSON
                                                                                                                        * ﻿﻿﻿Plain text is easier for people to read
                                                                                                                        * ﻿﻿﻿JSON is easier to parse using tools and logging systems
• Logging for C++
• tither log4cxx or spalog.
o Both support console and file logging, file rotation, timing, stack-traces, text and JSON
rutmatuno, elt..
• nups://logging.apacne.org/1094CXX
https://github.com/gabime/spdlog
The C++ codebase already has a version of spolog, so migrating the pipeline code to use
It mignt be easier than Introaucing a new lorary.
• Logging for C#
Microsoft.Extensions.Logging
0 Can log to the Ul, console, and files
supports file rotation, timing, stack traces, and text and JSON formatting
https://www.nuget.org/packages/Microsoft.Extensions.Logging
Replace or augment all calls to Trace/Message
Push-oased notiticationsi
Push important messages to Slack
• All major events or loops should push a Slack notification.
                                                                                                                        * ﻿﻿Inis Includes when an event or loop completes
                                                                                                                        * ﻿﻿Make sure the notification has enough context to identify what happened, where and with what.
                                                                                                                        * ﻿﻿This includes significant errors or exceptions
                                                                                                                        * ﻿﻿FIPS, Job ID, Parcel ID, and stacktrace, etc..
                                                                                                                        * ﻿﻿There should be a dedicated Slack Channel allocated for each environment this runs in
                                                                                                                        * ﻿﻿It: One channel for debugging and test bullds, and one channel for production bullds
• Slack Integration for C++
• Slacking
https://github.com/coin-au-carre/slacking
• Slack integration for C#
slack.webnooks
https://www.nuget.org/packages/Slack.Webhooks
Merrics
Use a standard Metrics Revorter and protocoli
                                                                                                                        * ﻿﻿StatsD or DataDog Protocol
                                                                                                                        * ﻿﻿Metrics for C++
                                                                                                                        * ﻿﻿https:/github.com/Darkwvanderer/metrics-cop
                                                                                                                        * ﻿﻿Metrics for C#
https://github.com/DataDog/dogstatsd-csharp-client
• https://www.nuget.org/packages/StatsdClient/
Code Optimization
Code optimizations and fixes can both improve code performance. but also prevent bugs and other unexpected failures
Ine Ub tixes especially coula nave a large Impact on application periormance. But since this Is Phase -, there is less impetus to refactor all queries, but rather focus on the queries that do simple things, and logic in code that could be easily moved into SQL. Later Phases can further optimize DB queries and statements, especially since by then we will have measured their performance, and better be able to
tocus on the biggest bottlenecks.
Refactoring the code to improve DB write stability (but not necessarily performance) in pre-normalization is a critical area, however, as connectivity problems occur there regularly. IE, the code
that writes the output of pre-normalization to the DB regularly has DB connections drop or reset
Inese are the lowest priority recommenaation, as they will Increase applcation performance, but that has been indicated by its users as the least of their concerns. Optimizing the DB Writes at the end of
pre-normalization Is the exception. and needs to be tixed in order to stabilize that part of the bulld.
Effort
Most of the critical C++ code changes will take two to three weeks at most, not counting the logger changes. The less critical code changes could easily take a month or more.
Ine Ub changes are much easier to tind. but much harder to moditv and get correct. Inese may take a couple of months to refactor, especially since they require code changes as well.
KISKS
Code changes always carry higher risk, and this cannot be emphasized enough. The Critical level 1 fixes should be simple enough to implement and verify, but should also be thoroughly tested. And since there
are limited unit tests, the testing would have to be manual. Ine Ub changes are the highest risk, and so should be thoroughly vetted and tested before being released.
Database
• Add logging around all (significant) DB queries and inserts
• This will give implicit telemetry around DB performance
• Refactor DB connections to share a single connection. wherever possible
• Rather than constantly opening and closing connections to the DB
• Ihis can cause connection thrashing, and often manitests as unrelated Ub connections
failing or timing out because the DB is overloaded.
                                                                                                                        * ﻿﻿Refactor DB Inserts or Updates to be done in bulk rather than one at a time
                                                                                                                        * ﻿﻿Most of the code actually does bulk writing, but there are a few places that do not
                                                                                                                        * ﻿﻿rix the Ub updates/inserts during pre-normalization
                                                                                                                        * ﻿﻿This seems to be another place where bulk inserts would help, but the code is so
snoula be tacklea even so
                                                                                                                        * ﻿﻿Refactor DQ queries to have the DB do the sorting and filtering, rather than in code
                                                                                                                        * ﻿﻿This reduces load in the application
                                                                                                                        * ﻿﻿It will lower the memory footprint, as you don't need to load the entire dataset into memory in order to sort it, as you can lazy-load rows.
• It also reduces code complexity and maintenance.
C++
These are mostly language optimizations that will make the code more efficient and easier to maintain.
Some are much more crucial than others, however, and can greatly impact the stability of the svstem All Critical level 1 recommendations should be implemented immediately, but the others are less critical and can be delayed. But if put off for too long, they can impact the performance, stability and
maintainaoility or the system
Critical level 1: Replace console and Ul logging with a Standard Logger
neplace all console logging uses or cout toninu crt)
This will fix all log corruption, timing, buffering and synchronization issues
• It also obviates the need to add redundant context to the messages (timing information location, etc)
                                                                                                                           * ﻿﻿This is in addition to adding more logging throughout the applications, not just where
console logging currently exists.
                                                                                                                           * ﻿﻿Critical level 1: Throw and catch Exceptions by reference
• Never throw a pointer to an Exception, but instead throw ov reterence.
                                                                                                                              * ﻿﻿Also, catch by reference, rather than catching a pointer to an Exception.
                                                                                                                              * ﻿﻿This is important because most exception handlers do not handle exception pointers,
ana using new means many expectea ana nontatal exceptions will not ve caugnt ana
handled which can potentiallv mess un or abnormallv terminate the abolication.
• This can cause memory leaks, and will emit unnecessary compiler warnings which can
obscure actual important warnings.
• Critical level 1: Use RAll for Mutexes instead of explicit lock) and unlock()
Use something like Locking::Mutex instead of calling lock() and unlock() directly
• |his is Important because locking a Mutex and talling to unlock it can cause race conditions, deadlocks and hard crashes.
• There are many places in the code where something is locked and then manually
unlockea verore une ena or tne tuncuon.
                                                                                                                              * ﻿﻿This is dangerous because functions don't always make it to the end, as sometimes an exception is thrown or an early return occurs, and the Mutex is never unlocked.
                                                                                                                              * ﻿﻿RAll guarantees both the unlocking of a locked Mutex, and also will preserve the order of multible locked Mutexes
• Critical level 1: Use RAll for Files instead of explicit open() and close)
Similar to the RAll Mutex recommendation above.
• In this case, Tiles can occasionally be let open and thus vou get resource and memory leaks.
• Even worse, if you have a file open in windows, other processes wanting to access that
Tle cannot ao so, ana this can cause nara to alagnose probiems In other parts or the
application, or even in other applications.
• Critical level 1: Remove the boost library entirely
This library is completely obsolete and unsupported
• Mixing standard sunchronization classes with boost sunchronization classes can have unpredictable results, including crashes and deadlocks
• Removing it will reduce the complexity and size of the codebase
• There are only a few places it is used, and can be replaced with Locking:: Mutex or similar
• Critical level 2: Reserve memory when populating new vectors.
IE: vector.reserve(size)
foctorse siste vector is going to be before populating it, you can save many
unnecessary memory allocations by reservice memory ahead of time.
• This is especially helpful for large vectors, which can reallocate large chunks of memory
over and over as they grow to fit the values being put into them
• Critical level 2: Use correct format when converting numbers to strings
                                                                                                                              * ﻿﻿Use correct format indicators for numeric types (%llu vs %d vs %zu)
                                                                                                                              * ﻿﻿Less critical than other problems, especially when creating log messages.
                                                                                                                              * But it does mean that numbers converted into strings might be tormatted wrong. and. thus confuse someone looking at the logs, as the number logged differs from the actual number.
                                                                                                                              * ﻿﻿Critical level 2: Use std::vector instead of native C arrays
                                                                                                                              * ﻿﻿This can prevent coding errors like buffer overruns and underruns.
Looping over them is easier, safer and more intuitive (range-based loops, above)
                                                                                                                              * ﻿﻿When a vector goes out of scope, all the things in the vector get destroyed as we Arrays do not do this, and so using dynamic arrays (rather than static) can cause memory
                                                                                                                              * ﻿﻿Critical level 2: Run any related C++ code through a modern linter (IE: Clang-Tidy)
his will catch manv unexpected code problems and bugs, and it tixed, can prevent manv potential problems from ever being in the code in the first place.
Also standardizes the code more, to be more readable and efficient for the compiler.
•This can improve performance and error handling, and puts more work on the compiler and less on the engineer. Thus potentially using less engineering resources
• Critical level 2: Make sure all File paths are OS neutral, rather than using Windows-specific path
separators ana arive names, etc..
• Otherwise migrating to a ditterent olattorm. like Linux. will not work
No part of the pipeline can be migrated to a newer system or platform without this
• Critical level 2: Use std::vector instead of native C arrays
This can prevent coding errors like buffer overruns and underruns.
Looping over them is easier, safer and more intuitive (range-based loops, above)
when a vector goes out or scope, all the things In the vector ger destrovea as well.
Arrays do not do this, and so using dynamic arrays (rather than static) can cause memory
• Critical level 2: Use range-based loops rather than iterators
This is less critical. but can make the code much more intuitive. prevent unnecessarv allocation of temporaries, and make the code much more readable and intuitive.
• It also makes the loop invariant easier to understand, and control. Especially if it should or should not be const (read-only)
• Prevents accidental misuse of iterators, where one compares an iterator to the beginning of the vector rather than the end, or compares to a different vector iterator
entirely
• IE, instead of doing this:
C/C++


for (vector<CityEntry>::const_iterator cityIt = cityEntries.begin();


cityIt != cityEntries.end();


++cityIt) {


const CityEntry& cityEntry = *cityIt;


}


...


• Instead. do this:
C/C++
for (const auto acityentry: cityentries)
• Critical level 3: Pass by reference when possible, even for smart pointers
Avoids having to allocate unnecessary temporary objects
• ror smart pointers this is less critical, but does ada unnecessary wasted cou time
                                                                                                                              * ﻿﻿Critical level 3: Use default destructors, constructors, and copy constructors when possible.
                                                                                                                              * ﻿﻿This is superior to manually writing and maintaining redundant constructors and
destructors
•This includes using default assignment operators ("="
It also prevents some logical errors, where a manually written constructor can accidentally forget to initialize all variables, or a copy constructor tails to copy one or
• Critical level 3: Use correct numerical types, and avoid implicit numerical casting
• Ihis includes tixing implicit casts trom signed<->unsigned or 64<->3Z-oit Integers.
This can cause some rare, but brutally difficult to detect errors to occur This is because, especially for large numbers, casting from signed to and from unsigned
can cause numerical wrapping, where a negative signea number becomes a nuge positive unsigned number during conversion
• Critical level 3: Use POSIX numeric types instead of custom types
                                                                                                                              * ﻿﻿The code overwhelmingly uses non-portable numeric types like int32 and uint64 and even in1
                                                                                                                              * ﻿﻿It is better to use Posix datatypes like Ints<_t, unts<_t and size_"
This is so endemic in the code, I don't recommend doing this everywhere all at once.
but using potentially incorrect macros it means the underiving datatypes may not match the type implied by the name
The int32 macro maps to int but int can have 32 bits on some machines, 64 bits
on others ana even 10 bits on older machines
                                                                                                                              * ﻿﻿Using the built in POSIX types guarantees variable alignment and prevents sneaky casts between incompatible datatypes, which the compiler has a harder time catching
                                                                                                                              * ﻿﻿Using standard datatypes also means the code is portable and POSIX compliant, so migrating from one architecture to another is wav easier
• The code base is smaller, because there are no custom datatype macros being defined
It can also improve performance as implicit casts can consume cpu cycles
• Critical level 3: Use size t numeric type for size of standard containers
                                                                                                                              * ﻿﻿Container sizes are natively size_t which is system dependent
Is usually uint64_t, but on older machines can be uint32_t
                                                                                                                              * ﻿﻿Using int32, or even uint32, for the size of a container can lose accuracy, due to integer
overflow
t also fores thedompler to ad conversion instructions, which slows down processing
•It is also extra code to maintain, constantly converting one datatype to another in order to do comparisons. especiallv checking for negative values.
                                                                                                                                 * ﻿﻿Critical level 3: Avoid unsafe parsers like scanf
                                                                                                                                 * ﻿﻿sscanf is notorious for being unsafe, as it can easily cause buffer overflows or even
memory corruption.
Is recommended to use stringstream or strtol instead
                                                                                                                                 * ﻿This is because sscan cannot easily detect parsing failures and/or strings with trailing
                                                                                                                                 * ﻿﻿1E: sscanf cannot easily detect invalid strings like "123abc'
                                                                                                                                 * ﻿﻿Critical level 3: Use vector.emplace_back() over vector.push_back()
This prevents the allocation of unnecessary temporaries.
                                                                                                                                 * ﻿Thus the code will perform faster and more efficiently.
                                                                                                                                 * ﻿﻿Critical level 3: Use std::make shared instead of manually creating a shared pointer using new
This prevents the allocation of unnecessary temporaries.
:
Using new is implementation specific, and sometimes costly.
std::make shared() uses a different memory allocator, which is more efficient.
Phase 2 - Data Operations
Data Integrity
The current Pipeline is frequently disrupted due to the lack of robust data validation. Data changes, corruption, or schema changes often necessitate the operator to rerun the entire ParcelPoint dataset
multiple times, as problems orten go unnoticea until much later in the process. Inis is a common occurrence, often due to significant changes in external datasets like USPS, Navteq, or Counties geo-spatial data, since the last time the pipeline was run (last quarter). When this happens, the engineer has to modify the code to support the new schema(s) or data structures and then rerun the entire dataset. The abilitv to detect these issues betore or between malor stages of the viveline run could. significantly reduce wasted time spent on rerunning entire Counties.
In summary, The system could be made less brittle by validating data both before initializing the pipeline and between tasks, as this would allow some problems or errors to be caught earlier in the processing pipeline. This will also increase confidence in the accuracy of the data, as well as confidence that the
process will tunction correculy.
Inis was not done in Phase 1 as it is an independent classification or work, and can be don independently. It also needs further discussion and design as to the implementation details
This is high priority, as having confidence and stability when running the application will greatly reduce the time an operator has to waste waiting for the pipeline to complete just for it to fail and need to be
run aaain.
Effort
Implementing tools to validate the contiguration and input and intermediate datasets should take 3 engineers 4 months to complete.
Much of this task is creating applications from scratch, so the level of effort will be high just for that. But the complexity of any single tool should be low and narrow in scope. And much of the validation is similar, like checking all Shapefiles could be done by a single tool. There also exist one-off custom scripts,
not maintainea In source control, that coula ve pullea In ana retoolea to speea unis process up. but as thev haven't been evaluated. they cannot be part of this estimate.
Risks
There should be very little risk to the pipeline by doing validation. The biggest risk is that validation
misses something, and causes downstream Tasks to fail. But this risk is not new, and is already part of the current pipeline. And these tools will by design not be 100% correct up front, but be iterative. That means they should be designed to validate all upstream data as best they can, there will likely be missed checks that need to be added as the pipeline evolves and is used.
Every input into the PxPoint build should be validated by tooling ahead of time.
This includes the Parcel data, external datasets and the Configuration file(s) used.
This tooling should also be idempotent and non-destructive, and not alter existing directories or files unless explicitly told to do so, and thus be safe to run even if the pipeline is running or has been run
• There should exist a set of tools to do this work
• Not ust quick, unmaintainable scripts.
: Mison insuredontrol
Python is acceptable, it writing in C# or C++ is too much effort
                                                                                                                                 * ﻿﻿Using C++ might be preferable, as it can use the PxPoint libraries directly.
                                                                                                                                 * ﻿﻿Must have sufficient documentation
• Must also show what tools to run against which dataset
• Must show what order to run them in, if there is not a single, top-level tool
• The tools must have sufficient output/feedback so that an operator can quickly fix any problems or discrepancies, without having to waste time figuring out where the problem is or how to fix it.
• The first thing the tooling must do is verify the pipeline configuration
That means validating the Parcels.ini file
                                                                                                                                 * ﻿﻿Making sure the file exists, and is in the proper location
                                                                                                                                 * ﻿﻿The tooling could also create create the file, as it is mostly boilerplate
                                                                                                                                 * ﻿﻿Making sure all path-based properties have the same root-path
• Making sure all config properties are within acceptable ranges
Flag warnings for any properties that within acceptable ranges but diverge from
the detault
• Make sure all required files exist in the referenced paths.
All config properties that point to files need to make sure those files are verified
• Then the tooling should check the size, schema and content of all datasets used
• Make sure all files have the correct naming convention
• All Shapefiles should be checked for structure and (approximate) expected size
Parcel and Point Shapefiles
Shapefiles containing Coastline boundaries
Shapefiles containing County boundaries
Shapetiles containing Lip Code boundaries
Shapefiles containing Municipal boundaries
Shapefiles containing State boundaries
snaperlies containing rorest ounaarles
Other files should be verified for size and structural content
USPS
Navrea
1. Known for field-name and schema changes
ili. Diablo
1. Known for field-name and schema changes
Spatial record data
ReprocessOverride.txt
Licence file
• Check all Database parameters, ana verity a connection to the Databas
• The tooling should then create or undate the databas
                                                                                                                                 * ﻿﻿A User should have to name the DB used, and warned if the DB already exists
                                                                                                                                 * ﻿﻿Only clear the DB if requested; We don't want to accidentally delete in-progress data
                                                                                                                                 * ﻿﻿Verify the Table structure and contents of each Table
                                                                                                                                 * ﻿﻿Check the existence and structure of all intermediate and final directories
                                                                                                                                 * ﻿﻿Make sure all required directories exist and are readable/writable, etc...
                                                                                                                                 * ﻿﻿Make sure all files in those directories match what is expected
• 1E: don't contain unexpected files or files with bad names
Intermediate validation
It is also favorable to validate the data between major stages/Tasks in the pipeline.
Adding this to the current pipeline itself might be too much effort in Phase 1.
But it would be very helpful to have user-runnable tooling, similar (or identical) to the Shapefile checking
tools above, available to run across any data in the pre-processing/pre-normalization part of the pipeline.
• For Phase 1, have the tools be manually runnable, rather than as part of the pipeline itself
• Re able to verifv the innut and outnut Shanefiles for anv Tac
•Should also verify the files (directories) validated are correctly configured in the Parcels.ini fil
• Should also be able to verify the content of the Database, showing what FIPS/Parcels are complete, which failed, and which are in-progress
Ingestion
Loading and preparing all data prior to the pipeline running is currently a very complex manual process.
Thus there is the need to automate the process of preparing all data used by the pipeline.
This is a very convoluted set of steps, somewhat documented, but not easily performed, especially if one simply wants to re-import one dataset, and run it through Data Integrity validation (see above).
This is such a painfully slow and fiddly process, that operators spend a lot of time doing this before even running the pipeline. Thus automating, or at least simplitying, the process will shave days off of the time it takes to build the PxPoint data.
This is not a high priority simply because it occurs outside of the pipeline itself.
Effort
More discussions and documentation needs to occur before we can fully specity this task.
Risks
If this Task takes a long time to run, it could itself turn into a bottleneck to the delivery process. Thus, adding Observability and Telemetry to this process will be essential as well.
ParcelPoint
Load data Trom ParcelPoint
USPS
Load data from USPS
Naved
Load data from Navteq/HERE
Diaolo
Load data from Diablo
Reference
Load data from reference datasets
• Coastlines, Forests, Municipal and County Shapefiles, etc...
Phase 3 - Orchestration
New Orchestrator
Replace the C# Ul with an off-the-shelf Orchestration tool.
The C# pipeline code will have to be rewritten to match the Orchestrator. I do not recommend completely rewriting the C++ code initially. Only when the pipeline is modularized and stabilized will
rewriting Ina viaual moaules will be recommenaea.
Effort
This will be a large task, and could take half a vear or more. with 4-5 engineers. And even then. it wil need to be iterated on to verify correctness and remove bottlenecks
RISkS
Other than the time it will take to accomplish, the biggest risk is losing the subtle nuances and hacks put into the original business logic to get the maximum amount of Parcels processed and Normalized. This is
partially why I recommend not rewriting the ++ code. where most or the business logic is, at least not until the migration to an Orchestrator stabilizes.

Image 1: Data Ingestion Architecture & Pain Points
Current Architecture Analysis:

Multi-source ingestion pattern with varied update frequencies (daily, monthly, quarterly, annually)
Central dataset builds acting as integration point
Complex dependency management between data sources with different refresh cycles

Critical Pain Points Identified:

Data Consistency Crisis: The quarterly build cycle creates a fundamental mismatch with faster-changing data sources (daily USPS, monthly parcels)
Production Data Brittleness: Cannot update production data independently, forcing full quarterly rebuilds
Integration Complexity: Adding new data sources requires significant engineering effort
Explainability Gap: Lack of audit trail for data transformations and decisions

Desired State Insights:
The vision for incremental, county-wise builds with quality checks and cascading fixes represents a much more resilient architecture. The emphasis on logging with reference data for decision provenance is particularly astute.
Image 2: Pipeline Flow Visualization
Pipeline Architecture Observations:

Linear dependency chain with clear phase separation (Pre-processing → Normalization → Post-processing)
Parallel processing opportunities evident in the normalization phase
Complex sub-workflows particularly around parcel preparation and normalization steps
Multiple output formats (PXY files) suggesting different product variants

Engineering Concerns:

Single Points of Failure: The linear nature means any step failure requires restart from beginning
Resource Utilization: Limited parallelization opportunities in pre/post-processing phases
State Management: No clear checkpointing or resume capabilities visible
Orchestration Complexity: Manual coordination of 15+ distinct processing steps

Strategic Assessment
Alignment with 3-Phase Plan:
Phase 1 (Observability) - Critical for this architecture because:

Complex multi-step pipeline needs comprehensive logging
Data consistency issues require detailed audit trails
Performance bottlenecks need measurement across all steps

Phase 2 (Data Operations) - Addresses core pain points:

Data validation tooling essential given multiple data sources with different schemas
Incremental build capability would revolutionize the architecture
Quality checks need to happen at ingestion, not just at final output

Phase 3 (Orchestration) - Natural evolution:

Current linear pipeline needs modern orchestrator for checkpointing/resume
Parallel processing opportunities need proper workflow management
State management for incremental builds requires sophisticated orchestration

Key Recommendations

Data Architecture: Consider implementing a data lake pattern with staged ingestion zones to handle the various update frequencies more elegantly
Pipeline Resilience: The checkpoint/resume capability should be a primary design goal for the new orchestrator
Monitoring Strategy: Given the complexity visible in these diagrams, implement circuit breaker patterns and health checks at each major stage boundary
Incremental Processing: The desired state's incremental build capability will require change data capture and dependency graph management

These diagrams confirm that the 3-phase approach in the document is well-founded and addresses the right architectural challenges. The complexity visible here justifies the conservative approach of preserving business logic while modernizing the infrastructure and operations around it.
